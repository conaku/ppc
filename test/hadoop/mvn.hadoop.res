[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-project:pom:2.4.1
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-enforcer-plugin @ line 1022, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-project-dist:pom:2.4.1
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-enforcer-plugin @ org.apache.hadoop:hadoop-project:2.4.1, /opt/develop/hadoop-common/hadoop-project/pom.xml, line 1022, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-auth:jar:2.4.1
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.mortbay.jetty:jetty-util:jar -> duplicate declaration of version (?) @ line 60, column 17
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-common:jar:2.4.1
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ line 485, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Apache Hadoop Main
[INFO] Apache Hadoop Project POM
[INFO] Apache Hadoop Annotations
[INFO] Apache Hadoop Project Dist POM
[INFO] Apache Hadoop Assemblies
[INFO] Apache Hadoop Maven Plugins
[INFO] Apache Hadoop MiniKDC
[INFO] Apache Hadoop Auth
[INFO] Apache Hadoop Auth Examples
[INFO] Apache Hadoop Common
[INFO] Apache Hadoop NFS
[INFO] Apache Hadoop Common Project
[INFO] Apache Hadoop HDFS
[INFO] Apache Hadoop HttpFS
[INFO] Apache Hadoop HDFS BookKeeper Journal
[INFO] Apache Hadoop HDFS-NFS
[INFO] Apache Hadoop HDFS Project
[INFO] hadoop-yarn
[INFO] hadoop-yarn-api
[INFO] hadoop-yarn-common
[INFO] hadoop-yarn-server
[INFO] hadoop-yarn-server-common
[INFO] hadoop-yarn-server-nodemanager
[INFO] hadoop-yarn-server-web-proxy
[INFO] hadoop-yarn-server-applicationhistoryservice
[INFO] hadoop-yarn-server-resourcemanager
[INFO] hadoop-yarn-server-tests
[INFO] hadoop-yarn-client
[INFO] hadoop-yarn-applications
[INFO] hadoop-yarn-applications-distributedshell
[INFO] hadoop-yarn-applications-unmanaged-am-launcher
[INFO] hadoop-yarn-site
[INFO] hadoop-yarn-project
[INFO] hadoop-mapreduce-client
[INFO] hadoop-mapreduce-client-core
[INFO] hadoop-mapreduce-client-common
[INFO] hadoop-mapreduce-client-shuffle
[INFO] hadoop-mapreduce-client-app
[INFO] hadoop-mapreduce-client-hs
[INFO] hadoop-mapreduce-client-jobclient
[INFO] hadoop-mapreduce-client-hs-plugins
[INFO] Apache Hadoop MapReduce Examples
[INFO] hadoop-mapreduce
[INFO] Apache Hadoop MapReduce Streaming
[INFO] Apache Hadoop Distributed Copy
[INFO] Apache Hadoop Archives
[INFO] Apache Hadoop Rumen
[INFO] Apache Hadoop Gridmix
[INFO] Apache Hadoop Data Join
[INFO] Apache Hadoop Extras
[INFO] Apache Hadoop Pipes
[INFO] Apache Hadoop OpenStack support
[INFO] Apache Hadoop Client
[INFO] Apache Hadoop Mini-Cluster
[INFO] Apache Hadoop Scheduler Load Simulator
[INFO] Apache Hadoop Tools Dist
[INFO] Apache Hadoop Tools
[INFO] Apache Hadoop Distribution
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Main 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (default) @ hadoop-main ---
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Project POM 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-project ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Annotations 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-annotations ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-annotations ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-annotations ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-annotations ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-annotations ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Project Dist POM 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-project-dist ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Assemblies 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (default) @ hadoop-assemblies ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-assemblies ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-assemblies ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-assemblies ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-assemblies ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-assemblies ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-assemblies ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Maven Plugins 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-maven-plugins ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (default-descriptor) @ hadoop-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 2 mojo descriptors.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-maven-plugins ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-maven-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-plugin-plugin:3.0:descriptor (mojo-descriptor) @ hadoop-maven-plugins ---
[INFO] Using 'UTF-8' encoding to read mojo metadata.
[INFO] Applying mojo extractor for language: java-annotations
[INFO] Mojo extractor for language: java-annotations found 2 mojo descriptors.
[INFO] Applying mojo extractor for language: java
[INFO] Mojo extractor for language: java found 0 mojo descriptors.
[INFO] Applying mojo extractor for language: bsh
[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-maven-plugins ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-maven-plugins ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-maven-plugins ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop MiniKDC 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-minikdc ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-minikdc ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-minikdc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-minikdc ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-minikdc ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-minikdc ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-common-project/hadoop-minikdc/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.minikdc.TestMiniKdc
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 27.687 sec - in org.apache.hadoop.minikdc.TestMiniKdc

Results :

Tests run: 3, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Auth 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-auth ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-auth ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-auth ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-auth ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-auth ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-auth ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-common-project/hadoop-auth/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.security.authentication.server.TestPseudoAuthenticationHandler
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.358 sec - in org.apache.hadoop.security.authentication.server.TestPseudoAuthenticationHandler
Running org.apache.hadoop.security.authentication.server.TestAltKerberosAuthenticationHandler
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 62.967 sec - in org.apache.hadoop.security.authentication.server.TestAltKerberosAuthenticationHandler
Running org.apache.hadoop.security.authentication.server.TestAuthenticationFilter
Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.858 sec - in org.apache.hadoop.security.authentication.server.TestAuthenticationFilter
Running org.apache.hadoop.security.authentication.server.TestAuthenticationToken
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.167 sec - in org.apache.hadoop.security.authentication.server.TestAuthenticationToken
Running org.apache.hadoop.security.authentication.server.TestKerberosAuthenticationHandler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.695 sec - in org.apache.hadoop.security.authentication.server.TestKerberosAuthenticationHandler
Running org.apache.hadoop.security.authentication.client.TestPseudoAuthenticator
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.778 sec - in org.apache.hadoop.security.authentication.client.TestPseudoAuthenticator
Running org.apache.hadoop.security.authentication.client.TestAuthenticatedURL
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.369 sec - in org.apache.hadoop.security.authentication.client.TestAuthenticatedURL
Running org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 47.416 sec - in org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator
Running org.apache.hadoop.security.authentication.util.TestKerberosUtil
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.108 sec - in org.apache.hadoop.security.authentication.util.TestKerberosUtil
Running org.apache.hadoop.security.authentication.util.TestKerberosName
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.548 sec - in org.apache.hadoop.security.authentication.util.TestKerberosName
Running org.apache.hadoop.security.authentication.util.TestSigner
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.376 sec - in org.apache.hadoop.security.authentication.util.TestSigner

Results :

Tests run: 65, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Auth Examples 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-auth-examples ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-auth-examples ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-auth-examples ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-auth-examples ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-auth-examples ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-auth-examples ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Common 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-os) @ hadoop-common ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-common ---
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:version-info (version-info) @ hadoop-common ---
[WARNING] [svn, info] failed with error code 1
[INFO] SCM: GIT
[INFO] Computed MD5: 64a9ffb73ee867113216d4ea6aa8ee
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-common ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-common ---
[INFO] Compiling 13 source files to /opt/develop/hadoop-common/hadoop-common-project/hadoop-common/target/classes
[INFO] 
[INFO] --- native-maven-plugin:1.0-alpha-7:javah (default) @ hadoop-common ---
[INFO] /bin/sh -c cd /opt/develop/hadoop-common/hadoop-common-project/hadoop-common && /opt/ibm/java-ppc64le-71/bin/javah -d /opt/develop/hadoop-common/hadoop-common-project/hadoop-common/target/native/javah -classpath /opt/develop/hadoop-common/hadoop-common-project/hadoop-common/target/classes:/opt/develop/hadoop-common/hadoop-common-project/hadoop-annotations/target/classes:/opt/ibm/java-ppc64le-71/jre/../lib/tools.jar:/root/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/root/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/root/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/root/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/root/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/root/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/root/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/root/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/root/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/root/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/root/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/root/.m2/repository/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar:/root/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/.m2/repository/asm/asm/3.2/asm-3.2.jar:/root/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/root/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/root/.m2/repository/org/apache/httpcomponents/httpcore/4.2.5/httpcore-4.2.5.jar:/root/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/root/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/root/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/root/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/root/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/root/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/root/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/root/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/root/.m2/repository/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar:/root/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/opt/develop/hadoop-common/hadoop-common-project/hadoop-auth/target/classes:/root/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/root/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/root/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/root/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar org.apache.hadoop.io.compress.zlib.ZlibCompressor org.apache.hadoop.io.compress.zlib.ZlibDecompressor org.apache.hadoop.io.compress.bzip2.Bzip2Compressor org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor org.apache.hadoop.security.JniBasedUnixGroupsMapping org.apache.hadoop.io.nativeio.NativeIO org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping org.apache.hadoop.io.compress.snappy.SnappyCompressor org.apache.hadoop.io.compress.snappy.SnappyDecompressor org.apache.hadoop.io.compress.lz4.Lz4Compressor org.apache.hadoop.io.compress.lz4.Lz4Decompressor org.apache.hadoop.util.NativeCrc32 org.apache.hadoop.net.unix.DomainSocket org.apache.hadoop.net.unix.DomainSocketWatcher
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (make) @ hadoop-common ---
[INFO] Executing tasks

main:
     [exec] JAVA_HOME=, JAVA_JVM_LIBRARY=/opt/ibm/java-ppc64le-71/jre/lib/ppc64le/default/libjvm.so
     [exec] JAVA_INCLUDE_PATH=/opt/ibm/java-ppc64le--- Configuring incomplete, errors occurred!
     [exec] See also "/opt/develop/hadoop-common/hadoop-common-project/hadoop-common/target/nati71/include, JAVA_INCLUDE_PATH2=/opt/ibm/java-ppc64le-71/include/linux
     [exec] Located all JNI components successfully.
     [exec] CMake Error at CMakeLists.txt:144 (MESSAGE):
     [exec]   Required snappy library could not be found.
     [exec]   SNAPPY_LIBRARY=SNAPPY_LIBRARY-NOTFOUND, SNAPPY_INCLUve/CMakeFiles/CMakeOutput.log".
     [exec] DE_DIR=,
     [exec]   CUSTOM_SNAPPY_INCLUDE_DIR=, CUSTOM_SNAPPY_PREFIX=, CUSTOM_SNAPPY_INCLUDE=
     [exec] 
     [exec] 
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop NFS 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-nfs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-nfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-nfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-nfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-nfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-nfs ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-common-project/hadoop-nfs/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.nfs.TestNfsTime
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.077 sec - in org.apache.hadoop.nfs.TestNfsTime
Running org.apache.hadoop.nfs.TestNfsExports
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.251 sec - in org.apache.hadoop.nfs.TestNfsExports
Running org.apache.hadoop.nfs.nfs3.TestFileHandle
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.138 sec - in org.apache.hadoop.nfs.nfs3.TestFileHandle
Running org.apache.hadoop.nfs.nfs3.TestIdUserGroup
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.442 sec - in org.apache.hadoop.nfs.nfs3.TestIdUserGroup
Running org.apache.hadoop.portmap.TestPortmap
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.666 sec - in org.apache.hadoop.portmap.TestPortmap
Running org.apache.hadoop.oncrpc.TestRpcDeniedReply
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.101 sec - in org.apache.hadoop.oncrpc.TestRpcDeniedReply
Running org.apache.hadoop.oncrpc.TestRpcReply
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.115 sec - in org.apache.hadoop.oncrpc.TestRpcReply
Running org.apache.hadoop.oncrpc.TestRpcMessage
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.083 sec - in org.apache.hadoop.oncrpc.TestRpcMessage
Running org.apache.hadoop.oncrpc.TestRpcAcceptedReply
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.096 sec - in org.apache.hadoop.oncrpc.TestRpcAcceptedReply
Running org.apache.hadoop.oncrpc.TestFrameDecoder
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.713 sec - in org.apache.hadoop.oncrpc.TestFrameDecoder
Running org.apache.hadoop.oncrpc.TestXDR
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.277 sec - in org.apache.hadoop.oncrpc.TestXDR
Running org.apache.hadoop.oncrpc.TestRpcCallCache
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.303 sec - in org.apache.hadoop.oncrpc.TestRpcCallCache
Running org.apache.hadoop.oncrpc.security.TestRpcAuthInfo
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.093 sec - in org.apache.hadoop.oncrpc.security.TestRpcAuthInfo
Running org.apache.hadoop.oncrpc.security.TestCredentialsSys
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.164 sec - in org.apache.hadoop.oncrpc.security.TestCredentialsSys
Running org.apache.hadoop.oncrpc.TestRpcCall
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.159 sec - in org.apache.hadoop.oncrpc.TestRpcCall

Results :

Tests run: 51, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Common Project 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-common-project ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop HDFS 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-hdfs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-jsp-generated-sources-directory) @ hadoop-hdfs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- jspc-maven-plugin:2.0-alpha-3:compile (hdfs) @ hadoop-hdfs ---
[WARNING] Compiled JSPs will not be added to the project and web.xml will not be modified, either because includeInProject is set to false or because the project's packaging is not 'war'.
[INFO] Compiling 8 JSP source files to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java
log4j:WARN No appenders could be found for logger (org.apache.jasper.JspC).
log4j:WARN Please initialize the log4j system properly.
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html for an explanation.
[INFO] Compiled completed in 0:00:00.117
[INFO] 
[INFO] --- jspc-maven-plugin:2.0-alpha-3:compile (secondary) @ hadoop-hdfs ---
[WARNING] Compiled JSPs will not be added to the project and web.xml will not be modified, either because includeInProject is set to false or because the project's packaging is not 'war'.
[INFO] Compiling 1 JSP source file to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html for an explanation.
[INFO] Compiled completed in 0:00:00.011
[INFO] 
[INFO] --- jspc-maven-plugin:2.0-alpha-3:compile (journal) @ hadoop-hdfs ---
[WARNING] Compiled JSPs will not be added to the project and web.xml will not be modified, either because includeInProject is set to false or because the project's packaging is not 'war'.
[INFO] Compiling 1 JSP source file to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html for an explanation.
[INFO] Compiled completed in 0:00:00.011
[INFO] 
[INFO] --- jspc-maven-plugin:2.0-alpha-3:compile (datanode) @ hadoop-hdfs ---
[WARNING] Compiled JSPs will not be added to the project and web.xml will not be modified, either because includeInProject is set to false or because the project's packaging is not 'war'.
[INFO] Compiling 4 JSP source files to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html for an explanation.
[INFO] Compiled completed in 0:00:00.012
[INFO] 
[INFO] --- build-helper-maven-plugin:1.5:add-source (add-jsp-generated-sources-directory) @ hadoop-hdfs ---
[INFO] Source directory: /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java added.
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-hdfs ---
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc-datanode) @ hadoop-hdfs ---
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc-namenode) @ hadoop-hdfs ---
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc-qjournal) @ hadoop-hdfs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-hdfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-hdfs ---
[INFO] Compiling 12 source files to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/classes
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-web-xmls) @ hadoop-hdfs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (make) @ hadoop-hdfs ---
[INFO] Executing tasks

main:
     [exec] -- checking for module 'fuse'
     [exec] --   package 'fuse' not found
     [exec] -- Failed to find Linux FUSE libraries or include files.  Will not bJAVA_HOME=, JAVA_JVM_LIBRARY=/opt/ibm/java-ppc64le-71/jre/lib/ppc64le/default/libjvm.so
     [exec] JAVA_INCLUDE_PATH=/opt/ibm/java-ppc64le-uild FUSE client.
     [exec] -- Configuring done
     [exec] -- Generating done
     [exec] -- Build files have been written to: /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native
     [exec] 71/include, JAVA_INCLUDE_PATH2=/opt/ibm/java-ppc64le-71/include/linux
     [exec] Located all JNI components successfully.
     [exec] /usr/bin/cmake -H/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src -B/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native --check-build-system CMakeFiles/Makefile.cmake 0
     [exec] /usr/bin/cmake -E cmake_progress_start /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/progress.marks
     [exec] make -f CMakeFiles/Makefile2 all
     [exec] make[1]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/hdfs.dir/build.make CMakeFiles/hdfs.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/hdfs.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/hdfs.dir/build.make CMakeFiles/hdfs.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/hdfs.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  1 2 3
     [exec] [ 18%] Built target hdfs
     [exec] make -f CMakeFiles/hdfs_static.dir/build.make CMakeFiles/hdfs_static.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/hdfs_static.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/hdfs_static.dir/build.make CMakeFiles/hdfs_static.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/hdfs_static.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  4 5 6
     [exec] [ 37%] Built target hdfs_static
     [exec] make -f CMakeFiles/native_mini_dfs.dir/build.make CMakeFiles/native_mini_dfs.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/native_mini_dfs.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/native_mini_dfs.dir/build.make CMakeFiles/native_mini_dfs.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/native_mini_dfs.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  7
     [exec] [ 43%] Built target native_mini_dfs
     [exec] make -f CMakeFiles/posix_util.dir/build.make CMakeFiles/posix_util.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/posix_util.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/posix_util.dir/build.make CMakeFiles/posix_util.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/posix_util.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  8
     [exec] [ 50%] Built target posix_util
     [exec] make -f CMakeFiles/test_libhdfs_ops.dir/build.make CMakeFiles/test_libhdfs_ops.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/test_libhdfs_ops.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/test_libhdfs_ops.dir/build.make CMakeFiles/test_libhdfs_ops.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/test_libhdfs_ops.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  9
     [exec] [ 56%] Built target test_libhdfs_ops
     [exec] make -f CMakeFiles/test_libhdfs_read.dir/build.make CMakeFiles/test_libhdfs_read.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/test_libhdfs_read.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/test_libhdfs_read.dir/build.make CMakeFiles/test_libhdfs_read.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/test_libhdfs_read.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  10
     [exec] [ 62%] Built target test_libhdfs_read
     [exec] make -f CMakeFiles/test_libhdfs_threaded.dir/build.make CMakeFiles/test_libhdfs_threaded.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/test_libhdfs_threaded.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/test_libhdfs_threaded.dir/build.make CMakeFiles/test_libhdfs_threaded.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/test_libhdfs_threaded.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  11 12
     [exec] [ 75%] Built target test_libhdfs_threaded
     [exec] make -f CMakeFiles/test_libhdfs_write.dir/build.make CMakeFiles/test_libhdfs_write.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/test_libhdfs_write.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/test_libhdfs_write.dir/build.make CMakeFiles/test_libhdfs_write.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/test_libhdfs_write.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  13
     [exec] [ 81%] Built target test_libhdfs_write
     [exec] make -f CMakeFiles/test_libhdfs_zerocopy.dir/build.make CMakeFiles/test_libhdfs_zerocopy.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/test_libhdfs_zerocopy.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/test_libhdfs_zerocopy.dir/build.make CMakeFiles/test_libhdfs_zerocopy.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/test_libhdfs_zerocopy.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  14 15
     [exec] [ 93%] Built target test_libhdfs_zerocopy
     [exec] make -f CMakeFiles/test_native_mini_dfs.dir/build.make CMakeFiles/test_native_mini_dfs.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles/test_native_mini_dfs.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make -f CMakeFiles/test_native_mini_dfs.dir/build.make CMakeFiles/test_native_mini_dfs.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/test_native_mini_dfs.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles  16
     [exec] [100%] Built target test_native_mini_dfs
     [exec] make[1]: Leaving directory `/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_start /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/CMakeFiles 0
     [exec] [ 18%] Built target hdfs
     [exec] [ 37%] Built target hdfs_static
     [exec] [ 43%] Built target native_mini_dfs
     [exec] [ 50%] Built target posix_util
     [exec] [ 56%] Built target test_libhdfs_ops
     [exec] [ 62%] Built target test_libhdfs_read
     [exec] [ 75%] Built target test_libhdfs_threaded
     [exec] [ 81%] Built target test_libhdfs_write
     [exec] [ 93%] Built target test_libhdfs_zerocopy
     [exec] [100%] Built target test_native_mini_dfs
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-hdfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-log-dir) @ hadoop-hdfs ---
[INFO] Executing tasks

main:
   [delete] Deleting directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data
    [mkdir] Created dir: /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-hdfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-hdfs ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.tools.TestDelegationTokenRemoteFetcher
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.47 sec - in org.apache.hadoop.tools.TestDelegationTokenRemoteFetcher
Running org.apache.hadoop.tools.TestJMXGet
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.815 sec - in org.apache.hadoop.tools.TestJMXGet
Running org.apache.hadoop.tools.TestTools
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.171 sec - in org.apache.hadoop.tools.TestTools
Running org.apache.hadoop.tools.TestDelegationTokenFetcher
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.174 sec - in org.apache.hadoop.tools.TestDelegationTokenFetcher
Running org.apache.hadoop.fs.permission.TestStickyBit
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.029 sec - in org.apache.hadoop.fs.permission.TestStickyBit
Running org.apache.hadoop.fs.TestHDFSFileContextMainOperations
Tests run: 67, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.308 sec - in org.apache.hadoop.fs.TestHDFSFileContextMainOperations
Running org.apache.hadoop.fs.TestResolveHdfsSymlink
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.947 sec - in org.apache.hadoop.fs.TestResolveHdfsSymlink
Running org.apache.hadoop.fs.TestSymlinkHdfsDisable
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.037 sec - in org.apache.hadoop.fs.TestSymlinkHdfsDisable
Running org.apache.hadoop.fs.loadGenerator.TestLoadGenerator
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.708 sec - in org.apache.hadoop.fs.loadGenerator.TestLoadGenerator
Running org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs
Tests run: 40, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.951 sec - in org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs
Running org.apache.hadoop.fs.viewfs.TestViewFsHdfs
Tests run: 42, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.965 sec - in org.apache.hadoop.fs.viewfs.TestViewFsHdfs
Running org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot
Tests run: 42, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.347 sec - in org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot
Running org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot
Tests run: 40, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.985 sec - in org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot
Running org.apache.hadoop.fs.viewfs.TestViewFsFileStatusHdfs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.237 sec - in org.apache.hadoop.fs.viewfs.TestViewFsFileStatusHdfs
Running org.apache.hadoop.fs.TestSymlinkHdfsFileContext
Tests run: 69, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.922 sec - in org.apache.hadoop.fs.TestSymlinkHdfsFileContext
Running org.apache.hadoop.fs.TestGlobPaths
Tests run: 33, Failures: 0, Errors: 0, Skipped: 6, Time elapsed: 9.139 sec - in org.apache.hadoop.fs.TestGlobPaths
Running org.apache.hadoop.fs.TestFcHdfsSetUMask
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.682 sec - in org.apache.hadoop.fs.TestFcHdfsSetUMask
Running org.apache.hadoop.fs.TestVolumeId
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.308 sec - in org.apache.hadoop.fs.TestVolumeId
Running org.apache.hadoop.fs.TestUrlStreamHandlerFactory
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.908 sec - in org.apache.hadoop.fs.TestUrlStreamHandlerFactory
Running org.apache.hadoop.fs.TestSymlinkHdfsFileSystem
Tests run: 72, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 18.13 sec - in org.apache.hadoop.fs.TestSymlinkHdfsFileSystem
Running org.apache.hadoop.fs.TestUrlStreamHandler
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.227 sec - in org.apache.hadoop.fs.TestUrlStreamHandler
Running org.apache.hadoop.fs.TestHdfsNativeCodeLoader
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.083 sec - in org.apache.hadoop.fs.TestHdfsNativeCodeLoader
Running org.apache.hadoop.fs.TestFcHdfsPermission
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.555 sec - in org.apache.hadoop.fs.TestFcHdfsPermission
Running org.apache.hadoop.fs.TestEnhancedByteBufferAccess
Tests run: 10, Failures: 0, Errors: 0, Skipped: 9, Time elapsed: 0.402 sec - in org.apache.hadoop.fs.TestEnhancedByteBufferAccess
Running org.apache.hadoop.fs.TestFcHdfsCreateMkdir
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.435 sec - in org.apache.hadoop.fs.TestFcHdfsCreateMkdir
Running org.apache.hadoop.net.TestHdfsNetworkTopologyWithNodeGroup
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.461 sec - in org.apache.hadoop.net.TestHdfsNetworkTopologyWithNodeGroup
Running org.apache.hadoop.net.TestNetworkTopology
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.23 sec - in org.apache.hadoop.net.TestNetworkTopology
Running org.apache.hadoop.hdfs.TestConnCache
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.165 sec - in org.apache.hadoop.hdfs.TestConnCache
Running org.apache.hadoop.hdfs.TestLocalDFS
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.507 sec - in org.apache.hadoop.hdfs.TestLocalDFS
Running org.apache.hadoop.hdfs.TestDFSClientRetries
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 158.349 sec - in org.apache.hadoop.hdfs.TestDFSClientRetries
Running org.apache.hadoop.hdfs.TestClientProtocolForPipelineRecovery
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.326 sec - in org.apache.hadoop.hdfs.TestClientProtocolForPipelineRecovery
Running org.apache.hadoop.hdfs.TestHdfsAdmin
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.2 sec - in org.apache.hadoop.hdfs.TestHdfsAdmin
Running org.apache.hadoop.hdfs.protocolPB.TestPBHelper
Tests run: 28, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.023 sec - in org.apache.hadoop.hdfs.protocolPB.TestPBHelper
Running org.apache.hadoop.hdfs.TestMiniDFSCluster
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.841 sec - in org.apache.hadoop.hdfs.TestMiniDFSCluster
Running org.apache.hadoop.hdfs.TestHFlush
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.18 sec - in org.apache.hadoop.hdfs.TestHFlush
Running org.apache.hadoop.hdfs.TestBlockReaderFactory
Tests run: 6, Failures: 0, Errors: 0, Skipped: 6, Time elapsed: 0.251 sec - in org.apache.hadoop.hdfs.TestBlockReaderFactory
Running org.apache.hadoop.hdfs.tools.TestDFSHAAdminMiniCluster
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.942 sec - in org.apache.hadoop.hdfs.tools.TestDFSHAAdminMiniCluster
Running org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOIVCanReadOldVersions
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.275 sec - in org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOIVCanReadOldVersions
Running org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.555 sec - in org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer
Running org.apache.hadoop.hdfs.tools.offlineImageViewer.TestDelimitedImageVisitor
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.094 sec - in org.apache.hadoop.hdfs.tools.offlineImageViewer.TestDelimitedImageVisitor
Running org.apache.hadoop.hdfs.tools.TestGetConf
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.101 sec - in org.apache.hadoop.hdfs.tools.TestGetConf
Running org.apache.hadoop.hdfs.tools.TestGetGroups
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.51 sec - in org.apache.hadoop.hdfs.tools.TestGetGroups
Running org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.538 sec - in org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
Running org.apache.hadoop.hdfs.tools.TestDFSHAAdmin
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.862 sec - in org.apache.hadoop.hdfs.tools.TestDFSHAAdmin
Running org.apache.hadoop.hdfs.TestDFSClientExcludedNodes
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.848 sec - in org.apache.hadoop.hdfs.TestDFSClientExcludedNodes
Running org.apache.hadoop.hdfs.TestDeprecatedKeys
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.493 sec - in org.apache.hadoop.hdfs.TestDeprecatedKeys
Running org.apache.hadoop.hdfs.TestParallelUnixDomainRead
Tests run: 4, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 0.26 sec - in org.apache.hadoop.hdfs.TestParallelUnixDomainRead
Running org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.134 sec - in org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart
Running org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.206 sec - in org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure
Running org.apache.hadoop.hdfs.TestDFSRollback
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.883 sec - in org.apache.hadoop.hdfs.TestDFSRollback
Running org.apache.hadoop.hdfs.TestSetrepDecreasing
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.83 sec - in org.apache.hadoop.hdfs.TestSetrepDecreasing
Running org.apache.hadoop.hdfs.TestSetrepIncreasing
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.789 sec - in org.apache.hadoop.hdfs.TestSetrepIncreasing
Running org.apache.hadoop.hdfs.TestLeaseRenewer
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.282 sec - in org.apache.hadoop.hdfs.TestLeaseRenewer
Running org.apache.hadoop.hdfs.TestSeekBug
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.63 sec - in org.apache.hadoop.hdfs.TestSeekBug
Running org.apache.hadoop.hdfs.TestDFSRemove
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.036 sec - in org.apache.hadoop.hdfs.TestDFSRemove
Running org.apache.hadoop.hdfs.TestDFSRename
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.119 sec - in org.apache.hadoop.hdfs.TestDFSRename
Running org.apache.hadoop.hdfs.TestFileCreationDelete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.351 sec - in org.apache.hadoop.hdfs.TestFileCreationDelete
Running org.apache.hadoop.hdfs.TestShortCircuitCache
Tests run: 7, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 11.199 sec - in org.apache.hadoop.hdfs.TestShortCircuitCache
Running org.apache.hadoop.hdfs.TestAbandonBlock
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.567 sec - in org.apache.hadoop.hdfs.TestAbandonBlock
Running org.apache.hadoop.hdfs.TestLargeBlock
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.261 sec - in org.apache.hadoop.hdfs.TestLargeBlock
Running org.apache.hadoop.hdfs.TestClose
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.699 sec - in org.apache.hadoop.hdfs.TestClose
Running org.apache.hadoop.hdfs.TestDFSShellGenericOptions
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.992 sec - in org.apache.hadoop.hdfs.TestDFSShellGenericOptions
Running org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.237 sec - in org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset
Running org.apache.hadoop.hdfs.server.datanode.TestReadOnlySharedStorage
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.863 sec - in org.apache.hadoop.hdfs.server.datanode.TestReadOnlySharedStorage
Running org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestInterDatanodeProtocol
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.428 sec - in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestInterDatanodeProtocol
Running org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.627 sec - in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart
Running org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestReplicaMap
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.078 sec - in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestReplicaMap
Running org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.846 sec - in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica
Running org.apache.hadoop.hdfs.server.datanode.fsdataset.TestAvailableSpaceVolumeChoosingPolicy
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.431 sec - in org.apache.hadoop.hdfs.server.datanode.fsdataset.TestAvailableSpaceVolumeChoosingPolicy
Running org.apache.hadoop.hdfs.server.datanode.fsdataset.TestRoundRobinVolumeChoosingPolicy
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.286 sec - in org.apache.hadoop.hdfs.server.datanode.fsdataset.TestRoundRobinVolumeChoosingPolicy
Running org.apache.hadoop.hdfs.server.datanode.TestDatanodeRegister
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.126 sec - in org.apache.hadoop.hdfs.server.datanode.TestDatanodeRegister
Running org.apache.hadoop.hdfs.server.datanode.TestRefreshNamenodes
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.615 sec - in org.apache.hadoop.hdfs.server.datanode.TestRefreshNamenodes
Running org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 65.023 sec - in org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport
Running org.apache.hadoop.hdfs.server.datanode.TestBlockPoolSliceStorage
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.526 sec - in org.apache.hadoop.hdfs.server.datanode.TestBlockPoolSliceStorage
Running org.apache.hadoop.hdfs.server.datanode.TestFsDatasetCache
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 99.992 sec - in org.apache.hadoop.hdfs.server.datanode.TestFsDatasetCache
Running org.apache.hadoop.hdfs.server.datanode.TestStartSecureDataNode
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.086 sec - in org.apache.hadoop.hdfs.server.datanode.TestStartSecureDataNode
Running org.apache.hadoop.hdfs.server.datanode.TestTransferRbw
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.81 sec - in org.apache.hadoop.hdfs.server.datanode.TestTransferRbw
Running org.apache.hadoop.hdfs.server.datanode.TestIncrementalBrVariations
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.629 sec - in org.apache.hadoop.hdfs.server.datanode.TestIncrementalBrVariations
Running org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.308 sec - in org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
Running org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.046 sec - in org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery
Running org.apache.hadoop.hdfs.server.datanode.TestBlockPoolManager
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.333 sec - in org.apache.hadoop.hdfs.server.datanode.TestBlockPoolManager
Running org.apache.hadoop.hdfs.server.datanode.TestDatanodeStartupOptions
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.584 sec - in org.apache.hadoop.hdfs.server.datanode.TestDatanodeStartupOptions
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 47.508 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics
Running org.apache.hadoop.hdfs.server.datanode.TestHdfsServerConstants
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.1 sec - in org.apache.hadoop.hdfs.server.datanode.TestHdfsServerConstants
Running org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 159.838 sec - in org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner
Running org.apache.hadoop.hdfs.server.datanode.TestDiskError
Running org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 8.794 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool
testDeleteBlockPool(org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool)  Time elapsed: 7.943 sec  <<< ERROR!
org.apache.hadoop.ipc.RemoteException: File /alpha could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2702)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy16.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:361)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1439)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1261)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)

testDfsAdminDeleteBlockPool(org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool)  Time elapsed: 0.757 sec  <<< ERROR!
java.io.IOException: Missing directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
	at org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker.addDirToCheck(NameNodeResourceChecker.java:162)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker.<init>(NameNodeResourceChecker.java:134)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startCommonServices(FSNamesystem.java:960)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:592)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:578)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool.testDfsAdminDeleteBlockPool(TestDeleteBlockPool.java:161)

Running org.apache.hadoop.hdfs.server.datanode.TestStorageReport
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.286 sec - in org.apache.hadoop.hdfs.server.datanode.TestStorageReport
Running org.apache.hadoop.hdfs.server.datanode.TestDnRespectsBlockReportSplitThreshold
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.283 sec - in org.apache.hadoop.hdfs.server.datanode.TestDnRespectsBlockReportSplitThreshold
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeMXBean
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.718 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeMXBean
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 55.508 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations
Running org.apache.hadoop.hdfs.server.datanode.TestBlockHasMultipleReplicasOnSameDN
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.545 sec - in org.apache.hadoop.hdfs.server.datanode.TestBlockHasMultipleReplicasOnSameDN
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeExit
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.465 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeExit
Running org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.09 sec - in org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement
Running org.apache.hadoop.hdfs.server.datanode.web.resources.TestDatanodeWebHdfsMethods
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.776 sec - in org.apache.hadoop.hdfs.server.datanode.web.resources.TestDatanodeWebHdfsMethods
Running org.apache.hadoop.hdfs.server.datanode.TestCachingStrategy
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.338 sec - in org.apache.hadoop.hdfs.server.datanode.TestCachingStrategy
Running org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.294 sec - in org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration
Tests run: 4, Failures: 2, Errors: 2, Skipped: 0, Time elapsed: 94.378 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration
testValidVolumesAtStartup(org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration)  Time elapsed: 17.668 sec  <<< FAILURE!
java.lang.AssertionError: The DN shouldn't have a bad directory.
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertFalse(Assert.java:68)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration.testValidVolumesAtStartup(TestDataNodeVolumeFailureToleration.java:125)

testConfigureMinValidVolumes(org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration)  Time elapsed: 42.703 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for /test1 to reach 2 replicas
	at org.apache.hadoop.hdfs.DFSTestUtil.waitReplication(DFSTestUtil.java:642)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration.testConfigureMinValidVolumes(TestDataNodeVolumeFailureToleration.java:160)

testVolumeAndTolerableConfiguration(org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration)  Time elapsed: 2.488 sec  <<< FAILURE!
java.lang.AssertionError: expected:<false> but was:<true>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:147)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration.testVolumeConfig(TestDataNodeVolumeFailureToleration.java:233)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration.testVolumeAndTolerableConfiguration(TestDataNodeVolumeFailureToleration.java:202)

testFailedVolumeOnStartupIsCounted(org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration)  Time elapsed: 31.403 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for capacity. Live = 1 Expected = 1 Dead = 0 Expected = 0 Total capacity = 105603260416 Expected = 52801630208 Vol Fails = 0 Expected = 1
	at org.apache.hadoop.hdfs.DFSTestUtil.waitForDatanodeStatus(DFSTestUtil.java:569)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration.testFailedVolumeOnStartupIsCounted(TestDataNodeVolumeFailureToleration.java:274)

Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.829 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure
Running org.apache.hadoop.hdfs.server.datanode.TestDatanodeJsp
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.799 sec - in org.apache.hadoop.hdfs.server.datanode.TestDatanodeJsp
Running org.apache.hadoop.hdfs.server.datanode.TestIncrementalBlockReports
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.418 sec - in org.apache.hadoop.hdfs.server.datanode.TestIncrementalBlockReports
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
Tests run: 2, Failures: 1, Errors: 1, Skipped: 0, Time elapsed: 42.103 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
testSuccessiveVolumeFailures(org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting)  Time elapsed: 9.602 sec  <<< FAILURE!
java.lang.AssertionError: Bad value for metric VolumeFailures expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.hadoop.test.MetricsAsserts.assertCounter(MetricsAsserts.java:228)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures(TestDataNodeVolumeFailureReporting.java:154)

testVolFailureStatsPreservedOnNNRestart(org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting)  Time elapsed: 32.304 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for capacity. Live = 3 Expected = 3 Dead = 0 Expected = 0 Total capacity = 316809781248 Expected = 211206520832 Vol Fails = 0 Expected = 2
	at org.apache.hadoop.hdfs.DFSTestUtil.waitForDatanodeStatus(DFSTestUtil.java:569)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testVolFailureStatsPreservedOnNNRestart(TestDataNodeVolumeFailureReporting.java:273)

Running org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 66.022 sec - in org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage
Running org.apache.hadoop.hdfs.server.datanode.TestDataDirs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.849 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataDirs
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeInitStorage
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.276 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeInitStorage
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 46.616 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade
Running org.apache.hadoop.hdfs.server.datanode.TestHSync
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.488 sec - in org.apache.hadoop.hdfs.server.datanode.TestHSync
Running org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl
Tests run: 61, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.627 sec - in org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl
Running org.apache.hadoop.hdfs.server.namenode.TestSecureNameNodeWithExternalKdc
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.094 sec - in org.apache.hadoop.hdfs.server.namenode.TestSecureNameNodeWithExternalKdc
Running org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.846 sec - in org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager
Running org.apache.hadoop.hdfs.server.namenode.TestFSEditLogLoader
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.036 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSEditLogLoader
Running org.apache.hadoop.hdfs.server.namenode.TestPathComponents
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.25 sec - in org.apache.hadoop.hdfs.server.namenode.TestPathComponents
Running org.apache.hadoop.hdfs.server.namenode.TestFSDirectory
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.443 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSDirectory
Running org.apache.hadoop.hdfs.server.namenode.TestStartup
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.324 sec - in org.apache.hadoop.hdfs.server.namenode.TestStartup
Running org.apache.hadoop.hdfs.server.namenode.TestEditLogFileInputStream
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.169 sec - in org.apache.hadoop.hdfs.server.namenode.TestEditLogFileInputStream
Running org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 49.226 sec - in org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
Running org.apache.hadoop.hdfs.server.namenode.metrics.TestNNMetricFilesInGetListingOps
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.515 sec - in org.apache.hadoop.hdfs.server.namenode.metrics.TestNNMetricFilesInGetListingOps
Running org.apache.hadoop.hdfs.server.namenode.TestHostsFiles
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.632 sec - in org.apache.hadoop.hdfs.server.namenode.TestHostsFiles
Running org.apache.hadoop.hdfs.server.namenode.TestEditLog
Tests run: 22, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 111.764 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestEditLog
testFailedOpen(org.apache.hadoop.hdfs.server.namenode.TestEditLog)  Time elapsed: 0.071 sec  <<< FAILURE!
java.lang.AssertionError: Did no throw exception on only having a bad dir
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hdfs.server.namenode.TestEditLog.testFailedOpen(TestEditLog.java:929)

Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.255 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer
org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer  Time elapsed: 0.254 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer.setUp(TestNameNodeHttpServer.java:73)

org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer  Time elapsed: 0.255 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer.tearDown(TestNameNodeHttpServer.java:82)

Running org.apache.hadoop.hdfs.server.namenode.TestFSImageWithAcl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 96.799 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSImageWithAcl
Running org.apache.hadoop.hdfs.server.namenode.TestDeadDatanode
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.691 sec - in org.apache.hadoop.hdfs.server.namenode.TestDeadDatanode
Running org.apache.hadoop.hdfs.server.namenode.TestLeaseManager
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.643 sec - in org.apache.hadoop.hdfs.server.namenode.TestLeaseManager
Running org.apache.hadoop.hdfs.server.namenode.TestDeduplicationMap
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.117 sec - in org.apache.hadoop.hdfs.server.namenode.TestDeduplicationMap
Running org.apache.hadoop.hdfs.server.namenode.TestBlockUnderConstruction
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.048 sec - in org.apache.hadoop.hdfs.server.namenode.TestBlockUnderConstruction
Running org.apache.hadoop.hdfs.server.namenode.TestNamenodeRetryCache
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.053 sec - in org.apache.hadoop.hdfs.server.namenode.TestNamenodeRetryCache
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeRecovery
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.704 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeRecovery
Running org.apache.hadoop.hdfs.server.namenode.TestEditLogAutoroll
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.757 sec - in org.apache.hadoop.hdfs.server.namenode.TestEditLogAutoroll
Running org.apache.hadoop.hdfs.server.namenode.TestAuditLogger
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.842 sec - in org.apache.hadoop.hdfs.server.namenode.TestAuditLogger
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeOptionParsing
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.543 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeOptionParsing
Running org.apache.hadoop.hdfs.server.namenode.TestStreamFile
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.073 sec - in org.apache.hadoop.hdfs.server.namenode.TestStreamFile
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeRpcServer
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.118 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeRpcServer
Running org.apache.hadoop.hdfs.server.namenode.TestFavoredNodesEndToEnd
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.237 sec - in org.apache.hadoop.hdfs.server.namenode.TestFavoredNodesEndToEnd
Running org.apache.hadoop.hdfs.server.namenode.TestSequentialBlockId
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.659 sec - in org.apache.hadoop.hdfs.server.namenode.TestSequentialBlockId
Running org.apache.hadoop.hdfs.server.namenode.TestGetImageServlet
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.484 sec - in org.apache.hadoop.hdfs.server.namenode.TestGetImageServlet
Running org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.965 sec - in org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage
Running org.apache.hadoop.hdfs.server.namenode.TestCreateEditsLog
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.636 sec - in org.apache.hadoop.hdfs.server.namenode.TestCreateEditsLog
Running org.apache.hadoop.hdfs.server.namenode.TestAddBlockRetry
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.748 sec - in org.apache.hadoop.hdfs.server.namenode.TestAddBlockRetry
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestCheckpointsWithSnapshots
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.98 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestCheckpointsWithSnapshots
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotListing
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.158 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotListing
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestINodeFileUnderConstructionWithSnapshot
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.245 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestINodeFileUnderConstructionWithSnapshot
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 57.567 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestRenameWithSnapshots
Tests run: 35, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 138.914 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestRenameWithSnapshots
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestDisallowModifyROSnapshot
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestDisallowModifyROSnapshot
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.984 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotBlocksMap
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.729 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotBlocksMap
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.966 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSetQuotaWithSnapshot
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.297 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSetQuotaWithSnapshot
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotMetrics
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.387 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotMetrics
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.234 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestNestedSnapshots
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 53.116 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestNestedSnapshots
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotReplication
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.588 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotReplication
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshottableDirListing
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.632 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshottableDirListing
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 98.757 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotManager
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.636 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotManager
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotNameWithInvalidCharacters
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.563 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotNameWithInvalidCharacters
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotRename
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.483 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotRename
Running org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 77.417 sec - in org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot
Running org.apache.hadoop.hdfs.server.namenode.TestHDFSConcat
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.455 sec - in org.apache.hadoop.hdfs.server.namenode.TestHDFSConcat
Running org.apache.hadoop.hdfs.server.namenode.TestCheckpoint
Tests run: 37, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 102.716 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestCheckpoint
testNameDirError(org.apache.hadoop.hdfs.server.namenode.TestCheckpoint)  Time elapsed: 5.741 sec  <<< FAILURE!
java.lang.AssertionError: NN should have failed to start with /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 set unreadable
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hdfs.server.namenode.TestCheckpoint.testNameDirError(TestCheckpoint.java:194)

testCheckpointWithSeparateDirsAfterNameFails(org.apache.hadoop.hdfs.server.namenode.TestCheckpoint)  Time elapsed: 1.535 sec  <<< FAILURE!
java.lang.AssertionError: Did not fail to checkpoint when there are no valid storage dirs
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hdfs.server.namenode.TestCheckpoint.testCheckpointWithSeparateDirsAfterNameFails(TestCheckpoint.java:2119)

Running org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.996 sec - in org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeAcl
Tests run: 61, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.261 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeAcl
Running org.apache.hadoop.hdfs.server.namenode.TestEditLogJournalFailures
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.866 sec - in org.apache.hadoop.hdfs.server.namenode.TestEditLogJournalFailures
Running org.apache.hadoop.hdfs.server.namenode.TestFsck
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 54.235 sec - in org.apache.hadoop.hdfs.server.namenode.TestFsck
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourceChecker
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.25 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourceChecker
Running org.apache.hadoop.hdfs.server.namenode.startupprogress.TestStartupProgressMetrics
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.594 sec - in org.apache.hadoop.hdfs.server.namenode.startupprogress.TestStartupProgressMetrics
Running org.apache.hadoop.hdfs.server.namenode.startupprogress.TestStartupProgress
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.407 sec - in org.apache.hadoop.hdfs.server.namenode.startupprogress.TestStartupProgress
Running org.apache.hadoop.hdfs.server.namenode.TestClusterId
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.229 sec - in org.apache.hadoop.hdfs.server.namenode.TestClusterId
Running org.apache.hadoop.hdfs.server.namenode.TestParallelImageWrite
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.545 sec - in org.apache.hadoop.hdfs.server.namenode.TestParallelImageWrite
Running org.apache.hadoop.hdfs.server.namenode.TestSecondaryWebUi
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.402 sec - in org.apache.hadoop.hdfs.server.namenode.TestSecondaryWebUi
Running org.apache.hadoop.hdfs.server.namenode.TestFSImage
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.64 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSImage
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeJspHelper
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.311 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeJspHelper
Running org.apache.hadoop.hdfs.server.namenode.TestFSImageWithSnapshot
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.945 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSImageWithSnapshot
Running org.apache.hadoop.hdfs.server.namenode.TestStartupOptionUpgrade
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.037 sec - in org.apache.hadoop.hdfs.server.namenode.TestStartupOptionUpgrade
Running org.apache.hadoop.hdfs.server.namenode.TestStorageRestore
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.107 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestStorageRestore
testStorageRestoreFailure(org.apache.hadoop.hdfs.server.namenode.TestStorageRestore)  Time elapsed: 1.784 sec  <<< FAILURE!
java.lang.AssertionError: null
	at org.apache.hadoop.hdfs.server.namenode.TestStorageRestore.testStorageRestoreFailure(TestStorageRestore.java:415)

Running org.apache.hadoop.hdfs.server.namenode.TestEditsDoubleBuffer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.092 sec - in org.apache.hadoop.hdfs.server.namenode.TestEditsDoubleBuffer
Running org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.736 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.2 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean
testNameNodeMXBeanInfo(org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean)  Time elapsed: 5.972 sec  <<< FAILURE!
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean.testNameNodeMXBeanInfo(TestNameNodeMXBean.java:178)

Running org.apache.hadoop.hdfs.server.namenode.TestFsLimits
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.587 sec - in org.apache.hadoop.hdfs.server.namenode.TestFsLimits
Running org.apache.hadoop.hdfs.server.namenode.TestNNThroughputBenchmark
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.841 sec - in org.apache.hadoop.hdfs.server.namenode.TestNNThroughputBenchmark
Running org.apache.hadoop.hdfs.server.namenode.web.resources.TestWebHdfsDataLocality
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.786 sec - in org.apache.hadoop.hdfs.server.namenode.web.resources.TestWebHdfsDataLocality
Running org.apache.hadoop.hdfs.server.namenode.TestListCorruptFileBlocks
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 100.123 sec - in org.apache.hadoop.hdfs.server.namenode.TestListCorruptFileBlocks
Running org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.403 sec - in org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens
Running org.apache.hadoop.hdfs.server.namenode.TestStartupProgressServlet
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.633 sec - in org.apache.hadoop.hdfs.server.namenode.TestStartupProgressServlet
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.966 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHAConfiguration
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.072 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHAConfiguration
Running org.apache.hadoop.hdfs.server.namenode.ha.TestDFSZKFailoverController
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 26.573 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestDFSZKFailoverController
Running org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 77.802 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
Running org.apache.hadoop.hdfs.server.namenode.ha.TestFailoverWithBlockTokensEnabled
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 44.314 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestFailoverWithBlockTokensEnabled
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.744 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend
Running org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA
Tests run: 9, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 19.016 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA
testUpgradeWithJournalNodes(org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA)  Time elapsed: 1 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testUpgradeWithJournalNodes(TestDFSUpgradeWithHA.java:310)

testFinalizeWithJournalNodes(org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA)  Time elapsed: 0.644 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testFinalizeWithJournalNodes(TestDFSUpgradeWithHA.java:378)

testFinalizeFromSecondNameNodeWithJournalNodes(org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA)  Time elapsed: 0.664 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testFinalizeFromSecondNameNodeWithJournalNodes(TestDFSUpgradeWithHA.java:440)

testRollbackWithJournalNodes(org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA)  Time elapsed: 0.563 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes(TestDFSUpgradeWithHA.java:604)

Running org.apache.hadoop.hdfs.server.namenode.ha.TestFailureOfSharedDir
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.559 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestFailureOfSharedDir
testFailureOfSharedDir(org.apache.hadoop.hdfs.server.namenode.ha.TestFailureOfSharedDir)  Time elapsed: 9.786 sec  <<< FAILURE!
java.lang.AssertionError: Succeeded in rolling edit log despite shared dir being deleted
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestFailureOfSharedDir.testFailureOfSharedDir(TestFailureOfSharedDir.java:169)

Running org.apache.hadoop.hdfs.server.namenode.ha.TestQuotasWithHA
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.562 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestQuotasWithHA
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHAWebUI
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.539 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHAWebUI
Running org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 57.374 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
Running org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 7.345 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits
testFailuretoReadEdits[0](org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits)  Time elapsed: 3.309 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.setUpCluster(TestFailureToReadEdits.java:108)

testCheckpointStartingMidEditsFile[0](org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits)  Time elapsed: 0.541 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.setUpCluster(TestFailureToReadEdits.java:108)

testFailureToReadEditsOnTransitionToActive[0](org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits)  Time elapsed: 0.532 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.setUpCluster(TestFailureToReadEdits.java:108)

testFailuretoReadEdits[1](org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits)  Time elapsed: 1.3 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.setUpCluster(TestFailureToReadEdits.java:116)

testCheckpointStartingMidEditsFile[1](org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits)  Time elapsed: 0.822 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.setUpCluster(TestFailureToReadEdits.java:116)

testFailureToReadEditsOnTransitionToActive[1](org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits)  Time elapsed: 0.722 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.setUpCluster(TestFailureToReadEdits.java:116)

Running org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.089 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer
Running org.apache.hadoop.hdfs.server.namenode.ha.TestStateTransitionFailure
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.601 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestStateTransitionFailure
Running org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 5.009 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM
testBootstrapStandbyWithStandbyNN(org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM)  Time elapsed: 3.919 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM.setup(TestBootstrapStandbyWithQJM.java:55)

testBootstrapStandbyWithActiveNN(org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM)  Time elapsed: 1.006 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM.setup(TestBootstrapStandbyWithQJM.java:55)

Running org.apache.hadoop.hdfs.server.namenode.ha.TestLossyRetryInvocationHandler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.196 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestLossyRetryInvocationHandler
Running org.apache.hadoop.hdfs.server.namenode.ha.TestNNHealthCheck
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.578 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestNNHealthCheck
Running org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 95.285 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 80.603 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
Running org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogsDuringFailover
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.509 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogsDuringFailover
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.597 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA
Running org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.068 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA
Running org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 50.571 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHAFsck
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.797 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHAFsck
Running org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.606 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby
Running org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.836 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA
Running org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 105.161 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
Running org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.376 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot
Running org.apache.hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.035 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 102.988 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode
Running org.apache.hadoop.hdfs.server.namenode.TestFileLimit
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.36 sec - in org.apache.hadoop.hdfs.server.namenode.TestFileLimit
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourcePolicy
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.359 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourcePolicy
Running org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.483 sec - in org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings
Running org.apache.hadoop.hdfs.server.namenode.TestEditLogFileOutputStream
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.982 sec - in org.apache.hadoop.hdfs.server.namenode.TestEditLogFileOutputStream
Running org.apache.hadoop.hdfs.server.namenode.TestFSImageStorageInspector
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.462 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSImageStorageInspector
Running org.apache.hadoop.hdfs.server.namenode.TestAclConfigFlag
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.287 sec - in org.apache.hadoop.hdfs.server.namenode.TestAclConfigFlag
Running org.apache.hadoop.hdfs.server.namenode.TestFSNamesystemMBean
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.988 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSNamesystemMBean
Running org.apache.hadoop.hdfs.server.namenode.TestNameNodeRetryCacheMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.796 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameNodeRetryCacheMetrics
Running org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace
Tests run: 13, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 19.275 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace
testReinsertnamedirsInSavenamespace(org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace)  Time elapsed: 4.4 sec  <<< FAILURE!
java.lang.AssertionError: Savenamespace should have marked one directory as bad. But found 0 bad directories.
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.testReinsertnamedirsInSavenamespace(TestSaveNamespace.java:260)

Running org.apache.hadoop.hdfs.server.namenode.TestSnapshotPathINodes
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.571 sec - in org.apache.hadoop.hdfs.server.namenode.TestSnapshotPathINodes
Running org.apache.hadoop.hdfs.server.namenode.TestGenericJournalConf
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.645 sec - in org.apache.hadoop.hdfs.server.namenode.TestGenericJournalConf
Running org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.665 sec - in org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog
Running org.apache.hadoop.hdfs.server.namenode.TestAddBlock
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.522 sec - in org.apache.hadoop.hdfs.server.namenode.TestAddBlock
Running org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.792 sec - in org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport
Running org.apache.hadoop.hdfs.server.namenode.TestFSPermissionChecker
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.791 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSPermissionChecker
Running org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives
Tests run: 12, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 398.895 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives
testWaitForCachedReplicas(org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives)  Time elapsed: 65.238 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
Timestamp: 2015-01-22 01:30:11,932

"Thread-878" daemon prio=5 tid=1024 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=34 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@f293033f" daemon prio=5 tid=907 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"CacheReplicationMonitor(157262157)"  prio=5 tid=902 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2188)
        at org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:181)
"IPC Server idle connection scanner for port 42180" daemon prio=5 tid=939 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"Attach API wait loop" daemon prio=10 tid=46 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.tools.attach.javaSE.IPC.waitSemaphore(Native Method)
        at com.ibm.tools.attach.javaSE.CommonDirectory.waitSemaphore(CommonDirectory.java:193)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.waitForNotification(AttachHandler.java:352)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.run(AttachHandler.java:437)
"IPC Server handler 1 on 47501" daemon prio=5 tid=967 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 55727" daemon prio=5 tid=879 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server handler 9 on 47501" daemon prio=5 tid=975 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=13 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=41 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=40 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@e22b3aa6" daemon prio=5 tid=986 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 51477" daemon prio=5 tid=923 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"1350097327@qtp--2069308903-1 - Acceptor0 SelectChannelConnector@localhost:60797" daemon prio=5 tid=934 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=36 runnable
java.lang.Thread.State: RUNNABLE
"Socket Reader #1 for port 42180"  prio=5 tid=938 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 8 on 47501" daemon prio=5 tid=974 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@8c1aa638" daemon prio=5 tid=932 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=33 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server Responder" daemon prio=5 tid=882 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/]]  heartbeating to localhost/127.0.0.1:55727" daemon prio=5 tid=991 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 1 on 42180" daemon prio=5 tid=943 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"main"  prio=5 tid=1 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.lang.Thread.join(Thread.java:794)
        at java.lang.Thread.join(Thread.java:755)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:28)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
"-324246401@qtp-2075800498-1 - Acceptor0 SelectChannelConnector@localhost:52960" daemon prio=5 tid=909 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 1 on 43311" daemon prio=5 tid=993 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@dcf8a08b" daemon prio=5 tid=888 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor.run(DecommissionManager.java:76)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=29 runnable
java.lang.Thread.State: RUNNABLE
"1778589268@qtp-2075800498-0" daemon prio=5 tid=908 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 7 on 42180" daemon prio=5 tid=949 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Concurrent Mark Helper" daemon prio=1 tid=11 runnable
java.lang.Thread.State: RUNNABLE
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1-0" daemon prio=5 tid=1089 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"JIT Compilation Thread-2 Suspended" daemon prio=10 tid=6 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=14 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 5 on 43311" daemon prio=5 tid=997 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 43311" daemon prio=5 tid=987 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"GC Slave" daemon prio=5 tid=21 runnable
java.lang.Thread.State: RUNNABLE
"-1395440927@qtp-32495386-1 - Acceptor0 SelectChannelConnector@localhost:51249" daemon prio=5 tid=958 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 6 on 42180" daemon prio=5 tid=948 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=940 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1015 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"Finalizer thread" daemon prio=5 tid=45 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server idle connection scanner for port 47501" daemon prio=5 tid=963 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"JIT Compilation Thread-1 Suspended" daemon prio=10 tid=5 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@aaa07e40" daemon prio=5 tid=877 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3360)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 3 on 47501" daemon prio=5 tid=969 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=30 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=18 runnable
java.lang.Thread.State: RUNNABLE
"Timer-23" daemon prio=5 tid=910 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 0 on 47501" daemon prio=5 tid=966 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 51477" daemon prio=5 tid=914 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 0 on 51477" daemon prio=5 tid=917 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 55727" daemon prio=5 tid=897 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 2 on 42180" daemon prio=5 tid=944 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=15 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=39 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@939e322c" daemon prio=5 tid=900 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run(FSNamesystem.java:4327)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@3bc0daa5" daemon prio=5 tid=901 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller.run(FSNamesystem.java:4366)
        at java.lang.Thread.run(Thread.java:857)
"IProfiler" daemon prio=5 tid=10 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 2 on 55727" daemon prio=5 tid=891 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1566929582@qtp--2124652438-1 - Acceptor0 SelectChannelConnector@localhost:44024" daemon prio=5 tid=875 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 4 on 51477" daemon prio=5 tid=921 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=25 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=42 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 43311" daemon prio=5 tid=992 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 47501" daemon prio=5 tid=970 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor@7fc44e94" daemon prio=5 tid=887 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:221)
        at java.lang.Thread.run(Thread.java:857)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:55727" daemon prio=5 tid=965 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 3 on 55727" daemon prio=5 tid=892 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 43311" daemon prio=5 tid=999 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Timer-26" daemon prio=5 tid=985 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 9 on 42180" daemon prio=5 tid=951 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 0 on 55727" daemon prio=5 tid=889 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 43311" daemon prio=5 tid=996 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 43311" daemon prio=5 tid=1000 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 42180" daemon prio=5 tid=937 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"java.util.concurrent.ThreadPoolExecutor$Worker@e45cd713[State = -1, empty queue]" daemon prio=5 tid=1030 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@7cd2546c" daemon prio=5 tid=936 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 5 on 42180" daemon prio=5 tid=947 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=990 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1053 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 43311" daemon prio=5 tid=998 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 51477" daemon prio=5 tid=918 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 9 on 51477" daemon prio=5 tid=927 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=27 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=20 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server listener on 51477" daemon prio=5 tid=912 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"JIT-SamplerThread" daemon prio=10 tid=9 timed_waiting
java.lang.Thread.State: TIMED_WAITING
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1016 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 4 on 42180" daemon prio=5 tid=946 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1471718508@qtp-32495386-0" daemon prio=5 tid=957 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 2 on 47501" daemon prio=5 tid=968 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Diagnostic Compilation Thread-4 Suspended" daemon prio=10 tid=8 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=24 runnable
java.lang.Thread.State: RUNNABLE
"Timer for 'NameNode' metrics system" daemon prio=5 tid=640 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 6 on 47501" daemon prio=5 tid=972 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=32 runnable
java.lang.Thread.State: RUNNABLE
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1042 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 3 on 51477" daemon prio=5 tid=920 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1037 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 55727" daemon prio=5 tid=896 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Socket Reader #1 for port 55727"  prio=5 tid=880 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"java.util.concurrent.ThreadPoolExecutor$Worker@90d859d6[State = -1, empty queue]" daemon prio=5 tid=1010 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 8 on 51477" daemon prio=5 tid=926 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=31 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=19 runnable
java.lang.Thread.State: RUNNABLE
"MemoryPoolMXBean notification dispatcher" daemon prio=6 tid=54 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.lang.management.MemoryNotificationThread.processNotificationLoop(Native Method)
        at com.ibm.lang.management.MemoryNotificationThread.run(MemoryNotificationThread.java:64)
"IPC Server handler 8 on 42180" daemon prio=5 tid=950 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 55727" daemon prio=5 tid=890 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Thread-905"  prio=5 tid=1057 runnable
java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.getStackTraceImpl(Native Method)
        at java.lang.Thread.getStackTrace(Thread.java:1207)
        at java.lang.Thread.getAllStackTraces(Thread.java:1235)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:86)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:72)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testWaitForCachedReplicas(TestCacheDirectives.java:878)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
        at java.lang.reflect.Method.invoke(Method.java:620)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
"IPC Server handler 0 on 42180" daemon prio=5 tid=942 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1039 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server idle connection scanner for port 55727" daemon prio=5 tid=881 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 2 on 43311" daemon prio=5 tid=994 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Parameter Sending Thread #1" daemon prio=5 tid=784 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:472)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:371)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:954)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@456c26be" daemon prio=5 tid=883 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server idle connection scanner for port 43311" daemon prio=5 tid=989 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 7 on 47501" daemon prio=5 tid=973 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=16 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 2 on 51477" daemon prio=5 tid=919 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Client (-772539878) connection to localhost/127.0.0.1:55727 from root" daemon prio=5 tid=925 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:903)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:948)
"-199443046@qtp--1778813282-1 - Acceptor0 SelectChannelConnector@localhost:48463" daemon prio=5 tid=984 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=38 runnable
java.lang.Thread.State: RUNNABLE
"Timer-25" daemon prio=5 tid=959 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"Socket Reader #1 for port 43311"  prio=5 tid=988 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"1283199306@qtp--2069308903-0" daemon prio=5 tid=933 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"GC Slave" daemon prio=5 tid=37 runnable
java.lang.Thread.State: RUNNABLE
"7846669@qtp--2124652438-0" daemon prio=5 tid=874 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@87b91529" daemon prio=5 tid=899 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:392)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=23 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 9 on 55727" daemon prio=5 tid=898 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 55727" daemon prio=5 tid=893 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 5 on 55727" daemon prio=5 tid=894 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Socket Reader #1 for port 47501"  prio=5 tid=962 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 5 on 47501" daemon prio=5 tid=971 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"java.util.concurrent.ThreadPoolExecutor$Worker@b3cb7815[State = -1, empty queue]" daemon prio=5 tid=1048 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=35 runnable
java.lang.Thread.State: RUNNABLE
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1038 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"Signal Dispatcher" daemon prio=5 tid=2 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.misc.SignalDispatcher.waitForSignal(Native Method)
        at com.ibm.misc.SignalDispatcher.run(SignalDispatcher.java:75)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@d03046eb" daemon prio=5 tid=960 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"java.util.concurrent.ThreadPoolExecutor$Worker@70cffe17[State = -1, empty queue]" daemon prio=5 tid=1025 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 5 on 51477" daemon prio=5 tid=922 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1135316616-192.168.250.172-1421890148097" daemon prio=5 tid=1054 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8-0" daemon prio=5 tid=1090 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=26 runnable
java.lang.Thread.State: RUNNABLE
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:55727" daemon prio=5 tid=916 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"Timer-24" daemon prio=5 tid=935 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@425a43e3" daemon prio=5 tid=956 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"JIT Compilation Thread-3 Suspended" daemon prio=10 tid=7 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5aa4fcd6" daemon prio=5 tid=980 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=28 runnable
java.lang.Thread.State: RUNNABLE
"JIT Compilation Thread-0" daemon prio=10 tid=4 runnable
java.lang.Thread.State: RUNNABLE
"Thread-895" daemon prio=5 tid=1045 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@270fb881" daemon prio=5 tid=911 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server listener on 47501" daemon prio=5 tid=961 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server handler 7 on 51477" daemon prio=5 tid=924 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Thread-875" daemon prio=5 tid=1021 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:55727" daemon prio=5 tid=941 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server Responder" daemon prio=5 tid=964 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 6 on 55727" daemon prio=5 tid=895 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 42180" daemon prio=5 tid=945 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=915 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"Timer-22" daemon prio=5 tid=876 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=17 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@43c4324a" daemon prio=5 tid=878 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run(HeartbeatManager.java:307)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 9 on 43311" daemon prio=5 tid=1001 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Thread-863" daemon prio=5 tid=1007 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=22 runnable
java.lang.Thread.State: RUNNABLE
"Socket Reader #1 for port 51477"  prio=5 tid=913 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"GC Slave" daemon prio=5 tid=12 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 3 on 43311" daemon prio=5 tid=995 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testWaitForCachedReplicas(TestCacheDirectives.java:878)

testWaitForCachedReplicasInDirectory(org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives)  Time elapsed: 62.731 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
Timestamp: 2015-01-22 01:31:15,195

"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1237 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"-2086295777@qtp-1628634949-1 - Acceptor0 SelectChannelConnector@localhost:44376" daemon prio=5 tid=1132 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 3 on 51653" daemon prio=5 tid=1194 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Client (-772539878) connection to localhost/127.0.0.1:35536 from root" daemon prio=5 tid=1144 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:903)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:948)
"IPC Server handler 2 on 51653" daemon prio=5 tid=1193 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=34 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 6 on 35536" daemon prio=5 tid=1118 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Timer-31" daemon prio=5 tid=1208 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:35536" daemon prio=5 tid=1190 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"Attach API wait loop" daemon prio=10 tid=46 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.tools.attach.javaSE.IPC.waitSemaphore(Native Method)
        at com.ibm.tools.attach.javaSE.CommonDirectory.waitSemaphore(CommonDirectory.java:193)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.waitForNotification(AttachHandler.java:352)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.run(AttachHandler.java:437)
"org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@1f1bd8a9" daemon prio=5 tid=1122 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:392)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 5 on 51653" daemon prio=5 tid=1196 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5-0" daemon prio=5 tid=1345 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/]]  heartbeating to localhost/127.0.0.1:35536" daemon prio=5 tid=1216 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@73fc15aa" daemon prio=5 tid=1210 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1074" daemon prio=5 tid=1243 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 1 on 46992" daemon prio=5 tid=1166 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 46992" daemon prio=5 tid=1168 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=13 runnable
java.lang.Thread.State: RUNNABLE
"java.util.concurrent.ThreadPoolExecutor$Worker@4e704d11[State = -1, empty queue]" daemon prio=5 tid=1260 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 0 on 35536" daemon prio=5 tid=1112 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 33195" daemon prio=5 tid=1141 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=41 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=40 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server listener on 35536" daemon prio=5 tid=1101 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server handler 2 on 46992" daemon prio=5 tid=1167 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=36 runnable
java.lang.Thread.State: RUNNABLE
"Socket Reader #1 for port 46992"  prio=5 tid=1161 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"Socket Reader #1 for port 51653"  prio=5 tid=1187 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4-0" daemon prio=5 tid=1344 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=33 runnable
java.lang.Thread.State: RUNNABLE
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:35536" daemon prio=5 tid=1139 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server listener on 33773" daemon prio=5 tid=1211 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"CacheReplicationMonitor(-2116226012)"  prio=5 tid=1125 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2188)
        at org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:181)
"IPC Server handler 2 on 35536" daemon prio=5 tid=1114 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor@86a58fd6" daemon prio=5 tid=1110 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:221)
        at java.lang.Thread.run(Thread.java:857)
"-1102554786@qtp-1295702558-1 - Acceptor0 SelectChannelConnector@localhost:50215" daemon prio=5 tid=1097 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@fea717b0" daemon prio=5 tid=1105 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1252 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"main"  prio=5 tid=1 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.lang.Thread.join(Thread.java:794)
        at java.lang.Thread.join(Thread.java:755)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:28)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
"Timer-29" daemon prio=5 tid=1158 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"1916896551@qtp-466171207-0" daemon prio=5 tid=1156 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"769417005@qtp--695866827-0" daemon prio=5 tid=1206 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"GC Slave" daemon prio=5 tid=29 runnable
java.lang.Thread.State: RUNNABLE
"java.util.concurrent.ThreadPoolExecutor$Worker@96cb6314[State = -1, empty queue]" daemon prio=5 tid=1231 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"Concurrent Mark Helper" daemon prio=1 tid=11 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 9 on 46992" daemon prio=5 tid=1174 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Compilation Thread-2 Suspended" daemon prio=10 tid=6 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=14 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 6 on 33195" daemon prio=5 tid=1147 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 46992" daemon prio=5 tid=1172 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 9 on 35536" daemon prio=5 tid=1121 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 33195" daemon prio=5 tid=1135 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5f4ffd1d" daemon prio=5 tid=1155 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=21 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 4 on 51653" daemon prio=5 tid=1195 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Socket Reader #1 for port 33773"  prio=5 tid=1212 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 2 on 33773" daemon prio=5 tid=1219 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 0 on 51653" daemon prio=5 tid=1191 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Finalizer thread" daemon prio=5 tid=45 runnable
java.lang.Thread.State: RUNNABLE
"Timer-30" daemon prio=5 tid=1184 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 5 on 35536" daemon prio=5 tid=1117 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 33195" daemon prio=5 tid=1148 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Compilation Thread-1 Suspended" daemon prio=10 tid=5 runnable
java.lang.Thread.State: RUNNABLE
"1250257954@qtp-1628634949-0" daemon prio=5 tid=1131 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"GC Slave" daemon prio=5 tid=30 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=18 runnable
java.lang.Thread.State: RUNNABLE
"Timer-27" daemon prio=5 tid=1098 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7-0" daemon prio=5 tid=1347 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=15 runnable
java.lang.Thread.State: RUNNABLE
"Thread-1106"  prio=5 tid=1281 runnable
java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.getStackTraceImpl(Native Method)
        at java.lang.Thread.getStackTrace(Thread.java:1207)
        at java.lang.Thread.getAllStackTraces(Thread.java:1235)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:86)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:72)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testWaitForCachedReplicasInDirectory(TestCacheDirectives.java:940)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
        at java.lang.reflect.Method.invoke(Method.java:620)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
"GC Slave" daemon prio=5 tid=39 runnable
java.lang.Thread.State: RUNNABLE
"IProfiler" daemon prio=5 tid=10 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server idle connection scanner for port 35536" daemon prio=5 tid=1103 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@7578403b" daemon prio=5 tid=1130 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 4 on 35536" daemon prio=5 tid=1116 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=25 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=42 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@e48536cd" daemon prio=5 tid=1099 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3360)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server listener on 51653" daemon prio=5 tid=1186 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server handler 1 on 33773" daemon prio=5 tid=1218 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3-0" daemon prio=5 tid=1343 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 51653" daemon prio=5 tid=1198 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 5 on 46992" daemon prio=5 tid=1170 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 33195" daemon prio=5 tid=1143 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=1163 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server Responder" daemon prio=5 tid=1215 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 9 on 33773" daemon prio=5 tid=1226 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1231668667@qtp-466171207-1 - Acceptor0 SelectChannelConnector@localhost:42436" daemon prio=5 tid=1157 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1266 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 46992" daemon prio=5 tid=1171 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1278 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server idle connection scanner for port 33195" daemon prio=5 tid=1137 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 4 on 33195" daemon prio=5 tid=1145 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 35536" daemon prio=5 tid=1113 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 35536" daemon prio=5 tid=1115 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=27 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@c69b410d" daemon prio=5 tid=1100 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run(HeartbeatManager.java:307)
        at java.lang.Thread.run(Thread.java:857)
"-882703214@qtp-1604141812-0" daemon prio=5 tid=1180 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1238 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=20 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server idle connection scanner for port 46992" daemon prio=5 tid=1162 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1251 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"JIT-SamplerThread" daemon prio=10 tid=9 timed_waiting
java.lang.Thread.State: TIMED_WAITING
"IPC Server handler 6 on 51653" daemon prio=5 tid=1197 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 5 on 33773" daemon prio=5 tid=1222 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 33773" daemon prio=5 tid=1220 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 35536" daemon prio=5 tid=1119 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=1189 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6-0" daemon prio=5 tid=1346 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"-1880647733@qtp-1295702558-0" daemon prio=5 tid=1096 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"JIT Diagnostic Compilation Thread-4 Suspended" daemon prio=10 tid=8 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=24 runnable
java.lang.Thread.State: RUNNABLE
"Timer for 'NameNode' metrics system" daemon prio=5 tid=640 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=32 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 46992" daemon prio=5 tid=1165 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@ae66deb" daemon prio=5 tid=1111 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor.run(DecommissionManager.java:76)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@ba5fad1c" daemon prio=5 tid=1159 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 33773" daemon prio=5 tid=1223 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-2096162869@qtp-1604141812-1 - Acceptor0 SelectChannelConnector@localhost:59769" daemon prio=5 tid=1181 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 8 on 33195" daemon prio=5 tid=1149 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"java.util.concurrent.ThreadPoolExecutor$Worker@a93108e4[State = -1, empty queue]" daemon prio=5 tid=1272 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 8 on 35536" daemon prio=5 tid=1120 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@192d970b" daemon prio=5 tid=1185 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@ee310859" daemon prio=5 tid=1179 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=31 runnable
java.lang.Thread.State: RUNNABLE
"Thread-1086" daemon prio=5 tid=1257 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=19 runnable
java.lang.Thread.State: RUNNABLE
"MemoryPoolMXBean notification dispatcher" daemon prio=6 tid=54 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.lang.management.MemoryNotificationThread.processNotificationLoop(Native Method)
        at com.ibm.lang.management.MemoryNotificationThread.run(MemoryNotificationThread.java:64)
"IPC Server handler 8 on 46992" daemon prio=5 tid=1173 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"LeaseRenewer:root@localhost:35536" daemon prio=5 tid=1283 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:438)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 2 on 33195" daemon prio=5 tid=1142 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Socket Reader #1 for port 35536"  prio=5 tid=1102 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 9 on 51653" daemon prio=5 tid=1200 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 33773" daemon prio=5 tid=1213 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"Timer-28" daemon prio=5 tid=1133 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Parameter Sending Thread #1" daemon prio=5 tid=784 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:472)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:371)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:954)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"Socket Reader #1 for port 33195"  prio=5 tid=1136 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 5 on 33195" daemon prio=5 tid=1146 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 33773" daemon prio=5 tid=1225 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=16 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 9 on 33195" daemon prio=5 tid=1150 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:35536" daemon prio=5 tid=1164 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server Responder" daemon prio=5 tid=1138 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 7 on 33773" daemon prio=5 tid=1224 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=38 runnable
java.lang.Thread.State: RUNNABLE
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1277 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"1641721693@qtp--695866827-1 - Acceptor0 SelectChannelConnector@localhost:39198" daemon prio=5 tid=1207 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=37 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=23 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@e08eeb55" daemon prio=5 tid=1134 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 0 on 33773" daemon prio=5 tid=1217 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=35 runnable
java.lang.Thread.State: RUNNABLE
"Thread-1096" daemon prio=5 tid=1269 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"Signal Dispatcher" daemon prio=5 tid=2 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.misc.SignalDispatcher.waitForSignal(Native Method)
        at com.ibm.misc.SignalDispatcher.run(SignalDispatcher.java:75)
"IPC Server handler 4 on 33773" daemon prio=5 tid=1221 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=26 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 4 on 46992" daemon prio=5 tid=1169 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 51653" daemon prio=5 tid=1188 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"JIT Compilation Thread-3 Suspended" daemon prio=10 tid=7 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=28 runnable
java.lang.Thread.State: RUNNABLE
"JIT Compilation Thread-0" daemon prio=10 tid=4 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 8 on 51653" daemon prio=5 tid=1199 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@e261cefb" daemon prio=5 tid=1124 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller.run(FSNamesystem.java:4366)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server Responder" daemon prio=5 tid=1104 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server listener on 46992" daemon prio=5 tid=1160 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server handler 1 on 51653" daemon prio=5 tid=1192 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1023043491-192.168.250.172-1421890213302" daemon prio=5 tid=1265 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1062" daemon prio=5 tid=1228 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=17 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 33195" daemon prio=5 tid=1140 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@e2b76052" daemon prio=5 tid=1205 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@682041ff" daemon prio=5 tid=1123 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run(FSNamesystem.java:4327)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=22 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=12 runnable
java.lang.Thread.State: RUNNABLE
"java.util.concurrent.ThreadPoolExecutor$Worker@14d54736[State = -1, empty queue]" daemon prio=5 tid=1246 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testWaitForCachedReplicasInDirectory(TestCacheDirectives.java:940)

testReplicationFactor(org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives)  Time elapsed: 62.793 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
Timestamp: 2015-01-22 01:32:18,053

"GC Slave" daemon prio=5 tid=34 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 9 on 50242" daemon prio=5 tid=1407 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=1361 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@e0a19918" daemon prio=5 tid=1380 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run(FSNamesystem.java:4327)
        at java.lang.Thread.run(Thread.java:857)
"Attach API wait loop" daemon prio=10 tid=46 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.tools.attach.javaSE.IPC.waitSemaphore(Native Method)
        at com.ibm.tools.attach.javaSE.CommonDirectory.waitSemaphore(CommonDirectory.java:193)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.waitForNotification(AttachHandler.java:352)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.run(AttachHandler.java:437)
"Timer-33" daemon prio=5 tid=1390 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server Responder" daemon prio=5 tid=1470 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"-1406036825@qtp-221143800-0" daemon prio=5 tid=1388 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 1 on 54514" daemon prio=5 tid=1370 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"java.util.concurrent.ThreadPoolExecutor$Worker@3c7024a5[State = -1, empty queue]" daemon prio=5 tid=1488 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 1 on 49427" daemon prio=5 tid=1449 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Timer-36" daemon prio=5 tid=1465 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=13 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 8 on 50242" daemon prio=5 tid=1406 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 49427" daemon prio=5 tid=1443 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"GC Slave" daemon prio=5 tid=41 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=40 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 50242" daemon prio=5 tid=1397 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 44050" daemon prio=5 tid=1430 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1092594022@qtp-1968822671-1 - Acceptor0 SelectChannelConnector@localhost:54167" daemon prio=5 tid=1354 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@8e4d2c8a" daemon prio=5 tid=1391 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=36 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 44050" daemon prio=5 tid=1422 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=33 runnable
java.lang.Thread.State: RUNNABLE
"Timer-32" daemon prio=5 tid=1355 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 7 on 49427" daemon prio=5 tid=1455 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7-0" daemon prio=5 tid=1603 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"main"  prio=5 tid=1 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.lang.Thread.join(Thread.java:794)
        at java.lang.Thread.join(Thread.java:755)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:28)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
"IPC Server handler 5 on 54514" daemon prio=5 tid=1374 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=29 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server Responder" daemon prio=5 tid=1395 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"Concurrent Mark Helper" daemon prio=1 tid=11 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor@4b4aba68" daemon prio=5 tid=1367 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:221)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server listener on 50242" daemon prio=5 tid=1392 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"-650858410@qtp--1788807173-1 - Acceptor0 SelectChannelConnector@localhost:59625" daemon prio=5 tid=1438 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"JIT Compilation Thread-2 Suspended" daemon prio=10 tid=6 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=14 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 54514" daemon prio=5 tid=1369 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 44050" daemon prio=5 tid=1423 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 35213" daemon prio=5 tid=1467 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"GC Slave" daemon prio=5 tid=21 runnable
java.lang.Thread.State: RUNNABLE
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6-0" daemon prio=5 tid=1600 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 9 on 54514" daemon prio=5 tid=1378 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1520 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3-0" daemon prio=5 tid=1602 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 4 on 50242" daemon prio=5 tid=1401 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 35213" daemon prio=5 tid=1480 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Finalizer thread" daemon prio=5 tid=45 runnable
java.lang.Thread.State: RUNNABLE
"JIT Compilation Thread-1 Suspended" daemon prio=10 tid=5 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@a1be22a7" daemon prio=5 tid=1362 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1521 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 0 on 49427" daemon prio=5 tid=1448 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 9 on 35213" daemon prio=5 tid=1481 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=30 runnable
java.lang.Thread.State: RUNNABLE
"Timer-34" daemon prio=5 tid=1415 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=18 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 2 on 44050" daemon prio=5 tid=1424 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@2f45ea60" daemon prio=5 tid=1381 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller.run(FSNamesystem.java:4366)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 3 on 50242" daemon prio=5 tid=1400 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=15 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=39 runnable
java.lang.Thread.State: RUNNABLE
"Socket Reader #1 for port 54514"  prio=5 tid=1359 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"Thread-1337"  prio=5 tid=1538 runnable
java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.getStackTraceImpl(Native Method)
        at java.lang.Thread.getStackTrace(Thread.java:1207)
        at java.lang.Thread.getAllStackTraces(Thread.java:1235)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:86)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:72)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testReplicationFactor(TestCacheDirectives.java:1032)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
        at java.lang.reflect.Method.invoke(Method.java:620)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
"IProfiler" daemon prio=5 tid=10 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 6 on 35213" daemon prio=5 tid=1478 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=25 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=42 runnable
java.lang.Thread.State: RUNNABLE
"-1437256852@qtp-1928225063-0" daemon prio=5 tid=1463 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 7 on 35213" daemon prio=5 tid=1479 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Client (-772539878) connection to localhost/127.0.0.1:54514 from root" daemon prio=5 tid=1403 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:903)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:948)
"IPC Server idle connection scanner for port 49427" daemon prio=5 tid=1445 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 7 on 44050" daemon prio=5 tid=1429 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 35213" daemon prio=5 tid=1469 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 4 on 54514" daemon prio=5 tid=1373 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 0 on 35213" daemon prio=5 tid=1472 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 54514" daemon prio=5 tid=1358 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"Timer-35" daemon prio=5 tid=1441 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 4 on 44050" daemon prio=5 tid=1426 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=27 runnable
java.lang.Thread.State: RUNNABLE
"-337417457@qtp-1968822671-0" daemon prio=5 tid=1353 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 6 on 44050" daemon prio=5 tid=1428 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Socket Reader #1 for port 49427"  prio=5 tid=1444 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 6 on 49427" daemon prio=5 tid=1454 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@96b92f0c" daemon prio=5 tid=1466 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"java.util.concurrent.ThreadPoolExecutor$Worker@3ed2864[State = -1, empty queue]" daemon prio=5 tid=1515 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1508 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 3 on 35213" daemon prio=5 tid=1475 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=20 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server idle connection scanner for port 54514" daemon prio=5 tid=1360 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"JIT-SamplerThread" daemon prio=10 tid=9 timed_waiting
java.lang.Thread.State: TIMED_WAITING
"IPC Server handler 2 on 49427" daemon prio=5 tid=1450 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/]]  heartbeating to localhost/127.0.0.1:54514" daemon prio=5 tid=1471 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 5 on 35213" daemon prio=5 tid=1477 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 54514" daemon prio=5 tid=1377 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 50242" daemon prio=5 tid=1394 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@a2bc8c7b" daemon prio=5 tid=1442 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server Responder" daemon prio=5 tid=1420 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"Thread-1305" daemon prio=5 tid=1500 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1509 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"Socket Reader #1 for port 35213"  prio=5 tid=1468 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@f1089eae" daemon prio=5 tid=1387 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@57b6da62" daemon prio=5 tid=1368 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor.run(DecommissionManager.java:76)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 2 on 35213" daemon prio=5 tid=1474 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 49427" daemon prio=5 tid=1452 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Diagnostic Compilation Thread-4 Suspended" daemon prio=10 tid=8 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=24 runnable
java.lang.Thread.State: RUNNABLE
"Timer for 'NameNode' metrics system" daemon prio=5 tid=640 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=32 runnable
java.lang.Thread.State: RUNNABLE
"438045471@qtp-1928225063-1 - Acceptor0 SelectChannelConnector@localhost:55967" daemon prio=5 tid=1464 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 4 on 35213" daemon prio=5 tid=1476 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@60dd6fc9" daemon prio=5 tid=1436 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 54514" daemon prio=5 tid=1376 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Thread-1293" daemon prio=5 tid=1485 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 9 on 49427" daemon prio=5 tid=1457 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 5 on 49427" daemon prio=5 tid=1453 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Thread-1315" daemon prio=5 tid=1512 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1534 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 1 on 50242" daemon prio=5 tid=1398 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=31 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=19 runnable
java.lang.Thread.State: RUNNABLE
"MemoryPoolMXBean notification dispatcher" daemon prio=6 tid=54 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.lang.management.MemoryNotificationThread.processNotificationLoop(Native Method)
        at com.ibm.lang.management.MemoryNotificationThread.run(MemoryNotificationThread.java:64)
"CacheReplicationMonitor(-394238761)"  prio=5 tid=1382 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2188)
        at org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:181)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@f17aefe5" daemon prio=5 tid=1462 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:54514" daemon prio=5 tid=1447 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"java.util.concurrent.ThreadPoolExecutor$Worker@9985672c[State = -1, empty queue]" daemon prio=5 tid=1503 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Parameter Sending Thread #1" daemon prio=5 tid=784 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:472)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:371)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:954)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server Responder" daemon prio=5 tid=1446 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 5 on 44050" daemon prio=5 tid=1427 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=16 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 1 on 35213" daemon prio=5 tid=1473 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Socket Reader #1 for port 44050"  prio=5 tid=1418 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 3 on 54514" daemon prio=5 tid=1372 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@2d66d1ce" daemon prio=5 tid=1357 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run(HeartbeatManager.java:307)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 2 on 54514" daemon prio=5 tid=1371 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 44050" daemon prio=5 tid=1417 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"GC Slave" daemon prio=5 tid=38 runnable
java.lang.Thread.State: RUNNABLE
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1535 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1495 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=37 runnable
java.lang.Thread.State: RUNNABLE
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:54514" daemon prio=5 tid=1396 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=23 runnable
java.lang.Thread.State: RUNNABLE
"464778275@qtp-221143800-1 - Acceptor0 SelectChannelConnector@localhost:43253" daemon prio=5 tid=1389 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 8 on 49427" daemon prio=5 tid=1456 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@a9a2fb5a" daemon prio=5 tid=1416 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=35 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@f97d7f24" daemon prio=5 tid=1412 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"Signal Dispatcher" daemon prio=5 tid=2 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.misc.SignalDispatcher.waitForSignal(Native Method)
        at com.ibm.misc.SignalDispatcher.run(SignalDispatcher.java:75)
"org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@5829d9c8" daemon prio=5 tid=1356 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3360)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@948029cc" daemon prio=5 tid=1379 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:392)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 5 on 50242" daemon prio=5 tid=1402 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=26 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 3 on 49427" daemon prio=5 tid=1451 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Compilation Thread-3 Suspended" daemon prio=10 tid=7 runnable
java.lang.Thread.State: RUNNABLE
"-1689103804@qtp--2104050134-1 - Acceptor0 SelectChannelConnector@localhost:48845" daemon prio=5 tid=1414 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=28 runnable
java.lang.Thread.State: RUNNABLE
"JIT Compilation Thread-0" daemon prio=10 tid=4 runnable
java.lang.Thread.State: RUNNABLE
"Thread-1327" daemon prio=5 tid=1526 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1-0" daemon prio=5 tid=1601 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-127200820-192.168.250.172-1421890276033" daemon prio=5 tid=1494 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"Socket Reader #1 for port 50242"  prio=5 tid=1393 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 3 on 44050" daemon prio=5 tid=1425 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:54514" daemon prio=5 tid=1421 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"LeaseRenewer:root@localhost:54514" daemon prio=5 tid=1540 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:438)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:857)
"-1583660850@qtp--2104050134-0" daemon prio=5 tid=1413 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 2 on 50242" daemon prio=5 tid=1399 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 50242" daemon prio=5 tid=1405 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 6 on 54514" daemon prio=5 tid=1375 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=17 runnable
java.lang.Thread.State: RUNNABLE
"java.util.concurrent.ThreadPoolExecutor$Worker@dac382e4[State = -1, empty queue]" daemon prio=5 tid=1529 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server idle connection scanner for port 44050" daemon prio=5 tid=1419 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=22 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=12 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 6 on 50242" daemon prio=5 tid=1404 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 9 on 44050" daemon prio=5 tid=1431 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1980687987@qtp--1788807173-0" daemon prio=5 tid=1437 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testReplicationFactor(TestCacheDirectives.java:1032)

testExpiry(org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives)  Time elapsed: 62.832 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
Timestamp: 2015-01-22 01:33:21,993

"IPC Server handler 8 on 45141" daemon prio=5 tid=1878 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=34 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 3 on 43410" daemon prio=5 tid=1899 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1981 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"Attach API wait loop" daemon prio=10 tid=46 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.tools.attach.javaSE.IPC.waitSemaphore(Native Method)
        at com.ibm.tools.attach.javaSE.CommonDirectory.waitSemaphore(CommonDirectory.java:193)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.waitForNotification(AttachHandler.java:352)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.run(AttachHandler.java:437)
"IPC Server handler 0 on 45141" daemon prio=5 tid=1870 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 38627" daemon prio=5 tid=1818 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 56058" daemon prio=5 tid=1846 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Timer-43" daemon prio=5 tid=1838 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 1 on 43410" daemon prio=5 tid=1897 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=13 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=41 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 4 on 43410" daemon prio=5 tid=1900 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=40 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@63048f40" daemon prio=5 tid=1805 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3360)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 8 on 43410" daemon prio=5 tid=1904 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 0 on 44409" daemon prio=5 tid=1920 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=36 runnable
java.lang.Thread.State: RUNNABLE
"Timer-45" daemon prio=5 tid=1889 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"Thread-1718" daemon prio=5 tid=1959 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 38627" daemon prio=5 tid=1824 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 43410" daemon prio=5 tid=1893 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=33 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server Responder" daemon prio=5 tid=1894 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"Socket Reader #1 for port 45141"  prio=5 tid=1866 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"java.util.concurrent.ThreadPoolExecutor$Worker@7fbc815f[State = -1, empty queue]" daemon prio=5 tid=1950 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@6415a3ca" daemon prio=5 tid=1828 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run(FSNamesystem.java:4327)
        at java.lang.Thread.run(Thread.java:857)
"1404641353@qtp-302178362-1 - Acceptor0 SelectChannelConnector@localhost:44756" daemon prio=5 tid=1886 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"main"  prio=5 tid=1 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.lang.Thread.join(Thread.java:794)
        at java.lang.Thread.join(Thread.java:755)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:28)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
"IPC Server handler 5 on 38627" daemon prio=5 tid=1822 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1982 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=29 runnable
java.lang.Thread.State: RUNNABLE
"Concurrent Mark Helper" daemon prio=1 tid=11 runnable
java.lang.Thread.State: RUNNABLE
"Socket Reader #1 for port 56058"  prio=5 tid=1841 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:38627" daemon prio=5 tid=1869 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"JIT Compilation Thread-2 Suspended" daemon prio=10 tid=6 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=14 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server Responder" daemon prio=5 tid=1868 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 4 on 56058" daemon prio=5 tid=1849 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=21 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server Responder" daemon prio=5 tid=1843 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 7 on 43410" daemon prio=5 tid=1903 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Finalizer thread" daemon prio=5 tid=45 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 8 on 44409" daemon prio=5 tid=1928 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Compilation Thread-1 Suspended" daemon prio=10 tid=5 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 5 on 56058" daemon prio=5 tid=1850 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 38627" daemon prio=5 tid=1807 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1-0" daemon prio=5 tid=1998 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=30 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=18 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 2 on 45141" daemon prio=5 tid=1872 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@fc40071c" daemon prio=5 tid=1806 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run(HeartbeatManager.java:307)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@42ce9085" daemon prio=5 tid=1839 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=15 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server idle connection scanner for port 44409" daemon prio=5 tid=1917 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=39 runnable
java.lang.Thread.State: RUNNABLE
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4-0" daemon prio=5 tid=2000 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IProfiler" daemon prio=5 tid=10 runnable
java.lang.Thread.State: RUNNABLE
"java.util.concurrent.ThreadPoolExecutor$Worker@9778668b[State = -1, empty queue]" daemon prio=5 tid=1936 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@b99ae334" daemon prio=5 tid=1827 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:392)
        at java.lang.Thread.run(Thread.java:857)
"Socket Reader #1 for port 38627"  prio=5 tid=1808 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server handler 3 on 56058" daemon prio=5 tid=1848 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=25 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=42 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 2 on 56058" daemon prio=5 tid=1847 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 38627" daemon prio=5 tid=1825 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 45141" daemon prio=5 tid=1877 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 2 on 44409" daemon prio=5 tid=1922 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1396317031@qtp--1163671201-1 - Acceptor0 SelectChannelConnector@localhost:37056" daemon prio=5 tid=1837 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 9 on 44409" daemon prio=5 tid=1929 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Timer-44" daemon prio=5 tid=1863 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@f17e69e4" daemon prio=5 tid=1816 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor.run(DecommissionManager.java:76)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 0 on 56058" daemon prio=5 tid=1845 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 45141" daemon prio=5 tid=1874 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1941 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1942 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 56058" daemon prio=5 tid=1851 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"1869241965@qtp-289328859-0" daemon prio=5 tid=1861 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 9 on 43410" daemon prio=5 tid=1905 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 45141" daemon prio=5 tid=1873 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@ab43591b" daemon prio=5 tid=1890 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1708" daemon prio=5 tid=1947 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=27 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@13745c7c" daemon prio=5 tid=1910 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 2 on 43410" daemon prio=5 tid=1898 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1956 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 4 on 38627" daemon prio=5 tid=1821 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=20 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 9 on 56058" daemon prio=5 tid=1855 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 44409" daemon prio=5 tid=1927 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT-SamplerThread" daemon prio=10 tid=9 timed_waiting
java.lang.Thread.State: TIMED_WAITING
"IPC Server listener on 43410" daemon prio=5 tid=1891 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server listener on 56058" daemon prio=5 tid=1840 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7-0" daemon prio=5 tid=2001 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 38627" daemon prio=5 tid=1823 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Diagnostic Compilation Thread-4 Suspended" daemon prio=10 tid=8 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=24 runnable
java.lang.Thread.State: RUNNABLE
"Timer for 'NameNode' metrics system" daemon prio=5 tid=640 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server idle connection scanner for port 56058" daemon prio=5 tid=1842 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 1 on 45141" daemon prio=5 tid=1871 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=32 runnable
java.lang.Thread.State: RUNNABLE
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1955 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server listener on 45141" daemon prio=5 tid=1865 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server Responder" daemon prio=5 tid=1810 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"Timer-42" daemon prio=5 tid=1804 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"1552137376@qtp--1163671201-0" daemon prio=5 tid=1836 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 0 on 38627" daemon prio=5 tid=1817 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 2 on 38627" daemon prio=5 tid=1819 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 0 on 43410" daemon prio=5 tid=1896 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 5 on 44409" daemon prio=5 tid=1925 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 44409" daemon prio=5 tid=1915 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:38627" daemon prio=5 tid=1844 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=31 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=19 runnable
java.lang.Thread.State: RUNNABLE
"MemoryPoolMXBean notification dispatcher" daemon prio=6 tid=54 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.lang.management.MemoryNotificationThread.processNotificationLoop(Native Method)
        at com.ibm.lang.management.MemoryNotificationThread.run(MemoryNotificationThread.java:64)
"Timer-46" daemon prio=5 tid=1913 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server idle connection scanner for port 45141" daemon prio=5 tid=1867 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"Socket Reader #1 for port 43410"  prio=5 tid=1892 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1968 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Parameter Sending Thread #1" daemon prio=5 tid=784 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:472)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:371)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:954)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=16 runnable
java.lang.Thread.State: RUNNABLE
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/]]  heartbeating to localhost/127.0.0.1:38627" daemon prio=5 tid=1919 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5d355519" daemon prio=5 tid=1884 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor@46fb1248" daemon prio=5 tid=1815 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:221)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 44409" daemon prio=5 tid=1926 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1603852932@qtp--1526030080-1 - Acceptor0 SelectChannelConnector@localhost:37729" daemon prio=5 tid=1803 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=38 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 6 on 43410" daemon prio=5 tid=1902 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 8 on 56058" daemon prio=5 tid=1854 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 1 on 44409" daemon prio=5 tid=1921 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 44409" daemon prio=5 tid=1923 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=37 runnable
java.lang.Thread.State: RUNNABLE
"22026256@qtp-289328859-1 - Acceptor0 SelectChannelConnector@localhost:39484" daemon prio=5 tid=1862 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=23 runnable
java.lang.Thread.State: RUNNABLE
"Socket Reader #1 for port 44409"  prio=5 tid=1916 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@112ead8c" daemon prio=5 tid=1829 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller.run(FSNamesystem.java:4366)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=35 runnable
java.lang.Thread.State: RUNNABLE
"101598293@qtp-2110943801-0" daemon prio=5 tid=1911 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"Thread-1740"  prio=5 tid=1985 runnable
java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.getStackTraceImpl(Native Method)
        at java.lang.Thread.getStackTrace(Thread.java:1207)
        at java.lang.Thread.getAllStackTraces(Thread.java:1235)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:86)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:72)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExpiry(TestCacheDirectives.java:1111)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
        at java.lang.reflect.Method.invoke(Method.java:620)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
"java.util.concurrent.ThreadPoolExecutor$Worker@ad579695[State = -1, empty queue]" daemon prio=5 tid=1976 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Client (-772539878) connection to localhost/127.0.0.1:38627 from root" daemon prio=5 tid=1852 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:903)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:948)
"Signal Dispatcher" daemon prio=5 tid=2 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.misc.SignalDispatcher.waitForSignal(Native Method)
        at com.ibm.misc.SignalDispatcher.run(SignalDispatcher.java:75)
"IPC Server Responder" daemon prio=5 tid=1918 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 6 on 45141" daemon prio=5 tid=1876 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@7b973ef0" daemon prio=5 tid=1835 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"-1852736044@qtp-2110943801-1 - Acceptor0 SelectChannelConnector@localhost:36556" daemon prio=5 tid=1912 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=26 runnable
java.lang.Thread.State: RUNNABLE
"Thread-1696" daemon prio=5 tid=1933 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"JIT Compilation Thread-3 Suspended" daemon prio=10 tid=7 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@ebe014dd" daemon prio=5 tid=1864 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"CacheReplicationMonitor(-1923341338)"  prio=5 tid=1830 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2188)
        at org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:181)
"662786415@qtp-302178362-0" daemon prio=5 tid=1885 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"GC Slave" daemon prio=5 tid=28 runnable
java.lang.Thread.State: RUNNABLE
"java.util.concurrent.ThreadPoolExecutor$Worker@1a3e0891[State = -1, empty queue]" daemon prio=5 tid=1962 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"JIT Compilation Thread-0" daemon prio=10 tid=4 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 7 on 56058" daemon prio=5 tid=1853 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@abe70757" daemon prio=5 tid=1914 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1730" daemon prio=5 tid=1973 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:38627" daemon prio=5 tid=1895 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 9 on 45141" daemon prio=5 tid=1879 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1902493981@qtp--1526030080-0" daemon prio=5 tid=1802 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3-0" daemon prio=5 tid=1999 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 5 on 43410" daemon prio=5 tid=1901 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 38627" daemon prio=5 tid=1820 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 9 on 38627" daemon prio=5 tid=1826 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"LeaseRenewer:root@localhost:38627" daemon prio=5 tid=1987 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:438)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=17 runnable
java.lang.Thread.State: RUNNABLE
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1754761744-192.168.250.172-1421890340333" daemon prio=5 tid=1967 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=22 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server idle connection scanner for port 38627" daemon prio=5 tid=1809 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=12 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@718b7ca9" daemon prio=5 tid=1811 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 4 on 44409" daemon prio=5 tid=1924 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@14ff6af4" daemon prio=5 tid=1860 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 5 on 45141" daemon prio=5 tid=1875 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachedBlocks(TestCacheDirectives.java:658)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExpiry(TestCacheDirectives.java:1111)

testLimit(org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives)  Time elapsed: 62.967 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
Timestamp: 2015-01-22 01:34:24,961

"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6-0" daemon prio=5 tid=2202 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1902" daemon prio=5 tid=2168 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2176 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2189 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2148 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=34 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 8 on 60404" daemon prio=5 tid=2084 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@b63850d1" daemon prio=5 tid=2011 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run(HeartbeatManager.java:307)
        at java.lang.Thread.run(Thread.java:857)
"IPC Client (-772539878) connection to localhost/127.0.0.1:46936 from root" daemon prio=5 tid=2136 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:903)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:948)
"IPC Server handler 1 on 46936" daemon prio=5 tid=2024 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 9 on 34848" daemon prio=5 tid=2135 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Attach API wait loop" daemon prio=10 tid=46 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.tools.attach.javaSE.IPC.waitSemaphore(Native Method)
        at com.ibm.tools.attach.javaSE.CommonDirectory.waitSemaphore(CommonDirectory.java:193)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.waitForNotification(AttachHandler.java:352)
        at com.ibm.tools.attach.javaSE.AttachHandler$WaitLoop.run(AttachHandler.java:437)
"Socket Reader #1 for port 34848"  prio=5 tid=2122 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2188 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 56460" daemon prio=5 tid=2108 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 60404" daemon prio=5 tid=2071 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"GC Slave" daemon prio=5 tid=13 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@3fd9eb3e" daemon prio=5 tid=2016 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 8 on 42447" daemon prio=5 tid=2060 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4df889ec" daemon prio=5 tid=2116 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 60404" daemon prio=5 tid=2082 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 0 on 42447" daemon prio=5 tid=2051 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 0 on 56460" daemon prio=5 tid=2101 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=41 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@8b69226f" daemon prio=5 tid=2066 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 1 on 42447" daemon prio=5 tid=2052 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=40 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=36 runnable
java.lang.Thread.State: RUNNABLE
"-956075130@qtp--524851621-1 - Acceptor0 SelectChannelConnector@localhost:35641" daemon prio=5 tid=2008 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 6 on 34848" daemon prio=5 tid=2132 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@abce01ae" daemon prio=5 tid=2034 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run(FSNamesystem.java:4327)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=33 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@c3beaca3" daemon prio=5 tid=2090 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"LeaseRenewer:root@localhost:46936" daemon prio=5 tid=2194 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:438)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:857)
"Socket Reader #1 for port 60404"  prio=5 tid=2072 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2163 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Client (-772539878) connection to localhost/127.0.0.1:46936 from root" daemon prio=5 tid=2056 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:903)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:948)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2162 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 60404" daemon prio=5 tid=2083 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@3fa3a390" daemon prio=5 tid=2035 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller.run(FSNamesystem.java:4366)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server listener on 56460" daemon prio=5 tid=2095 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@4d28ef9d" daemon prio=5 tid=2010 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3360)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1912" daemon prio=5 tid=2180 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"-170039422@qtp--524851621-0" daemon prio=5 tid=2007 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"main"  prio=5 tid=1 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.lang.Thread.join(Thread.java:794)
        at java.lang.Thread.join(Thread.java:755)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:28)
        at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
"1855072541@qtp--79115063-0" daemon prio=5 tid=2117 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 8 on 46936" daemon prio=5 tid=2031 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=29 runnable
java.lang.Thread.State: RUNNABLE
"Concurrent Mark Helper" daemon prio=1 tid=11 runnable
java.lang.Thread.State: RUNNABLE
"JIT Compilation Thread-2 Suspended" daemon prio=10 tid=6 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=14 runnable
java.lang.Thread.State: RUNNABLE
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/]]  heartbeating to localhost/127.0.0.1:46936" daemon prio=5 tid=2125 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=21 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 5 on 42447" daemon prio=5 tid=2057 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"java.util.concurrent.ThreadPoolExecutor$Worker@5df922a6[State = -1, empty queue]" daemon prio=5 tid=2171 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@30fd16d1" daemon prio=5 tid=2033 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:392)
        at java.lang.Thread.run(Thread.java:857)
"Finalizer thread" daemon prio=5 tid=45 runnable
java.lang.Thread.State: RUNNABLE
"-1041422948@qtp-1267089632-0" daemon prio=5 tid=2091 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 9 on 46936" daemon prio=5 tid=2032 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-1715489247@qtp-1267089632-1 - Acceptor0 SelectChannelConnector@localhost:39116" daemon prio=5 tid=2092 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"JIT Compilation Thread-1 Suspended" daemon prio=10 tid=5 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 1 on 60404" daemon prio=5 tid=2077 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 42447" daemon prio=5 tid=2055 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 42447" daemon prio=5 tid=2054 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=30 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=18 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server Responder" daemon prio=5 tid=2049 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 5 on 56460" daemon prio=5 tid=2106 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=15 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=39 runnable
java.lang.Thread.State: RUNNABLE
"Socket Reader #1 for port 42447"  prio=5 tid=2047 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IProfiler" daemon prio=5 tid=10 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 2 on 56460" daemon prio=5 tid=2103 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 6 on 42447" daemon prio=5 tid=2058 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 60404" daemon prio=5 tid=2073 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server Responder" daemon prio=5 tid=2074 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 3 on 56460" daemon prio=5 tid=2104 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=25 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=42 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 34848" daemon prio=5 tid=2126 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Timer-47" daemon prio=5 tid=2009 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"CacheReplicationMonitor(-779546200)"  prio=5 tid=2036 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2188)
        at org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:181)
"IPC Server handler 3 on 60404" daemon prio=5 tid=2079 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"java.util.concurrent.ThreadPoolExecutor$Worker@5ed71594[State = -1, empty queue]" daemon prio=5 tid=2142 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 9 on 56460" daemon prio=5 tid=2110 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Socket Reader #1 for port 46936"  prio=5 tid=2013 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"Socket Reader #1 for port 56460"  prio=5 tid=2096 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:616)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
"IPC Server idle connection scanner for port 42447" daemon prio=5 tid=2048 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@d725d068" daemon prio=5 tid=2041 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:306)
        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:113)
        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:134)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:132)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server idle connection scanner for port 56460" daemon prio=5 tid=2097 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"Timer-48" daemon prio=5 tid=2044 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 0 on 60404" daemon prio=5 tid=2076 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=2015 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 8 on 56460" daemon prio=5 tid=2109 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@67f6fac5" daemon prio=5 tid=2022 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor.run(DecommissionManager.java:76)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 1 on 56460" daemon prio=5 tid=2102 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server listener on 34848" daemon prio=5 tid=2121 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"GC Slave" daemon prio=5 tid=27 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@aa4f8e8b" daemon prio=5 tid=2070 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1922"  prio=5 tid=2192 runnable
java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.getStackTraceImpl(Native Method)
        at java.lang.Thread.getStackTrace(Thread.java:1207)
        at java.lang.Thread.getAllStackTraces(Thread.java:1235)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:86)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:72)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachePoolStats(TestCacheDirectives.java:749)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testLimit(TestCacheDirectives.java:1169)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
        at java.lang.reflect.Method.invoke(Method.java:620)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
"process reaper" daemon prio=10 tid=2145 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:472)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:371)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:954)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"990977000@qtp--1891115240-1 - Acceptor0 SelectChannelConnector@localhost:37131" daemon prio=5 tid=2043 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Server handler 3 on 34848" daemon prio=5 tid=2129 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=20 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 5 on 46936" daemon prio=5 tid=2028 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT-SamplerThread" daemon prio=10 tid=9 timed_waiting
java.lang.Thread.State: TIMED_WAITING
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2149 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server Responder" daemon prio=5 tid=2124 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"IPC Server handler 6 on 56460" daemon prio=5 tid=2107 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 2 on 60404" daemon prio=5 tid=2078 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"JIT Diagnostic Compilation Thread-4 Suspended" daemon prio=10 tid=8 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=24 runnable
java.lang.Thread.State: RUNNABLE
"Timer for 'NameNode' metrics system" daemon prio=5 tid=640 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"GC Slave" daemon prio=5 tid=32 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 4 on 56460" daemon prio=5 tid=2105 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server idle connection scanner for port 34848" daemon prio=5 tid=2123 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"Timer-51" daemon prio=5 tid=2119 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 4 on 46936" daemon prio=5 tid=2027 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Timer-50" daemon prio=5 tid=2093 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"refreshUsed-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-2049089869-192.168.250.172-1421890403142" daemon prio=5 tid=2177 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:115)
        at java.lang.Thread.run(Thread.java:857)
"FsVolumeImplWorker-/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5-0" daemon prio=5 tid=2201 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"Timer-49" daemon prio=5 tid=2069 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 9 on 42447" daemon prio=5 tid=2061 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 3 on 46936" daemon prio=5 tid=2026 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 7 on 46936" daemon prio=5 tid=2030 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"Thread-1878" daemon prio=5 tid=2139 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=31 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server listener on 46936" daemon prio=5 tid=2012 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"234520450@qtp--1891115240-0" daemon prio=5 tid=2042 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"GC Slave" daemon prio=5 tid=19 runnable
java.lang.Thread.State: RUNNABLE
"MemoryPoolMXBean notification dispatcher" daemon prio=6 tid=54 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.lang.management.MemoryNotificationThread.processNotificationLoop(Native Method)
        at com.ibm.lang.management.MemoryNotificationThread.run(MemoryNotificationThread.java:64)
"-5331407@qtp--1599756842-1 - Acceptor0 SelectChannelConnector@localhost:51217" daemon prio=5 tid=2068 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"IPC Parameter Sending Thread #1" daemon prio=5 tid=784 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:472)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:371)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:954)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"Thread-1890" daemon prio=5 tid=2154 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:78)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=16 runnable
java.lang.Thread.State: RUNNABLE
"java.util.concurrent.ThreadPoolExecutor$Worker@45d5f091[State = -1, empty queue]" daemon prio=5 tid=2183 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 34848" daemon prio=5 tid=2133 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=38 runnable
java.lang.Thread.State: RUNNABLE
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:46936" daemon prio=5 tid=2075 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 6 on 46936" daemon prio=5 tid=2029 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 60404" daemon prio=5 tid=2080 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@cfef8854" daemon prio=5 tid=2094 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=37 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=23 runnable
java.lang.Thread.State: RUNNABLE
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:46936" daemon prio=5 tid=2100 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 2 on 46936" daemon prio=5 tid=2025 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=35 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 0 on 46936" daemon prio=5 tid=2023 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@d0f3cd96" daemon prio=5 tid=2120 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"Signal Dispatcher" daemon prio=5 tid=2 runnable
java.lang.Thread.State: RUNNABLE
        at com.ibm.misc.SignalDispatcher.waitForSignal(Native Method)
        at com.ibm.misc.SignalDispatcher.run(SignalDispatcher.java:75)
"IPC Server handler 1 on 34848" daemon prio=5 tid=2127 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"-2051589987@qtp--1599756842-0" daemon prio=5 tid=2067 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
"IPC Server handler 5 on 34848" daemon prio=5 tid=2131 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"java.util.concurrent.ThreadPoolExecutor$Worker@d0a025dc[State = -1, empty queue]" daemon prio=5 tid=2157 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1106)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1100)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.lang.Thread.run(Thread.java:857)
"GC Slave" daemon prio=5 tid=26 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 5 on 60404" daemon prio=5 tid=2081 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"DataNode: [[[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:46936" daemon prio=5 tid=2050 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:724)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:838)
        at java.lang.Thread.run(Thread.java:857)
"JIT Compilation Thread-3 Suspended" daemon prio=10 tid=7 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server handler 2 on 34848" daemon prio=5 tid=2128 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 4 on 34848" daemon prio=5 tid=2130 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"GC Slave" daemon prio=5 tid=28 runnable
java.lang.Thread.State: RUNNABLE
"JIT Compilation Thread-0" daemon prio=10 tid=4 runnable
java.lang.Thread.State: RUNNABLE
"org.apache.hadoop.util.JvmPauseMonitor$Monitor@3f23d1c6" daemon prio=5 tid=2045 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:161)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server listener on 42447" daemon prio=5 tid=2046 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:115)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:668)
"IPC Server handler 8 on 34848" daemon prio=5 tid=2134 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server Responder" daemon prio=5 tid=2098 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:835)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:818)
"org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor@9349e90e" daemon prio=5 tid=2021 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:981)
        at org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:221)
        at java.lang.Thread.run(Thread.java:857)
"IPC Server handler 7 on 42447" daemon prio=5 tid=2059 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"IPC Server handler 9 on 60404" daemon prio=5 tid=2085 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)
"703400389@qtp--79115063-1 - Acceptor0 SelectChannelConnector@localhost:47824" daemon prio=5 tid=2118 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:282)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:92)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:100)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:111)
        at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
        at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
        at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
        at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
"GC Slave" daemon prio=5 tid=17 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=22 runnable
java.lang.Thread.State: RUNNABLE
"GC Slave" daemon prio=5 tid=12 runnable
java.lang.Thread.State: RUNNABLE
"IPC Server idle connection scanner for port 46936" daemon prio=5 tid=2014 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:201)
        at java.util.TimerThread.mainLoop(Timer.java:564)
        at java.util.TimerThread.run(Timer.java:517)
"IPC Server handler 2 on 42447" daemon prio=5 tid=2053 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:238)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2094)
        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:479)
        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1985)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:123)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.waitForCachePoolStats(TestCacheDirectives.java:749)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testLimit(TestCacheDirectives.java:1169)

testExceedsCapacity(org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives)  Time elapsed: 62.025 sec  <<< ERROR!
java.lang.Exception: test timed out after 60000 milliseconds

Running org.apache.hadoop.hdfs.server.namenode.TestEditLogRace
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 45.603 sec - in org.apache.hadoop.hdfs.server.namenode.TestEditLogRace
Running org.apache.hadoop.hdfs.server.namenode.TestINodeFile
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 61.613 sec - in org.apache.hadoop.hdfs.server.namenode.TestINodeFile
Running org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionFunctional
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.138 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionFunctional
testPurgingWithNameEditsDirAfterFailure(org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionFunctional)  Time elapsed: 5.856 sec  <<< FAILURE!
org.junit.ComparisonFailure: Bad files matching fsimage_\d* in /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/nn0/current expected:<...e_000000000000000000[2,fsimage_0000000000000000004]> but was:<...e_000000000000000000[4,fsimage_0000000000000000006]>
	at org.junit.Assert.assertEquals(Assert.java:125)
	at org.apache.hadoop.test.GenericTestUtils.assertGlobEquals(GenericTestUtils.java:97)
	at org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionFunctional.testPurgingWithNameEditsDirAfterFailure(TestNNStorageRetentionFunctional.java:117)

Running org.apache.hadoop.hdfs.server.namenode.TestNameCache
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.2 sec - in org.apache.hadoop.hdfs.server.namenode.TestNameCache
Running org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.602 sec - in org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks
Running org.apache.hadoop.hdfs.server.namenode.TestSecondaryNameNodeUpgrade
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.298 sec - in org.apache.hadoop.hdfs.server.namenode.TestSecondaryNameNodeUpgrade
Running org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.353 sec - in org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager
Running org.apache.hadoop.hdfs.server.namenode.TestBackupNode
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.837 sec - in org.apache.hadoop.hdfs.server.namenode.TestBackupNode
Running org.apache.hadoop.hdfs.server.namenode.TestMetaSave
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 36.749 sec - in org.apache.hadoop.hdfs.server.namenode.TestMetaSave
Running org.apache.hadoop.hdfs.server.namenode.TestAllowFormat
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.127 sec - in org.apache.hadoop.hdfs.server.namenode.TestAllowFormat
Running org.apache.hadoop.hdfs.server.namenode.TestAuditLogs
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 43.591 sec - in org.apache.hadoop.hdfs.server.namenode.TestAuditLogs
Running org.apache.hadoop.hdfs.server.namenode.TestAclTransformation
Tests run: 51, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.283 sec - in org.apache.hadoop.hdfs.server.namenode.TestAclTransformation
Running org.apache.hadoop.hdfs.server.namenode.TestClusterJspHelper
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.249 sec - in org.apache.hadoop.hdfs.server.namenode.TestClusterJspHelper
Running org.apache.hadoop.hdfs.server.namenode.TestLargeDirectoryDelete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 127.762 sec - in org.apache.hadoop.hdfs.server.namenode.TestLargeDirectoryDelete
Running org.apache.hadoop.hdfs.server.namenode.TestCorruptFilesJsp
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.727 sec - in org.apache.hadoop.hdfs.server.namenode.TestCorruptFilesJsp
Running org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.432 sec - in org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus
Running org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.065 sec - in org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem
Running org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 70.89 sec - in org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup
Running org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 47.396 sec - in org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer
Running org.apache.hadoop.hdfs.server.balancer.TestBalancer
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 59.1 sec - in org.apache.hadoop.hdfs.server.balancer.TestBalancer
Running org.apache.hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.396 sec - in org.apache.hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes
Running org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.12 sec - in org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes
Running org.apache.hadoop.hdfs.server.common.TestJspHelper
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.141 sec - in org.apache.hadoop.hdfs.server.common.TestJspHelper
Running org.apache.hadoop.hdfs.server.common.TestGetUriFromString
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.208 sec - in org.apache.hadoop.hdfs.server.common.TestGetUriFromString
Running org.apache.hadoop.hdfs.server.blockmanagement.TestCachedBlocksList
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.419 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestCachedBlocksList
Running org.apache.hadoop.hdfs.server.blockmanagement.TestPendingDataNodeMessages
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.499 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestPendingDataNodeMessages
Running org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicy
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.718 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicy
Running org.apache.hadoop.hdfs.server.blockmanagement.TestComputeInvalidateWork
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.021 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestComputeInvalidateWork
Running org.apache.hadoop.hdfs.server.blockmanagement.TestCorruptReplicaInfo
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.695 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestCorruptReplicaInfo
Running org.apache.hadoop.hdfs.server.blockmanagement.TestBlockInfoUnderConstruction
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.532 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestBlockInfoUnderConstruction
Running org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReplication
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.578 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReplication
Running org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.441 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks
Running org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 41.431 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager
Running org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 45.598 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS
Running org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.372 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager
Running org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 26.373 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks
Running org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 53.032 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
Running org.apache.hadoop.hdfs.server.blockmanagement.TestNodeCount
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.959 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestNodeCount
Running org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithNodeGroup
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.271 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithNodeGroup
Running org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.437 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation
Running org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicyConsiderLoad
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.781 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicyConsiderLoad
Running org.apache.hadoop.hdfs.server.blockmanagement.TestHost2NodesMap
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.523 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestHost2NodesMap
Running org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlockQueues
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.283 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlockQueues
Running org.apache.hadoop.hdfs.server.blockmanagement.TestHeartbeatHandling
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.495 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestHeartbeatHandling
Running org.apache.hadoop.hdfs.server.blockmanagement.TestBlockInfo
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.516 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestBlockInfo
Running org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeDescriptor
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.562 sec - in org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeDescriptor
Running org.apache.hadoop.hdfs.TestDFSOutputStream
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.096 sec - in org.apache.hadoop.hdfs.TestDFSOutputStream
Running org.apache.hadoop.hdfs.TestCrcCorruption
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 46.189 sec - in org.apache.hadoop.hdfs.TestCrcCorruption
Running org.apache.hadoop.hdfs.TestFileConcurrentReader
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.408 sec - in org.apache.hadoop.hdfs.TestFileConcurrentReader
Running org.apache.hadoop.hdfs.TestRestartDFS
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.58 sec - in org.apache.hadoop.hdfs.TestRestartDFS
Running org.apache.hadoop.hdfs.TestDataTransferKeepalive
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.633 sec - in org.apache.hadoop.hdfs.TestDataTransferKeepalive
Running org.apache.hadoop.hdfs.TestFileCreation
Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 93 sec - in org.apache.hadoop.hdfs.TestFileCreation
Running org.apache.hadoop.hdfs.TestRollingUpgrade
Tests run: 9, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 28.192 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestRollingUpgrade
testFinalize(org.apache.hadoop.hdfs.TestRollingUpgrade)  Time elapsed: 0.708 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.TestRollingUpgrade.testFinalize(TestRollingUpgrade.java:361)

testQuery(org.apache.hadoop.hdfs.TestRollingUpgrade)  Time elapsed: 0.747 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.TestRollingUpgrade.testQuery(TestRollingUpgrade.java:405)

testCheckpoint(org.apache.hadoop.hdfs.TestRollingUpgrade)  Time elapsed: 0.586 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.TestRollingUpgrade.testCheckpoint(TestRollingUpgrade.java:472)

Running org.apache.hadoop.hdfs.TestFSInputChecker
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.635 sec - in org.apache.hadoop.hdfs.TestFSInputChecker
Running org.apache.hadoop.hdfs.TestParallelReadUtil
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.04 sec - in org.apache.hadoop.hdfs.TestParallelReadUtil
Running org.apache.hadoop.hdfs.TestDatanodeDeath
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 137.651 sec - in org.apache.hadoop.hdfs.TestDatanodeDeath
Running org.apache.hadoop.hdfs.TestDataTransferProtocol
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.391 sec - in org.apache.hadoop.hdfs.TestDataTransferProtocol
Running org.apache.hadoop.hdfs.TestFileAppend2
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.906 sec - in org.apache.hadoop.hdfs.TestFileAppend2
Running org.apache.hadoop.hdfs.TestParallelShortCircuitRead
Tests run: 4, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 0.255 sec - in org.apache.hadoop.hdfs.TestParallelShortCircuitRead
Running org.apache.hadoop.hdfs.TestHDFSFileSystemContract
Tests run: 31, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 69.2 sec - in org.apache.hadoop.hdfs.TestHDFSFileSystemContract
Running org.apache.hadoop.hdfs.TestPeerCache
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.46 sec - in org.apache.hadoop.hdfs.TestPeerCache
Running org.apache.hadoop.hdfs.TestDFSPermission
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.355 sec - in org.apache.hadoop.hdfs.TestDFSPermission
Running org.apache.hadoop.hdfs.TestListFilesInDFS
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.734 sec - in org.apache.hadoop.hdfs.TestListFilesInDFS
Running org.apache.hadoop.hdfs.TestLease
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.696 sec - in org.apache.hadoop.hdfs.TestLease
Running org.apache.hadoop.hdfs.TestParallelShortCircuitReadNoChecksum
Tests run: 4, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 0.241 sec - in org.apache.hadoop.hdfs.TestParallelShortCircuitReadNoChecksum
Running org.apache.hadoop.hdfs.TestAppendDifferentChecksum
Tests run: 3, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 13.339 sec - in org.apache.hadoop.hdfs.TestAppendDifferentChecksum
Running org.apache.hadoop.hdfs.TestFetchImage
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.411 sec - in org.apache.hadoop.hdfs.TestFetchImage
Running org.apache.hadoop.hdfs.TestRead
Tests run: 2, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 6.387 sec - in org.apache.hadoop.hdfs.TestRead
Running org.apache.hadoop.hdfs.TestSafeMode
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 50.066 sec - in org.apache.hadoop.hdfs.TestSafeMode
Running org.apache.hadoop.hdfs.client.TestShortCircuitShm
Tests run: 2, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 0.207 sec - in org.apache.hadoop.hdfs.client.TestShortCircuitShm
Running org.apache.hadoop.hdfs.TestHttpPolicy
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.431 sec - in org.apache.hadoop.hdfs.TestHttpPolicy
Running org.apache.hadoop.hdfs.TestClientBlockVerification
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.661 sec - in org.apache.hadoop.hdfs.TestClientBlockVerification
Running org.apache.hadoop.hdfs.TestSnapshotCommands
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.481 sec - in org.apache.hadoop.hdfs.TestSnapshotCommands
Running org.apache.hadoop.hdfs.TestFSOutputSummer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.547 sec - in org.apache.hadoop.hdfs.TestFSOutputSummer
Running org.apache.hadoop.hdfs.TestDefaultNameNodePort
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.714 sec - in org.apache.hadoop.hdfs.TestDefaultNameNodePort
Running org.apache.hadoop.hdfs.TestPread
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 105.455 sec - in org.apache.hadoop.hdfs.TestPread
Running org.apache.hadoop.hdfs.TestPersistBlocks
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 36.985 sec - in org.apache.hadoop.hdfs.TestPersistBlocks
Running org.apache.hadoop.hdfs.TestDFSUpgrade
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.614 sec - in org.apache.hadoop.hdfs.TestDFSUpgrade
Running org.apache.hadoop.hdfs.TestLeaseRecovery2
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 66.991 sec - in org.apache.hadoop.hdfs.TestLeaseRecovery2
Running org.apache.hadoop.hdfs.TestRenameWhileOpen
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 53.498 sec - in org.apache.hadoop.hdfs.TestRenameWhileOpen
Running org.apache.hadoop.hdfs.TestWriteRead
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 62.368 sec - in org.apache.hadoop.hdfs.TestWriteRead
Running org.apache.hadoop.hdfs.TestFileAppend4
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.043 sec - in org.apache.hadoop.hdfs.TestFileAppend4
Running org.apache.hadoop.hdfs.TestReplication
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.513 sec - in org.apache.hadoop.hdfs.TestReplication
Running org.apache.hadoop.hdfs.TestParallelShortCircuitLegacyRead
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 38.881 sec - in org.apache.hadoop.hdfs.TestParallelShortCircuitLegacyRead
Running org.apache.hadoop.hdfs.TestQuota
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.491 sec - in org.apache.hadoop.hdfs.TestQuota
Running org.apache.hadoop.hdfs.TestDatanodeConfig
Tests run: 2, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 6.467 sec - in org.apache.hadoop.hdfs.TestDatanodeConfig
Running org.apache.hadoop.hdfs.TestHDFSServerPorts
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.26 sec - in org.apache.hadoop.hdfs.TestHDFSServerPorts
Running org.apache.hadoop.hdfs.TestDFSClientFailover
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.053 sec - in org.apache.hadoop.hdfs.TestDFSClientFailover
Running org.apache.hadoop.hdfs.TestFileAppend
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.863 sec - in org.apache.hadoop.hdfs.TestFileAppend
Running org.apache.hadoop.hdfs.TestRollingUpgradeRollback
Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 9.881 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestRollingUpgradeRollback
testRollbackWithHAQJM(org.apache.hadoop.hdfs.TestRollingUpgradeRollback)  Time elapsed: 0.932 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.TestRollingUpgradeRollback.testRollbackWithHAQJM(TestRollingUpgradeRollback.java:210)

Running org.apache.hadoop.hdfs.TestGetBlocks
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 54.948 sec - in org.apache.hadoop.hdfs.TestGetBlocks
Running org.apache.hadoop.hdfs.TestFileAppendRestart
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.727 sec - in org.apache.hadoop.hdfs.TestFileAppendRestart
Running org.apache.hadoop.hdfs.TestReadWhileWriting
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.836 sec - in org.apache.hadoop.hdfs.TestReadWhileWriting
Running org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.358 sec - in org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts
Running org.apache.hadoop.hdfs.web.resources.TestParam
Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.904 sec - in org.apache.hadoop.hdfs.web.resources.TestParam
Running org.apache.hadoop.hdfs.web.TestJsonUtil
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.268 sec - in org.apache.hadoop.hdfs.web.TestJsonUtil
Running org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 70.801 sec <<< FAILURE! - in org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes
testRedirect(org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes)  Time elapsed: 61.962 sec  <<< ERROR!
java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.read(SocketInputStream.java:164)
	at java.net.SocketInputStream.read(SocketInputStream.java:134)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:247)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:287)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:346)
	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:717)
	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:663)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1335)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:307)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:108)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.twoStepWrite(WebHdfsFileSystem.java:590)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:527)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$200(WebHdfsFileSystem.java:444)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:474)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:471)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:470)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.create(WebHdfsFileSystem.java:871)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:773)
	at org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes.testRedirect(TestWebHdfsWithMultipleNameNodes.java:130)

Running org.apache.hadoop.hdfs.web.TestTokenAspect
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.44 sec - in org.apache.hadoop.hdfs.web.TestTokenAspect
Running org.apache.hadoop.hdfs.web.TestHftpDelegationToken
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.133 sec - in org.apache.hadoop.hdfs.web.TestHftpDelegationToken
Running org.apache.hadoop.hdfs.web.TestWebHDFS
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 114.922 sec - in org.apache.hadoop.hdfs.web.TestWebHDFS
Running org.apache.hadoop.hdfs.web.TestWebHdfsWithAuthenticationFilter
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.053 sec - in org.apache.hadoop.hdfs.web.TestWebHdfsWithAuthenticationFilter
Running org.apache.hadoop.hdfs.web.TestWebHdfsTokens
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.355 sec - in org.apache.hadoop.hdfs.web.TestWebHdfsTokens
Running org.apache.hadoop.hdfs.web.TestOffsetUrlInputStream
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.224 sec - in org.apache.hadoop.hdfs.web.TestOffsetUrlInputStream
Running org.apache.hadoop.hdfs.web.TestHftpFileSystem
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 5.938 sec <<< FAILURE! - in org.apache.hadoop.hdfs.web.TestHftpFileSystem
org.apache.hadoop.hdfs.web.TestHftpFileSystem  Time elapsed: 5.937 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.hdfs.web.TestHftpFileSystem.setUp(TestHftpFileSystem.java:98)

org.apache.hadoop.hdfs.web.TestHftpFileSystem  Time elapsed: 5.938 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.hdfs.web.TestHftpFileSystem.tearDown(TestHftpFileSystem.java:109)

Running org.apache.hadoop.hdfs.web.TestByteRangeInputStream
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.508 sec - in org.apache.hadoop.hdfs.web.TestByteRangeInputStream
Running org.apache.hadoop.hdfs.web.TestWebHDFSForHA
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.618 sec - in org.apache.hadoop.hdfs.web.TestWebHDFSForHA
Running org.apache.hadoop.hdfs.web.TestFSMainOperationsWebHdfs
Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.409 sec - in org.apache.hadoop.hdfs.web.TestFSMainOperationsWebHdfs
Running org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
Tests run: 38, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 49.267 sec - in org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
Running org.apache.hadoop.hdfs.web.TestWebHdfsUrl
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.317 sec - in org.apache.hadoop.hdfs.web.TestWebHdfsUrl
Running org.apache.hadoop.hdfs.web.TestHttpsFileSystem
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.387 sec <<< FAILURE! - in org.apache.hadoop.hdfs.web.TestHttpsFileSystem
org.apache.hadoop.hdfs.web.TestHttpsFileSystem  Time elapsed: 0.387 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.hdfs.web.TestHttpsFileSystem.setUp(TestHttpsFileSystem.java:64)

org.apache.hadoop.hdfs.web.TestHttpsFileSystem  Time elapsed: 0.387 sec  <<< ERROR!
java.lang.NullPointerException: null
	at org.apache.hadoop.hdfs.web.TestHttpsFileSystem.tearDown(TestHttpsFileSystem.java:80)

Running org.apache.hadoop.hdfs.web.TestURLConnectionFactory
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.228 sec - in org.apache.hadoop.hdfs.web.TestURLConnectionFactory
Running org.apache.hadoop.hdfs.web.TestAuthFilter
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.615 sec - in org.apache.hadoop.hdfs.web.TestAuthFilter
Running org.apache.hadoop.hdfs.web.TestWebHDFSAcl
Tests run: 61, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 15.255 sec - in org.apache.hadoop.hdfs.web.TestWebHDFSAcl
Running org.apache.hadoop.hdfs.TestFileStatus
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.723 sec - in org.apache.hadoop.hdfs.TestFileStatus
Running org.apache.hadoop.hdfs.TestDFSMkdirs
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.525 sec - in org.apache.hadoop.hdfs.TestDFSMkdirs
Running org.apache.hadoop.hdfs.TestLeaseRecovery
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.602 sec - in org.apache.hadoop.hdfs.TestLeaseRecovery
Running org.apache.hadoop.hdfs.TestBlockMissingException
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.969 sec - in org.apache.hadoop.hdfs.TestBlockMissingException
Running org.apache.hadoop.hdfs.TestRollingUpgradeDowngrade
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.235 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestRollingUpgradeDowngrade
testDowngrade(org.apache.hadoop.hdfs.TestRollingUpgradeDowngrade)  Time elapsed: 4.045 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:10000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:297)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:587)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:561)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:95)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:36)
	at org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:64)
	at org.apache.hadoop.hdfs.TestRollingUpgradeDowngrade.testDowngrade(TestRollingUpgradeDowngrade.java:47)

Running org.apache.hadoop.hdfs.qjournal.server.TestJournalNode
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.027 sec - in org.apache.hadoop.hdfs.qjournal.server.TestJournalNode
Running org.apache.hadoop.hdfs.qjournal.server.TestJournal
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.864 sec - in org.apache.hadoop.hdfs.qjournal.server.TestJournal
Running org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeMXBean
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.757 sec - in org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeMXBean
Running org.apache.hadoop.hdfs.qjournal.client.TestIPCLoggerChannel
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.155 sec - in org.apache.hadoop.hdfs.qjournal.client.TestIPCLoggerChannel
Running org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager
Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.414 sec - in org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager
Running org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.516 sec - in org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit
Running org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.318 sec - in org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique
Running org.apache.hadoop.hdfs.qjournal.client.TestQuorumCall
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 sec - in org.apache.hadoop.hdfs.qjournal.client.TestQuorumCall
Running org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 228.53 sec - in org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults
Running org.apache.hadoop.hdfs.qjournal.client.TestSegmentRecoveryComparator
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.287 sec - in org.apache.hadoop.hdfs.qjournal.client.TestSegmentRecoveryComparator
Running org.apache.hadoop.hdfs.qjournal.TestMiniJournalCluster
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.721 sec - in org.apache.hadoop.hdfs.qjournal.TestMiniJournalCluster
Running org.apache.hadoop.hdfs.qjournal.TestNNWithQJM
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.354 sec - in org.apache.hadoop.hdfs.qjournal.TestNNWithQJM
Running org.apache.hadoop.hdfs.TestDistributedFileSystem
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 62.193 sec - in org.apache.hadoop.hdfs.TestDistributedFileSystem
Running org.apache.hadoop.hdfs.TestDFSShell
Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 38.519 sec - in org.apache.hadoop.hdfs.TestDFSShell
Running org.apache.hadoop.hdfs.TestDecommission
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 141.812 sec - in org.apache.hadoop.hdfs.TestDecommission
Running org.apache.hadoop.hdfs.TestEncryptedTransfer
Tests run: 24, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 159.721 sec - in org.apache.hadoop.hdfs.TestEncryptedTransfer
Running org.apache.hadoop.hdfs.TestFileCreationClient
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 63.079 sec - in org.apache.hadoop.hdfs.TestFileCreationClient
Running org.apache.hadoop.hdfs.TestBalancerBandwidth
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.11 sec - in org.apache.hadoop.hdfs.TestBalancerBandwidth
Running org.apache.hadoop.hdfs.TestClientReportBadBlock
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 49.064 sec - in org.apache.hadoop.hdfs.TestClientReportBadBlock
Running org.apache.hadoop.hdfs.TestDFSFinalize
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.477 sec - in org.apache.hadoop.hdfs.TestDFSFinalize
Running org.apache.hadoop.hdfs.TestWriteConfigurationToDFS
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.222 sec - in org.apache.hadoop.hdfs.TestWriteConfigurationToDFS
Running org.apache.hadoop.hdfs.TestDisableConnCache
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.403 sec - in org.apache.hadoop.hdfs.TestDisableConnCache
Running org.apache.hadoop.hdfs.TestDFSUpgradeFromImage
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.746 sec - in org.apache.hadoop.hdfs.TestDFSUpgradeFromImage
Running org.apache.hadoop.hdfs.TestDFSStorageStateRecovery
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 131.99 sec - in org.apache.hadoop.hdfs.TestDFSStorageStateRecovery
Running org.apache.hadoop.hdfs.TestSetTimes
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.64 sec - in org.apache.hadoop.hdfs.TestSetTimes
Running org.apache.hadoop.hdfs.TestListPathServlet
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.367 sec - in org.apache.hadoop.hdfs.TestListPathServlet
Running org.apache.hadoop.hdfs.TestDatanodeBlockScanner
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 125.695 sec - in org.apache.hadoop.hdfs.TestDatanodeBlockScanner
Running org.apache.hadoop.hdfs.TestPipelines
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.067 sec - in org.apache.hadoop.hdfs.TestPipelines
Running org.apache.hadoop.hdfs.TestBlocksScheduledCounter
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.204 sec - in org.apache.hadoop.hdfs.TestBlocksScheduledCounter
Running org.apache.hadoop.hdfs.TestDFSStartupVersions
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.191 sec - in org.apache.hadoop.hdfs.TestDFSStartupVersions
Running org.apache.hadoop.hdfs.TestDatanodeRegistration
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.958 sec - in org.apache.hadoop.hdfs.TestDatanodeRegistration
Running org.apache.hadoop.hdfs.TestBlockReaderLocalLegacy
Tests run: 2, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 8.841 sec - in org.apache.hadoop.hdfs.TestBlockReaderLocalLegacy
Running org.apache.hadoop.hdfs.TestParallelRead
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 56.821 sec - in org.apache.hadoop.hdfs.TestParallelRead
Running org.apache.hadoop.hdfs.TestModTime
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.957 sec - in org.apache.hadoop.hdfs.TestModTime
Running org.apache.hadoop.hdfs.TestListFilesInFileContext
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.569 sec - in org.apache.hadoop.hdfs.TestListFilesInFileContext
Running org.apache.hadoop.hdfs.TestBlockReaderLocal
Tests run: 37, Failures: 0, Errors: 0, Skipped: 37, Time elapsed: 0.291 sec - in org.apache.hadoop.hdfs.TestBlockReaderLocal
Running org.apache.hadoop.hdfs.TestParallelShortCircuitReadUnCached
Tests run: 4, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 0.227 sec - in org.apache.hadoop.hdfs.TestParallelShortCircuitReadUnCached
Running org.apache.hadoop.hdfs.util.TestCyclicIteration
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.083 sec - in org.apache.hadoop.hdfs.util.TestCyclicIteration
Running org.apache.hadoop.hdfs.util.TestDiff
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.205 sec - in org.apache.hadoop.hdfs.util.TestDiff
Running org.apache.hadoop.hdfs.util.TestChunkedArrayList
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.045 sec - in org.apache.hadoop.hdfs.util.TestChunkedArrayList
Running org.apache.hadoop.hdfs.util.TestMD5FileUtils
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.771 sec - in org.apache.hadoop.hdfs.util.TestMD5FileUtils
Running org.apache.hadoop.hdfs.util.TestAtomicFileOutputStream
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.28 sec - in org.apache.hadoop.hdfs.util.TestAtomicFileOutputStream
Running org.apache.hadoop.hdfs.util.TestLightWeightHashSet
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.25 sec - in org.apache.hadoop.hdfs.util.TestLightWeightHashSet
Running org.apache.hadoop.hdfs.util.TestDirectBufferPool
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.112 sec - in org.apache.hadoop.hdfs.util.TestDirectBufferPool
Running org.apache.hadoop.hdfs.util.TestExactSizeInputStream
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.108 sec - in org.apache.hadoop.hdfs.util.TestExactSizeInputStream
Running org.apache.hadoop.hdfs.util.TestLightWeightLinkedSet
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.23 sec - in org.apache.hadoop.hdfs.util.TestLightWeightLinkedSet
Running org.apache.hadoop.hdfs.util.TestBestEffortLongFile
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.286 sec - in org.apache.hadoop.hdfs.util.TestBestEffortLongFile
Running org.apache.hadoop.hdfs.util.TestXMLUtils
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.08 sec - in org.apache.hadoop.hdfs.util.TestXMLUtils
Running org.apache.hadoop.hdfs.TestFileAppend3
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.203 sec - in org.apache.hadoop.hdfs.TestFileAppend3
Running org.apache.hadoop.hdfs.TestIsMethodSupported
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.172 sec - in org.apache.hadoop.hdfs.TestIsMethodSupported
Running org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.385 sec - in org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage
Running org.apache.hadoop.hdfs.TestMultiThreadedHflush
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.438 sec - in org.apache.hadoop.hdfs.TestMultiThreadedHflush
Running org.apache.hadoop.hdfs.TestSmallBlock
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.468 sec - in org.apache.hadoop.hdfs.TestSmallBlock
Running org.apache.hadoop.hdfs.security.TestClientProtocolWithDelegationToken
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.413 sec - in org.apache.hadoop.hdfs.security.TestClientProtocolWithDelegationToken
Running org.apache.hadoop.hdfs.security.TestDelegationToken
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 41.826 sec - in org.apache.hadoop.hdfs.security.TestDelegationToken
Running org.apache.hadoop.hdfs.security.token.block.TestBlockToken
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.503 sec - in org.apache.hadoop.hdfs.security.token.block.TestBlockToken
Running org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.11 sec - in org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser
Running org.apache.hadoop.hdfs.TestDFSAddressConfig
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.082 sec - in org.apache.hadoop.hdfs.TestDFSAddressConfig
Running org.apache.hadoop.hdfs.TestFileCorruption
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.822 sec - in org.apache.hadoop.hdfs.TestFileCorruption
Running org.apache.hadoop.hdfs.protocol.TestAnnotations
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.174 sec - in org.apache.hadoop.hdfs.protocol.TestAnnotations
Running org.apache.hadoop.hdfs.protocol.datatransfer.TestPacketReceiver
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.411 sec - in org.apache.hadoop.hdfs.protocol.datatransfer.TestPacketReceiver
Running org.apache.hadoop.hdfs.protocol.TestExtendedBlock
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.081 sec - in org.apache.hadoop.hdfs.protocol.TestExtendedBlock
Running org.apache.hadoop.hdfs.protocol.TestLayoutVersion
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.097 sec - in org.apache.hadoop.hdfs.protocol.TestLayoutVersion
Running org.apache.hadoop.hdfs.TestDFSUtil
Tests run: 29, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.676 sec - in org.apache.hadoop.hdfs.TestDFSUtil
Running org.apache.hadoop.hdfs.TestDatanodeReport
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.362 sec - in org.apache.hadoop.hdfs.TestDatanodeReport
Running org.apache.hadoop.hdfs.TestShortCircuitLocalRead
Tests run: 10, Failures: 0, Errors: 0, Skipped: 10, Time elapsed: 0.312 sec - in org.apache.hadoop.hdfs.TestShortCircuitLocalRead
Running org.apache.hadoop.hdfs.TestMissingBlocksAlert
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.686 sec - in org.apache.hadoop.hdfs.TestMissingBlocksAlert
Running org.apache.hadoop.hdfs.TestFileCreationEmpty
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.305 sec - in org.apache.hadoop.hdfs.TestFileCreationEmpty
Running org.apache.hadoop.hdfs.TestHDFSTrash
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.357 sec - in org.apache.hadoop.hdfs.TestHDFSTrash
Running org.apache.hadoop.security.TestPermission
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.844 sec - in org.apache.hadoop.security.TestPermission
Running org.apache.hadoop.security.TestPermissionSymlinks
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.127 sec - in org.apache.hadoop.security.TestPermissionSymlinks
Running org.apache.hadoop.security.TestRefreshUserMappings
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.833 sec - in org.apache.hadoop.security.TestRefreshUserMappings
Running org.apache.hadoop.cli.TestHDFSCLI
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 120.632 sec - in org.apache.hadoop.cli.TestHDFSCLI
Running org.apache.hadoop.cli.TestCacheAdminCLI
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.023 sec - in org.apache.hadoop.cli.TestCacheAdminCLI
Running org.apache.hadoop.cli.TestAclCLI
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.335 sec - in org.apache.hadoop.cli.TestAclCLI

Results :

Failed tests: 
  TestDataNodeVolumeFailureToleration.testValidVolumesAtStartup:125 The DN shouldn't have a bad directory.
  TestDataNodeVolumeFailureToleration.testVolumeAndTolerableConfiguration:202->testVolumeConfig:233 expected:<false> but was:<true>
  TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures:154 Bad value for metric VolumeFailures expected:<1> but was:<0>
  TestEditLog.testFailedOpen:929 Did no throw exception on only having a bad dir
  TestCheckpoint.testNameDirError:194 NN should have failed to start with /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 set unreadable
  TestCheckpoint.testCheckpointWithSeparateDirsAfterNameFails:2119 Did not fail to checkpoint when there are no valid storage dirs
  TestStorageRestore.testStorageRestoreFailure:415 null
  TestNameNodeMXBean.testNameNodeMXBeanInfo:178 null
  TestFailureOfSharedDir.testFailureOfSharedDir:169 Succeeded in rolling edit log despite shared dir being deleted
  TestSaveNamespace.testReinsertnamedirsInSavenamespace:260 Savenamespace should have marked one directory as bad. But found 0 bad directories.
  TestNNStorageRetentionFunctional.testPurgingWithNameEditsDirAfterFailure:117 Bad files matching fsimage_\d* in /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/nn0/current expected:<...e_000000000000000000[2,fsimage_0000000000000000004]> but was:<...e_000000000000000000[4,fsimage_0000000000000000006]>

Tests in error: 
  TestDeleteBlockPool.testDeleteBlockPool » Remote File /alpha could only be rep...
  TestDeleteBlockPool.testDfsAdminDeleteBlockPool:161 » IO Missing directory /op...
  TestDataNodeVolumeFailureToleration.testConfigureMinValidVolumes:160 » Timeout
  TestDataNodeVolumeFailureToleration.testFailedVolumeOnStartupIsCounted:274 » Timeout
  TestDataNodeVolumeFailureReporting.testVolFailureStatsPreservedOnNNRestart:273 » Timeout
  TestNameNodeHttpServer.setUp:73 NoClassDefFound sun.security.x509.X509CertImpl
  TestNameNodeHttpServer.tearDown:82 NoClassDefFound sun.security.x509.X509CertI...
  TestDFSUpgradeWithHA.testUpgradeWithJournalNodes:310 » Bind Problem binding to...
  TestDFSUpgradeWithHA.testFinalizeWithJournalNodes:378 » Bind Problem binding t...
  TestDFSUpgradeWithHA.testFinalizeFromSecondNameNodeWithJournalNodes:440 » Bind
  TestDFSUpgradeWithHA.testRollbackWithJournalNodes:604 » Bind Problem binding t...
  TestFailureToReadEdits.setUpCluster:108 » Bind Problem binding to [localhost:1...
  TestFailureToReadEdits.setUpCluster:108 » Bind Problem binding to [localhost:1...
  TestFailureToReadEdits.setUpCluster:108 » Bind Problem binding to [localhost:1...
  TestFailureToReadEdits.setUpCluster:116 » Bind Problem binding to [localhost:1...
  TestFailureToReadEdits.setUpCluster:116 » Bind Problem binding to [localhost:1...
  TestFailureToReadEdits.setUpCluster:116 » Bind Problem binding to [localhost:1...
  TestBootstrapStandbyWithQJM.setup:55 » Bind Problem binding to [localhost:1000...
  TestBootstrapStandbyWithQJM.setup:55 » Bind Problem binding to [localhost:1000...
  TestCacheDirectives.testWaitForCachedReplicas:878->waitForCachedBlocks:658 » Timeout
  TestCacheDirectives.testWaitForCachedReplicasInDirectory:940->waitForCachedBlocks:658 » Timeout
  TestCacheDirectives.testReplicationFactor:1032->waitForCachedBlocks:658 » Timeout
  TestCacheDirectives.testExpiry:1111->waitForCachedBlocks:658 » Timeout Timed o...
  TestCacheDirectives.testLimit:1169->waitForCachePoolStats:749 » Timeout Timed ...
  TestCacheDirectives.testExceedsCapacity »  test timed out after 60000 millisec...
  TestRollingUpgrade.testFinalize:361 » Bind Problem binding to [localhost:10000...
  TestRollingUpgrade.testQuery:405 » Bind Problem binding to [localhost:10000] j...
  TestRollingUpgrade.testCheckpoint:472 » Bind Problem binding to [localhost:100...
  TestRollingUpgradeRollback.testRollbackWithHAQJM:210 » Bind Problem binding to...
  TestWebHdfsWithMultipleNameNodes.testRedirect:130 » SocketTimeout Read timed o...
  TestHftpFileSystem.setUp:98 NoClassDefFound sun.security.x509.X509CertImpl
  TestHftpFileSystem.tearDown:109 NoClassDefFound sun.security.x509.X509CertImpl
  TestHttpsFileSystem.setUp:64 NoClassDefFound sun.security.x509.X509CertImpl
  TestHttpsFileSystem.tearDown:80 NullPointer
  TestRollingUpgradeDowngrade.testDowngrade:47 » Bind Problem binding to [localh...

Tests run: 2641, Failures: 11, Errors: 35, Skipped: 99

[ERROR] There was a timeout or other error in the fork
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (native_tests) @ hadoop-hdfs ---
[INFO] Executing tasks

main:
     [exec] 2015-01-22 02:48:31,557 WARN  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
     [exec] 2015-01-22 02:48:31,665 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(370)) - starting cluster: numNameNodes=1, numDataNodes=1
     [exec] Formatting using clusterid: testClusterID
     [exec] 2015-01-22 02:48:31,751 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(677)) - fsLock is fair:true
     [exec] 2015-01-22 02:48:31,775 INFO  namenode.HostFileManager (HostFileManager.java:refresh(304)) - read includes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:31,775 INFO  namenode.HostFileManager (HostFileManager.java:refresh(311)) - read excludes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:31,777 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1009)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
     [exec] 2015-01-22 02:48:31,777 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(234)) - dfs.block.invalidate.limit=1000
     [exec] 2015-01-22 02:48:31,777 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(240)) - dfs.namenode.datanode.registration.ip-hostname-check=true
     [exec] 2015-01-22 02:48:31,779 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
     [exec] 2015-01-22 02:48:31,779 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:31,786 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 512 MB = 10.2 MB
     [exec] 2015-01-22 02:48:31,786 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^20 = 1048576 entries
     [exec] 2015-01-22 02:48:31,811 INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(349)) - dfs.block.access.token.enable=false
     [exec] 2015-01-22 02:48:31,811 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(334)) - defaultReplication         = 1
     [exec] 2015-01-22 02:48:31,812 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(335)) - maxReplication             = 512
     [exec] 2015-01-22 02:48:31,814 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(336)) - minReplication             = 1
     [exec] 2015-01-22 02:48:31,814 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(337)) - maxReplicationStreams      = 2
     [exec] 2015-01-22 02:48:31,814 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(338)) - shouldCheckForEnoughRacks  = false
     [exec] 2015-01-22 02:48:31,814 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(339)) - replicationRecheckInterval = 3000
     [exec] 2015-01-22 02:48:31,814 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(340)) - encryptDataTransfer        = false
     [exec] 2015-01-22 02:48:31,814 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(341)) - maxNumBlocksToLog          = 1000
     [exec] 2015-01-22 02:48:31,815 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(694)) - fsOwner             = root (auth:SIMPLE)
     [exec] 2015-01-22 02:48:31,815 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(695)) - supergroup          = supergroup
     [exec] 2015-01-22 02:48:31,815 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(696)) - isPermissionEnabled = true
     [exec] 2015-01-22 02:48:31,816 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(707)) - HA Enabled: false
     [exec] 2015-01-22 02:48:31,818 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(744)) - Append Enabled: true
     [exec] 2015-01-22 02:48:32,715 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
     [exec] 2015-01-22 02:48:32,715 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:32,716 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 512 MB = 5.1 MB
     [exec] 2015-01-22 02:48:32,716 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^19 = 524288 entries
     [exec] 2015-01-22 02:48:32,773 INFO  namenode.NameNode (FSDirectory.java:<init>(204)) - Caching file names occuring more than 10 times
     [exec] 2015-01-22 02:48:32,781 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
     [exec] 2015-01-22 02:48:32,782 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:32,782 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 512 MB = 1.3 MB
     [exec] 2015-01-22 02:48:32,782 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^17 = 131072 entries
     [exec] 2015-01-22 02:48:32,784 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4712)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
     [exec] 2015-01-22 02:48:32,785 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4713)) - dfs.namenode.safemode.min.datanodes = 0
     [exec] 2015-01-22 02:48:32,785 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4714)) - dfs.namenode.safemode.extension     = 0
     [exec] 2015-01-22 02:48:32,786 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(828)) - Retry cache on namenode is enabled
     [exec] 2015-01-22 02:48:32,786 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(836)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
     [exec] 2015-01-22 02:48:32,788 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
     [exec] 2015-01-22 02:48:32,793 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:32,794 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 512 MB = 157.3 KB
     [exec] 2015-01-22 02:48:32,794 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^14 = 16384 entries
     [exec] 2015-01-22 02:48:32,809 INFO  namenode.AclConfigFlag (AclConfigFlag.java:<init>(40)) - ACLs enabled? false
     [exec] 2015-01-22 02:48:32,894 INFO  namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-1369820412-192.168.250.172-1421894912822
     [exec] 2015-01-22 02:48:33,019 INFO  common.Storage (NNStorage.java:format(550)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1 has been successfully formatted.
     [exec] 2015-01-22 02:48:33,059 INFO  common.Storage (NNStorage.java:format(550)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2 has been successfully formatted.
     [exec] 2015-01-22 02:48:33,222 INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(201)) - Going to retain 1 images with txid >= 0
     [exec] 2015-01-22 02:48:33,226 INFO  namenode.NameNode (NameNode.java:createNameNode(1298)) - createNameNode []
     [exec] 2015-01-22 02:48:33,264 WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
     [exec] 2015-01-22 02:48:33,369 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(344)) - Scheduled snapshot period at 10 second(s).
     [exec] 2015-01-22 02:48:33,369 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(183)) - NameNode metrics system started
     [exec] 2015-01-22 02:48:33,372 INFO  namenode.NameNode (NameNode.java:setClientNamenodeAddress(337)) - fs.defaultFS is hdfs://127.0.0.1:0
     [exec] 2015-01-22 02:48:33,401 INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1654)) - Starting web server as: ${dfs.web.authentication.kerberos.principal}
     [exec] 2015-01-22 02:48:33,401 INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1665)) - Starting Web-server for hdfs at: http://localhost:0
     [exec] 2015-01-22 02:48:33,434 INFO  mortbay.log (Slf4jLog.java:info(67)) - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
     [exec] 2015-01-22 02:48:33,438 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
     [exec] 2015-01-22 02:48:33,445 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(667)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
     [exec] 2015-01-22 02:48:33,446 INFO  http.HttpServer2 (HttpServer2.java:addFilter(645)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
     [exec] 2015-01-22 02:48:33,446 INFO  http.HttpServer2 (HttpServer2.java:addFilter(652)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
     [exec] 2015-01-22 02:48:33,459 INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
     [exec] 2015-01-22 02:48:33,460 INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(571)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
     [exec] 2015-01-22 02:48:33,484 INFO  http.HttpServer2 (HttpServer2.java:openListeners(855)) - Jetty bound to port 44040
     [exec] 2015-01-22 02:48:33,484 INFO  mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
     [exec] 2015-01-22 02:48:33,694 WARN  server.AuthenticationFilter (AuthenticationFilter.java:init(162)) - 'signature.secret' configuration not set, using a random value as secret
     [exec] 2015-01-22 02:48:33,723 INFO  mortbay.log (Slf4jLog.java:info(67)) - Started SelectChannelConnector@localhost:44040
     [exec] 2015-01-22 02:48:33,725 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(677)) - fsLock is fair:true
     [exec] 2015-01-22 02:48:33,726 INFO  namenode.HostFileManager (HostFileManager.java:refresh(304)) - read includes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:33,727 INFO  namenode.HostFileManager (HostFileManager.java:refresh(311)) - read excludes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:33,727 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(234)) - dfs.block.invalidate.limit=1000
     [exec] 2015-01-22 02:48:33,727 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(240)) - dfs.namenode.datanode.registration.ip-hostname-check=true
     [exec] 2015-01-22 02:48:33,727 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
     [exec] 2015-01-22 02:48:33,728 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:33,728 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 512 MB = 10.2 MB
     [exec] 2015-01-22 02:48:33,728 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^20 = 1048576 entries
     [exec] 2015-01-22 02:48:33,745 INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(349)) - dfs.block.access.token.enable=false
     [exec] 2015-01-22 02:48:33,745 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(334)) - defaultReplication         = 1
     [exec] 2015-01-22 02:48:33,746 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(335)) - maxReplication             = 512
     [exec] 2015-01-22 02:48:33,746 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(336)) - minReplication             = 1
     [exec] 2015-01-22 02:48:33,746 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(337)) - maxReplicationStreams      = 2
     [exec] 2015-01-22 02:48:33,746 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(338)) - shouldCheckForEnoughRacks  = false
     [exec] 2015-01-22 02:48:33,746 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(339)) - replicationRecheckInterval = 3000
     [exec] 2015-01-22 02:48:33,747 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(340)) - encryptDataTransfer        = false
     [exec] 2015-01-22 02:48:33,747 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(341)) - maxNumBlocksToLog          = 1000
     [exec] 2015-01-22 02:48:33,747 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(694)) - fsOwner             = root (auth:SIMPLE)
     [exec] 2015-01-22 02:48:33,747 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(695)) - supergroup          = supergroup
     [exec] 2015-01-22 02:48:33,748 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(696)) - isPermissionEnabled = true
     [exec] 2015-01-22 02:48:33,748 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(707)) - HA Enabled: false
     [exec] 2015-01-22 02:48:33,749 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(744)) - Append Enabled: true
     [exec] 2015-01-22 02:48:33,751 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
     [exec] 2015-01-22 02:48:33,751 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:33,752 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 512 MB = 5.1 MB
     [exec] 2015-01-22 02:48:33,753 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^19 = 524288 entries
     [exec] 2015-01-22 02:48:33,768 INFO  namenode.NameNode (FSDirectory.java:<init>(204)) - Caching file names occuring more than 10 times
     [exec] 2015-01-22 02:48:33,769 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
     [exec] 2015-01-22 02:48:33,769 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:33,770 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 512 MB = 1.3 MB
     [exec] 2015-01-22 02:48:33,771 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^17 = 131072 entries
     [exec] 2015-01-22 02:48:33,780 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4712)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
     [exec] 2015-01-22 02:48:33,781 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4713)) - dfs.namenode.safemode.min.datanodes = 0
     [exec] 2015-01-22 02:48:33,781 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4714)) - dfs.namenode.safemode.extension     = 0
     [exec] 2015-01-22 02:48:33,781 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(828)) - Retry cache on namenode is enabled
     [exec] 2015-01-22 02:48:33,781 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(836)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
     [exec] 2015-01-22 02:48:33,782 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
     [exec] 2015-01-22 02:48:33,782 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:33,783 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 512 MB = 157.3 KB
     [exec] 2015-01-22 02:48:33,783 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^14 = 16384 entries
     [exec] 2015-01-22 02:48:33,785 INFO  namenode.AclConfigFlag (AclConfigFlag.java:<init>(40)) - ACLs enabled? false
     [exec] 2015-01-22 02:48:33,811 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/in_use.lock acquired by nodename 21124@sovmp172
     [exec] 2015-01-22 02:48:33,823 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/in_use.lock acquired by nodename 21124@sovmp172
     [exec] 2015-01-22 02:48:33,827 INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(322)) - Recovering unfinalized segments in /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current
     [exec] 2015-01-22 02:48:33,827 INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(322)) - Recovering unfinalized segments in /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/current
     [exec] 2015-01-22 02:48:33,828 INFO  namenode.FSImage (FSImage.java:loadFSImage(642)) - No edit log streams selected.
     [exec] 2015-01-22 02:48:33,840 INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(173)) - Loading 1 INodes.
     [exec] 2015-01-22 02:48:33,845 INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(170)) - Loaded FSImage in 0 seconds.
     [exec] 2015-01-22 02:48:33,846 INFO  namenode.FSImage (FSImage.java:loadFSImage(915)) - Loaded image for txid 0 from /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current/fsimage_0000000000000000000
     [exec] 2015-01-22 02:48:33,849 INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(897)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
     [exec] 2015-01-22 02:48:33,850 INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1112)) - Starting log segment at 1
     [exec] 2015-01-22 02:48:35,147 INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
     [exec] 2015-01-22 02:48:35,148 INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(645)) - Finished loading FSImage in 1362 msecs
     [exec] 2015-01-22 02:48:35,290 INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(295)) - RPC server is binding to localhost:0
     [exec] 2015-01-22 02:48:35,295 INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
     [exec] 2015-01-22 02:48:35,305 INFO  ipc.Server (Server.java:run(593)) - Starting Socket Reader #1 for port 34324
     [exec] 2015-01-22 02:48:35,367 INFO  namenode.NameNode (NameNode.java:initialize(567)) - Clients are to use localhost:34324 to access this namenode/service.
     [exec] 2015-01-22 02:48:35,374 INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5643)) - Registered FSNamesystemState MBean
     [exec] 2015-01-22 02:48:35,388 INFO  namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(5299)) - Number of blocks under construction: 0
     [exec] 2015-01-22 02:48:35,388 INFO  namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(5299)) - Number of blocks under construction: 0
     [exec] 2015-01-22 02:48:35,388 INFO  namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1067)) - initializing replication queues
     [exec] 2015-01-22 02:48:35,389 INFO  hdfs.StateChange (FSNamesystem.java:leave(4786)) - STATE* Leaving safe mode after 1 secs
     [exec] 2015-01-22 02:48:35,389 INFO  hdfs.StateChange (FSNamesystem.java:leave(4797)) - STATE* Network topology has 0 racks and 0 datanodes
     [exec] 2015-01-22 02:48:35,389 INFO  hdfs.StateChange (FSNamesystem.java:leave(4800)) - STATE* UnderReplicatedBlocks has 0 blocks
     [exec] 2015-01-22 02:48:35,416 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2489)) - Total number of blocks            = 0
     [exec] 2015-01-22 02:48:35,417 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2490)) - Number of invalid blocks          = 0
     [exec] 2015-01-22 02:48:35,417 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2491)) - Number of under-replicated blocks = 0
     [exec] 2015-01-22 02:48:35,418 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2492)) - Number of  over-replicated blocks = 0
     [exec] 2015-01-22 02:48:35,418 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2494)) - Number of blocks being written    = 0
     [exec] 2015-01-22 02:48:35,419 INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2495)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 30 msec
     [exec] 2015-01-22 02:48:35,437 INFO  ipc.Server (Server.java:run(662)) - IPC Server listener on 34324: starting
     [exec] 2015-01-22 02:48:35,437 INFO  ipc.Server (Server.java:run(815)) - IPC Server Responder: starting
     [exec] 2015-01-22 02:48:35,440 INFO  namenode.NameNode (NameNode.java:startCommonServices(609)) - NameNode RPC up at: localhost/127.0.0.1:34324
     [exec] 2015-01-22 02:48:35,440 INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(997)) - Starting services required for active state
     [exec] 2015-01-22 02:48:35,442 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(159)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
     [exec] 2015-01-22 02:48:35,443 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(172)) - Rescanning because of pending operations
     [exec] 2015-01-22 02:48:35,447 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(202)) - Scanned 0 directive(s) and 0 block(s) in 5 millisecond(s).
     [exec] 2015-01-22 02:48:35,447 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1290)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1,[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2
     [exec] 2015-01-22 02:48:35,478 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(151)) - DataNode metrics system started (again)
     [exec] 2015-01-22 02:48:35,479 INFO  datanode.DataNode (DataNode.java:<init>(288)) - Configured hostname is 127.0.0.1
     [exec] 2015-01-22 02:48:35,480 INFO  datanode.DataNode (DataNode.java:startDataNode(764)) - Starting DataNode with maxLockedMemory = 0
     [exec] 2015-01-22 02:48:35,486 INFO  datanode.DataNode (DataNode.java:initDataXceiver(564)) - Opened streaming server at /127.0.0.1:34194
     [exec] 2015-01-22 02:48:35,487 INFO  datanode.DataNode (DataXceiverServer.java:<init>(73)) - Balancing bandwith is 1048576 bytes/s
     [exec] 2015-01-22 02:48:35,495 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
     [exec] 2015-01-22 02:48:35,496 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(667)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
     [exec] 2015-01-22 02:48:35,497 INFO  http.HttpServer2 (HttpServer2.java:addFilter(645)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
     [exec] 2015-01-22 02:48:35,498 INFO  http.HttpServer2 (HttpServer2.java:addFilter(652)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
     [exec] 2015-01-22 02:48:35,501 INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(571)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
     [exec] 2015-01-22 02:48:35,502 INFO  http.HttpServer2 (HttpServer2.java:openListeners(855)) - Jetty bound to port 33374
     [exec] 2015-01-22 02:48:35,502 INFO  mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
     [exec] 2015-01-22 02:48:35,639 INFO  mortbay.log (Slf4jLog.java:info(67)) - Started SelectChannelConnector@localhost:33374
     [exec] 2015-01-22 02:48:35,640 INFO  datanode.DataNode (DataNode.java:startDataNode(781)) - dnUserName = root
     [exec] 2015-01-22 02:48:35,640 INFO  datanode.DataNode (DataNode.java:startDataNode(782)) - supergroup = supergroup
     [exec] 2015-01-22 02:48:35,646 INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
     [exec] 2015-01-22 02:48:35,647 INFO  ipc.Server (Server.java:run(593)) - Starting Socket Reader #1 for port 38727
     [exec] 2015-01-22 02:48:35,654 INFO  datanode.DataNode (DataNode.java:initIpcServer(439)) - Opened IPC server at /127.0.0.1:38727
     [exec] 2015-01-22 02:48:35,667 INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
     [exec] 2015-01-22 02:48:35,669 INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
     [exec] 2015-01-22 02:48:35,674 INFO  datanode.DataNode (BPServiceActor.java:run(809)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:34324 starting to offer service
     [exec] 2015-01-22 02:48:35,675 INFO  ipc.Server (Server.java:run(815)) - IPC Server Responder: starting
     [exec] 2015-01-22 02:48:35,675 INFO  ipc.Server (Server.java:run(662)) - IPC Server listener on 38727: starting
     [exec] 2015-01-22 02:48:35,923 INFO  common.Storage (DataStorage.java:recoverTransitionRead(173)) - Data-node version: -55 and name-node layout version: -56
     [exec] 2015-01-22 02:48:35,955 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 21124@sovmp172
     [exec] 2015-01-22 02:48:35,955 INFO  common.Storage (DataStorage.java:recoverTransitionRead(197)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1 is not formatted
     [exec] 2015-01-22 02:48:35,956 INFO  common.Storage (DataStorage.java:recoverTransitionRead(198)) - Formatting ...
     [exec] 2015-01-22 02:48:35,967 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:35,967 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:35,992 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 21124@sovmp172
     [exec] 2015-01-22 02:48:35,993 INFO  common.Storage (DataStorage.java:recoverTransitionRead(197)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2 is not formatted
     [exec] 2015-01-22 02:48:35,993 INFO  common.Storage (DataStorage.java:recoverTransitionRead(198)) - Formatting ...
     [exec] 2015-01-22 02:48:36,070 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:36,071 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:36,103 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(108)) - Analyzing storage directories for bpid BP-1369820412-192.168.250.172-1421894912822
     [exec] 2015-01-22 02:48:36,103 INFO  common.Storage (Storage.java:lock(666)) - Locking is disabled
     [exec] 2015-01-22 02:48:36,104 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(130)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current/BP-1369820412-192.168.250.172-1421894912822 is not formatted.
     [exec] 2015-01-22 02:48:36,105 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(131)) - Formatting ...
     [exec] 2015-01-22 02:48:36,105 INFO  common.Storage (BlockPoolSliceStorage.java:format(183)) - Formatting block pool BP-1369820412-192.168.250.172-1421894912822 directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current/BP-1369820412-192.168.250.172-1421894912822/current
     [exec] 2015-01-22 02:48:36,128 INFO  common.Storage (Storage.java:lock(666)) - Locking is disabled
     [exec] 2015-01-22 02:48:36,129 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(130)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current/BP-1369820412-192.168.250.172-1421894912822 is not formatted.
     [exec] 2015-01-22 02:48:36,129 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(131)) - Formatting ...
     [exec] 2015-01-22 02:48:36,129 INFO  common.Storage (BlockPoolSliceStorage.java:format(183)) - Formatting block pool BP-1369820412-192.168.250.172-1421894912822 directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current/BP-1369820412-192.168.250.172-1421894912822/current
     [exec] 2015-01-22 02:48:36,149 INFO  common.Storage (BlockPoolSliceStorage.java:doTransition(254)) - Restored 0 block files from trash.
     [exec] 2015-01-22 02:48:36,149 INFO  common.Storage (BlockPoolSliceStorage.java:doTransition(254)) - Restored 0 block files from trash.
     [exec] 2015-01-22 02:48:36,173 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:36,173 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:36,220 INFO  datanode.DataNode (DataNode.java:initStorage(976)) - Setting up storage: nsid=1363446927;bpid=BP-1369820412-192.168.250.172-1421894912822;lv=-55;nsInfo=lv=-56;cid=testClusterID;nsid=1363446927;c=0;bpid=BP-1369820412-192.168.250.172-1421894912822;dnuuid=null
     [exec] 2015-01-22 02:48:36,256 INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(809)) - Generated and persisted new Datanode UUID 25a08130-115d-4e4b-bd3a-ae15a6c3be03
     [exec] 2015-01-22 02:48:36,287 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:36,287 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:36,291 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:<init>(216)) - Added volume - /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current, StorageType: DISK
     [exec] 2015-01-22 02:48:36,292 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:<init>(216)) - Added volume - /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current, StorageType: DISK
     [exec] 2015-01-22 02:48:36,301 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1378)) - Registered FSDatasetState MBean
     [exec] 2015-01-22 02:48:36,304 INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1421896403304 with interval 21600000
     [exec] 2015-01-22 02:48:36,305 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(1747)) - Adding block pool BP-1369820412-192.168.250.172-1421894912822
     [exec] 2015-01-22 02:48:36,306 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(207)) - Scanning block pool BP-1369820412-192.168.250.172-1421894912822 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current...
     [exec] 2015-01-22 02:48:36,306 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(207)) - Scanning block pool BP-1369820412-192.168.250.172-1421894912822 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current...
     [exec] 2015-01-22 02:48:36,320 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(212)) - Time taken to scan block pool BP-1369820412-192.168.250.172-1421894912822 on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current: 14ms
     [exec] 2015-01-22 02:48:36,326 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(212)) - Time taken to scan block pool BP-1369820412-192.168.250.172-1421894912822 on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current: 19ms
     [exec] 2015-01-22 02:48:36,326 INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(236)) - Total time to scan all replicas for block pool BP-1369820412-192.168.250.172-1421894912822: 20ms
     [exec] 2015-01-22 02:48:36,327 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(108)) - Adding replicas to map for block pool BP-1369820412-192.168.250.172-1421894912822 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current...
     [exec] 2015-01-22 02:48:36,327 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(108)) - Adding replicas to map for block pool BP-1369820412-192.168.250.172-1421894912822 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current...
     [exec] 2015-01-22 02:48:36,327 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(113)) - Time to add replicas to map for block pool BP-1369820412-192.168.250.172-1421894912822 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current: 0ms
     [exec] 2015-01-22 02:48:36,327 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(113)) - Time to add replicas to map for block pool BP-1369820412-192.168.250.172-1421894912822 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current: 0ms
     [exec] 2015-01-22 02:48:36,328 INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(136)) - Total time to add all replicas to map: 2ms
     [exec] 2015-01-22 02:48:36,330 INFO  datanode.DataNode (BPServiceActor.java:register(769)) - Block pool BP-1369820412-192.168.250.172-1421894912822 (Datanode Uuid null) service to localhost/127.0.0.1:34324 beginning handshake with NN
     [exec] 2015-01-22 02:48:36,338 INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(822)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=25a08130-115d-4e4b-bd3a-ae15a6c3be03, infoPort=33374, ipcPort=38727, storageInfo=lv=-55;cid=testClusterID;nsid=1363446927;c=0) storage 25a08130-115d-4e4b-bd3a-ae15a6c3be03
     [exec] 2015-01-22 02:48:36,342 INFO  net.NetworkTopology (NetworkTopology.java:add(413)) - Adding a new node: /default-rack/127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,345 INFO  datanode.DataNode (BPServiceActor.java:register(782)) - Block pool Block pool BP-1369820412-192.168.250.172-1421894912822 (Datanode Uuid null) service to localhost/127.0.0.1:34324 successfully registered with NN
     [exec] 2015-01-22 02:48:36,346 INFO  datanode.DataNode (BPServiceActor.java:offerService(641)) - For namenode localhost/127.0.0.1:34324 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
     [exec] 2015-01-22 02:48:36,355 INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(667)) - Adding new storage ID DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3 for DN 127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,356 INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(667)) - Adding new storage ID DS-00010471-70c0-4f13-acea-0ce70a1ad7c3 for DN 127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,363 INFO  datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(439)) - Namenode Block pool BP-1369820412-192.168.250.172-1421894912822 (Datanode Uuid 25a08130-115d-4e4b-bd3a-ae15a6c3be03) service to localhost/127.0.0.1:34324 trying to claim ACTIVE state with txid=1
     [exec] 2015-01-22 02:48:36,363 INFO  datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(451)) - Acknowledging ACTIVE Namenode Block pool BP-1369820412-192.168.250.172-1421894912822 (Datanode Uuid 25a08130-115d-4e4b-bd3a-ae15a6c3be03) service to localhost/127.0.0.1:34324
     [exec] 2015-01-22 02:48:36,368 INFO  blockmanagement.BlockManager (BlockManager.java:processReport(1707)) - BLOCK* processReport: Received first block report from org.apache.hadoop.hdfs.server.protocol.DatanodeStorage@4c8df517 after starting up or becoming active. Its block contents are no longer considered stale
     [exec] 2015-01-22 02:48:36,369 INFO  BlockStateChange (BlockManager.java:processReport(1723)) - BLOCK* processReport: from storage DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3 node DatanodeRegistration(127.0.0.1, datanodeUuid=25a08130-115d-4e4b-bd3a-ae15a6c3be03, infoPort=33374, ipcPort=38727, storageInfo=lv=-55;cid=testClusterID;nsid=1363446927;c=0), blocks: 0, processing time: 1 msecs
     [exec] 2015-01-22 02:48:36,369 INFO  blockmanagement.BlockManager (BlockManager.java:processReport(1707)) - BLOCK* processReport: Received first block report from org.apache.hadoop.hdfs.server.protocol.DatanodeStorage@5ab3ae56 after starting up or becoming active. Its block contents are no longer considered stale
     [exec] 2015-01-22 02:48:36,369 INFO  BlockStateChange (BlockManager.java:processReport(1723)) - BLOCK* processReport: from storage DS-00010471-70c0-4f13-acea-0ce70a1ad7c3 node DatanodeRegistration(127.0.0.1, datanodeUuid=25a08130-115d-4e4b-bd3a-ae15a6c3be03, infoPort=33374, ipcPort=38727, storageInfo=lv=-55;cid=testClusterID;nsid=1363446927;c=0), blocks: 0, processing time: 0 msecs
     [exec] 2015-01-22 02:48:36,378 INFO  datanode.DataNode (BPServiceActor.java:blockReport(502)) - Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 14 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4187acde
     [exec] 2015-01-22 02:48:36,379 INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(618)) - Got finalize command for block pool BP-1369820412-192.168.250.172-1421894912822
     [exec] 2015-01-22 02:48:36,380 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
     [exec] 2015-01-22 02:48:36,380 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:36,381 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 512 MB = 2.6 MB
     [exec] 2015-01-22 02:48:36,381 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
     [exec] testHdfsOperations(threadIdx=0): starting
     [exec] testHdfsOperations(threadIdx=1): starting
     [exec] testHdfsOperations(threadIdx=2): starting
     [exec] 2015-01-22 02:48:36,406 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(186)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1369820412-192.168.250.172-1421894912822
     [exec] 2015-01-22 02:48:36,416 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2055)) - Cluster is active
     [exec] 2015-01-22 02:48:36,417 INFO  datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(265)) - Added bpid=BP-1369820412-192.168.250.172-1421894912822 to blockPoolScannerMap, new size=1
     [exec] 2015-01-22 02:48:36,457 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1009)) - dfs.block.size is deprecated. Instead, use dfs.blocksize
     [exec] 2015-01-22 02:48:36,465 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,467 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,467 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,491 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/tlhData0002	dst=null	perm=root:supergroup:rwxr-xr-x
     [exec] 2015-01-22 02:48:36,499 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/tlhData0001	dst=null	perm=root:supergroup:rwxr-xr-x
     [exec] hdfsOpenFile(/tlhData0002/file1): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:
     [exec] hdfsOpenFile(/tlhData0001/file1): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:
     [exec] java.io.FileNotFoundException: File does not exist: /tlhData0002/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
     [exec] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
     [exec] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1144)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1122)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:264)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:231)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:224)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1295)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:300)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:296)
     [exec] 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:296)
     [exec] Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tlhData0002/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:620)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:219)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
     [exec] 	... 10 more
     [exec] ERROR: cannot open an hdfs file in mode 0x3
     [exec] java.io.FileNotFoundException: File does not exist: /tlhData0001/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
     [exec] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
     [exec] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1144)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1122)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:264)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:231)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:224)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1295)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:300)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:296)
     [exec] 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:296)
     [exec] Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tlhData0001/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:620)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:219)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
     [exec] 	... 10 more
     [exec] ERROR: cannot open an hdfs file in mode 0x3
     [exec] hdfsOpenFile(/tlhData0000/file1): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:
     [exec] java.io.FileNotFoundException: File does not exist: /tlhData0000/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
     [exec] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
     [exec] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1144)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1122)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:264)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:231)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:224)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1295)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:300)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:296)
     [exec] 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:296)
     [exec] Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tlhData0000/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:620)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:219)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
     [exec] 	... 10 more
     [exec] ERROR: cannot open an hdfs file in mode 0x3
     [exec] 2015-01-22 02:48:36,507 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/tlhData0000	dst=null	perm=root:supergroup:rwxr-xr-x
     [exec] 2015-01-22 02:48:36,510 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 6 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:40238 Call#15 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0001/file1
     [exec] 2015-01-22 02:48:36,510 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 7 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:40238 Call#16 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0002/file1
     [exec] 2015-01-22 02:48:36,512 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 7 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:40238 Call#17 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0000/file1
     [exec] 2015-01-22 02:48:36,547 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0002/file1	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:36,555 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0001/file1	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:36,567 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0000/file1	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:36,576 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0001/file1. BP-1369820412-192.168.250.172-1421894912822 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:36,577 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0002/file1. BP-1369820412-192.168.250.172-1421894912822 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00010471-70c0-4f13-acea-0ce70a1ad7c3:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:36,596 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0000/file1. BP-1369820412-192.168.250.172-1421894912822 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:36,649 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-1369820412-192.168.250.172-1421894912822:blk_1073741826_1002 src: /127.0.0.1:56035 dest: /127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,650 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-1369820412-192.168.250.172-1421894912822:blk_1073741825_1001 src: /127.0.0.1:56034 dest: /127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,661 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-1369820412-192.168.250.172-1421894912822:blk_1073741827_1003 src: /127.0.0.1:56036 dest: /127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,747 INFO  hdfs.StateChange (FSNamesystem.java:fsync(3699)) - BLOCK* fsync: /tlhData0002/file1 for DFSClient_NONMAPREDUCE_-788636501_132
     [exec] 2015-01-22 02:48:36,759 INFO  hdfs.StateChange (FSNamesystem.java:fsync(3699)) - BLOCK* fsync: /tlhData0001/file1 for DFSClient_NONMAPREDUCE_-1511522266_133
     [exec] 2015-01-22 02:48:36,763 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:56035, dest: /127.0.0.1:34194, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-788636501_132, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741826_1002, duration: 84457317
     [exec] 2015-01-22 02:48:36,763 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-1369820412-192.168.250.172-1421894912822:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:36,765 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:34194 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]} size 12
     [exec] 2015-01-22 02:48:36,791 INFO  hdfs.StateChange (FSNamesystem.java:fsync(3699)) - BLOCK* fsync: /tlhData0000/file1 for DFSClient_NONMAPREDUCE_-704960448_131
     [exec] 2015-01-22 02:48:36,793 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:56034, dest: /127.0.0.1:34194, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1511522266_133, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741825_1001, duration: 115629696
     [exec] 2015-01-22 02:48:36,793 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-1369820412-192.168.250.172-1421894912822:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:36,794 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:34194 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00010471-70c0-4f13-acea-0ce70a1ad7c3:NORMAL|RBW]]} size 12
     [exec] 2015-01-22 02:48:36,802 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0002/file1 is closed by DFSClient_NONMAPREDUCE_-788636501_132
     [exec] 2015-01-22 02:48:36,822 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:56036, dest: /127.0.0.1:34194, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-704960448_131, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741827_1003, duration: 143506160
     [exec] 2015-01-22 02:48:36,822 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,823 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-1369820412-192.168.250.172-1421894912822:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:36,822 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:34194 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]} size 12
     [exec] 2015-01-22 02:48:36,830 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0001/file1 is closed by DFSClient_NONMAPREDUCE_-1511522266_133
     [exec] 2015-01-22 02:48:36,833 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,846 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0000/file1 is closed by DFSClient_NONMAPREDUCE_-704960448_131
     [exec] 2015-01-22 02:48:36,854 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:34194, dest: /127.0.0.1:56037, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1511522266_133, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741825_1001, duration: 1340031
     [exec] 2015-01-22 02:48:36,854 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:34194, dest: /127.0.0.1:56038, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-788636501_132, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741826_1002, duration: 1368685
     [exec] 2015-01-22 02:48:36,861 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,861 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,863 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,863 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,864 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,866 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,867 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,867 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:34194, dest: /127.0.0.1:56038, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-704960448_131, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741827_1003, duration: 358832
     [exec] 2015-01-22 02:48:36,869 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,882 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0002/file2	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:36,883 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,886 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,890 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0001/file2	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:36,898 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0000/file2	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:36,904 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:34194, dest: /127.0.0.1:56037, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-788636501_132, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741826_1002, duration: 384266
     [exec] 2015-01-22 02:48:36,907 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0002/file2. BP-1369820412-192.168.250.172-1421894912822 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:36,932 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:34194, dest: /127.0.0.1:56038, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1511522266_133, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741825_1001, duration: 464558
     [exec] 2015-01-22 02:48:36,935 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0001/file2. BP-1369820412-192.168.250.172-1421894912822 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:36,936 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-1369820412-192.168.250.172-1421894912822:blk_1073741828_1004 src: /127.0.0.1:56040 dest: /127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,940 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:56040, dest: /127.0.0.1:34194, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-788636501_132, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741828_1004, duration: 1731103
     [exec] 2015-01-22 02:48:36,940 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-1369820412-192.168.250.172-1421894912822:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:36,941 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:34194 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00010471-70c0-4f13-acea-0ce70a1ad7c3:NORMAL|RBW]]} size 0
     [exec] 2015-01-22 02:48:36,943 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:34194, dest: /127.0.0.1:56041, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-704960448_131, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741827_1003, duration: 520979
     [exec] 2015-01-22 02:48:36,948 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0000/file2. BP-1369820412-192.168.250.172-1421894912822 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:36,949 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-1369820412-192.168.250.172-1421894912822:blk_1073741829_1005 src: /127.0.0.1:56042 dest: /127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,952 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:56042, dest: /127.0.0.1:34194, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1511522266_133, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741829_1005, duration: 1826068
     [exec] 2015-01-22 02:48:36,953 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-1369820412-192.168.250.172-1421894912822:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:36,954 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:34194 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-d2a45eb5-3c84-469c-9596-7aeaf80f6fc3:NORMAL|RBW]]} size 0
     [exec] 2015-01-22 02:48:36,958 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0002/file2 is closed by DFSClient_NONMAPREDUCE_-788636501_132
     [exec] 2015-01-22 02:48:36,968 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-1369820412-192.168.250.172-1421894912822:blk_1073741830_1006 src: /127.0.0.1:56043 dest: /127.0.0.1:34194
     [exec] 2015-01-22 02:48:36,972 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:56043, dest: /127.0.0.1:34194, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-704960448_131, offset: 0, srvID: 25a08130-115d-4e4b-bd3a-ae15a6c3be03, blockid: BP-1369820412-192.168.250.172-1421894912822:blk_1073741830_1006, duration: 1247385
     [exec] 2015-01-22 02:48:36,972 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-1369820412-192.168.250.172-1421894912822:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:36,972 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:34194 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-00010471-70c0-4f13-acea-0ce70a1ad7c3:NORMAL|RBW]]} size 0
     [exec] 2015-01-22 02:48:36,973 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0001/file2 is closed by DFSClient_NONMAPREDUCE_-1511522266_133
     [exec] 2015-01-22 02:48:36,982 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file2	dst=null	perm=root:doop:rw-r--r--
     [exec] 2015-01-22 02:48:36,989 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0000/file2 is closed by DFSClient_NONMAPREDUCE_-704960448_131
     [exec] 2015-01-22 02:48:36,991 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:36,997 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file2	dst=null	perm=root:doop:rw-r--r--
     [exec] 2015-01-22 02:48:36,999 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,001 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,005 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file2	dst=null	perm=root:doop:rw-r--r--
     [exec] 2015-01-22 02:48:37,007 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,008 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,013 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file2	dst=null	perm=ha:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:37,015 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,017 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,021 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file2	dst=null	perm=ha:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:37,023 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,025 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,029 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file2	dst=null	perm=ha:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:37,031 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,032 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,037 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file2	dst=null	perm=ha2:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:37,039 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,040 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,045 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file2	dst=null	perm=ha2:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:37,046 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,048 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,049 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 2 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:40238 Call#89 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0002/nonexistent-file-name
     [exec] 2015-01-22 02:48:37,051 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,053 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 0 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:40238 Call#91 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0001/nonexistent-file-name
     [exec] 2015-01-22 02:48:37,053 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file2	dst=null	perm=ha2:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:37,055 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,091 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,106 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 4 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:40238 Call#95 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0000/nonexistent-file-name
     [exec] 2015-01-22 02:48:37,119 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(86)) - got exception trying to get groups for user foo
     [exec] org.apache.hadoop.util.Shell$ExitCodeException: id: foo: no such user
     [exec] 
     [exec] 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
     [exec] 	at org.apache.hadoop.util.Shell.run(Shell.java:418)
     [exec] 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
     [exec] 	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
     [exec] 	at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:81)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3324)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwnerInt(FSNamesystem.java:1592)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1581)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:563)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 2015-01-22 02:48:37,155 WARN  security.UserGroupInformation (UserGroupInformation.java:getGroupNames(1420)) - No groups available for user foo
     [exec] 2015-01-22 02:48:37,156 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=false	ugi=foo (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,156 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 5 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:40249 Call#94 Retry#0: org.apache.hadoop.security.AccessControlException: Permission denied
     [exec] 2015-01-22 02:48:37,169 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(86)) - got exception trying to get groups for user foo
     [exec] org.apache.hadoop.util.Shell$ExitCodeException: id: foo: no such user
     [exec] 
     [exec] 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
     [exec] 	at org.apache.hadoop.util.Shell.run(Shell.java:418)
     [exec] 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
     [exec] 	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
     [exec] 	at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:81)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3324)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwnerInt(FSNamesystem.java:1592)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1581)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:563)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 2015-01-22 02:48:37,171 WARN  security.UserGroupInformation (UserGroupInformation.java:getGroupNames(1420)) - No groups available for user foo
     [exec] 2015-01-22 02:48:37,172 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=false	ugi=foo (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,172 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 8 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:40250 Call#96 Retry#0: org.apache.hadoop.security.AccessControlException: Permission denied
     [exec] 2015-01-22 02:48:37,180 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(86)) - got exception trying to get groups for user foo
     [exec] org.apache.hadoop.util.Shell$ExitCodeException: id: foo: no such user
     [exec] 
     [exec] 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
     [exec] 	at org.apache.hadoop.util.Shell.run(Shell.java:418)
     [exec] 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
     [exec] 	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
     [exec] 	at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:81)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3324)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwnerInt(FSNamesystem.java:1592)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1581)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:563)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 2015-01-22 02:48:37,183 WARN  security.UserGroupInformation (UserGroupInformation.java:getGroupNames(1420)) - No groups available for user foo
     [exec] 2015-01-22 02:48:37,184 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=false	ugi=foo (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,185 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 9 on 34324, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:40251 Call#97 Retry#0: org.apache.hadoop.security.AccessControlException: Permission denied
     [exec] 2015-01-22 02:48:37,195 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741826_1002 127.0.0.1:34194 
     [exec] 2015-01-22 02:48:37,196 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741828_1004 127.0.0.1:34194 
     [exec] 2015-01-22 02:48:37,196 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tlhData0002	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,209 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:34194 
     [exec] 2015-01-22 02:48:37,209 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:34194 
     [exec] 2015-01-22 02:48:37,210 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tlhData0001	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,221 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741827_1003 127.0.0.1:34194 
     [exec] 2015-01-22 02:48:37,221 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:34194 
     [exec] 2015-01-22 02:48:37,221 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tlhData0000	dst=null	perm=null
     [exec] 2015-01-22 02:48:37,222 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1546)) - Shutting down the Mini HDFS Cluster
     [exec] 2015-01-22 02:48:37,223 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1578)) - Shutting down DataNode 0
     [exec] 2015-01-22 02:48:37,223 WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
     [exec] 2015-01-22 02:48:37,223 INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(240)) - Closing all peers.
     [exec] 2015-01-22 02:48:37,228 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped SelectChannelConnector@localhost:0
     [exec] 2015-01-22 02:48:37,229 INFO  datanode.DataNode (DataNode.java:shutdown(1303)) - Waiting for threadgroup to exit, active threads is 3
     [exec] 2015-01-22 02:48:37,232 INFO  datanode.DataNode (DataNode.java:shutdown(1303)) - Waiting for threadgroup to exit, active threads is 0
     [exec] 2015-01-22 02:48:37,232 INFO  ipc.Server (Server.java:stop(2398)) - Stopping server on 38727
     [exec] 2015-01-22 02:48:37,232 INFO  ipc.Server (Server.java:run(694)) - Stopping IPC Server listener on 38727
     [exec] 2015-01-22 02:48:37,233 WARN  datanode.DataNode (BPServiceActor.java:offerService(726)) - BPOfferService for Block pool BP-1369820412-192.168.250.172-1421894912822 (Datanode Uuid 25a08130-115d-4e4b-bd3a-ae15a6c3be03) service to localhost/127.0.0.1:34324 interrupted
     [exec] 2015-01-22 02:48:37,233 INFO  ipc.Server (Server.java:run(820)) - Stopping IPC Server Responder
     [exec] 2015-01-22 02:48:37,233 WARN  datanode.DataNode (BPServiceActor.java:run(849)) - Ending block pool service for: Block pool BP-1369820412-192.168.250.172-1421894912822 (Datanode Uuid 25a08130-115d-4e4b-bd3a-ae15a6c3be03) service to localhost/127.0.0.1:34324
     [exec] 2015-01-22 02:48:37,236 INFO  datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1369820412-192.168.250.172-1421894912822 (Datanode Uuid 25a08130-115d-4e4b-bd3a-ae15a6c3be03)
     [exec] 2015-01-22 02:48:37,237 INFO  datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(274)) - Removed bpid=BP-1369820412-192.168.250.172-1421894912822 from blockPoolScannerMap
     [exec] 2015-01-22 02:48:37,237 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(1757)) - Removing block pool BP-1369820412-192.168.250.172-1421894912822
     [exec] 2015-01-22 02:48:37,239 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(139)) - Shutting down all async disk service threads
     [exec] 2015-01-22 02:48:37,239 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(147)) - All async disk service threads have been shut down
     [exec] 2015-01-22 02:48:37,239 INFO  datanode.DataNode (DataNode.java:shutdown(1365)) - Shutdown complete.
     [exec] 2015-01-22 02:48:37,240 INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1095)) - Stopping services started for active state
     [exec] 2015-01-22 02:48:37,240 INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1153)) - Ending log segment 1
     [exec] 2015-01-22 02:48:37,241 INFO  namenode.FSNamesystem (FSNamesystem.java:run(4368)) - NameNodeEditLogRoller was interrupted, exiting
     [exec] 2015-01-22 02:48:37,249 INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(673)) - Number of transactions: 50 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 39 SyncTimes(ms): 236 240 
     [exec] 2015-01-22 02:48:37,251 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(130)) - Finalizing edits file /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000050
     [exec] 2015-01-22 02:48:37,252 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(130)) - Finalizing edits file /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000050
     [exec] 2015-01-22 02:48:37,257 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(168)) - Shutting down CacheReplicationMonitor
     [exec] 2015-01-22 02:48:37,258 INFO  ipc.Server (Server.java:stop(2398)) - Stopping server on 34324
     [exec] 2015-01-22 02:48:37,259 INFO  ipc.Server (Server.java:run(694)) - Stopping IPC Server listener on 34324
     [exec] 2015-01-22 02:48:37,259 INFO  blockmanagement.BlockManager (BlockManager.java:run(3363)) - Stopping ReplicationMonitor.
     [exec] 2015-01-22 02:48:37,259 INFO  ipc.Server (Server.java:run(820)) - Stopping IPtestLibHdfs: all threads succeeded.  SUCCESS.
     [exec] C Server Responder
     [exec] 2015-01-22 02:48:37,259 WARN  blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException
     [exec] 2015-01-22 02:48:37,293 INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1095)) - Stopping services started for active state
     [exec] 2015-01-22 02:48:37,293 INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1177)) - Stopping services started for standby state
     [exec] 2015-01-22 02:48:37,295 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped SelectChannelConnector@localhost:0
     [exec] 2015-01-22 02:48:37,296 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(200)) - Stopping DataNode metrics system...
     [exec] 2015-01-22 02:48:37,297 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(206)) - DataNode metrics system stopped.
     [exec] 2015-01-22 02:48:37,297 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(572)) - DataNode metrics system shutdown complete.
     [exec] 2015-01-22 02:48:38,690 WARN  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
     [exec] 2015-01-22 02:48:38,825 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(370)) - starting cluster: numNameNodes=1, numDataNodes=1
     [exec] Formatting using clusterid: testClusterID
     [exec] 2015-01-22 02:48:38,915 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(677)) - fsLock is fair:true
     [exec] 2015-01-22 02:48:38,940 INFO  namenode.HostFileManager (HostFileManager.java:refresh(304)) - read includes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:38,940 INFO  namenode.HostFileManager (HostFileManager.java:refresh(311)) - read excludes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:38,941 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1009)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
     [exec] 2015-01-22 02:48:38,941 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(234)) - dfs.block.invalidate.limit=1000
     [exec] 2015-01-22 02:48:38,942 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(240)) - dfs.namenode.datanode.registration.ip-hostname-check=true
     [exec] 2015-01-22 02:48:38,945 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
     [exec] 2015-01-22 02:48:38,945 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:38,952 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 512 MB = 10.2 MB
     [exec] 2015-01-22 02:48:38,952 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^20 = 1048576 entries
     [exec] 2015-01-22 02:48:38,979 INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(349)) - dfs.block.access.token.enable=false
     [exec] 2015-01-22 02:48:38,979 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(334)) - defaultReplication         = 1
     [exec] 2015-01-22 02:48:38,979 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(335)) - maxReplication             = 512
     [exec] 2015-01-22 02:48:38,981 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(336)) - minReplication             = 1
     [exec] 2015-01-22 02:48:38,982 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(337)) - maxReplicationStreams      = 2
     [exec] 2015-01-22 02:48:38,982 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(338)) - shouldCheckForEnoughRacks  = false
     [exec] 2015-01-22 02:48:38,982 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(339)) - replicationRecheckInterval = 3000
     [exec] 2015-01-22 02:48:38,982 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(340)) - encryptDataTransfer        = false
     [exec] 2015-01-22 02:48:38,982 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(341)) - maxNumBlocksToLog          = 1000
     [exec] 2015-01-22 02:48:38,983 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(694)) - fsOwner             = root (auth:SIMPLE)
     [exec] 2015-01-22 02:48:38,983 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(695)) - supergroup          = supergroup
     [exec] 2015-01-22 02:48:38,983 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(696)) - isPermissionEnabled = true
     [exec] 2015-01-22 02:48:38,984 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(707)) - HA Enabled: false
     [exec] 2015-01-22 02:48:38,985 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(744)) - Append Enabled: true
     [exec] 2015-01-22 02:48:39,961 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
     [exec] 2015-01-22 02:48:39,961 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:39,962 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 512 MB = 5.1 MB
     [exec] 2015-01-22 02:48:39,962 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^19 = 524288 entries
     [exec] 2015-01-22 02:48:40,017 INFO  namenode.NameNode (FSDirectory.java:<init>(204)) - Caching file names occuring more than 10 times
     [exec] 2015-01-22 02:48:40,023 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
     [exec] 2015-01-22 02:48:40,023 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:40,023 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 512 MB = 1.3 MB
     [exec] 2015-01-22 02:48:40,024 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^17 = 131072 entries
     [exec] 2015-01-22 02:48:40,025 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4712)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
     [exec] 2015-01-22 02:48:40,025 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4713)) - dfs.namenode.safemode.min.datanodes = 0
     [exec] 2015-01-22 02:48:40,025 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4714)) - dfs.namenode.safemode.extension     = 0
     [exec] 2015-01-22 02:48:40,026 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(828)) - Retry cache on namenode is enabled
     [exec] 2015-01-22 02:48:40,027 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(836)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
     [exec] 2015-01-22 02:48:40,029 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
     [exec] 2015-01-22 02:48:40,029 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:40,029 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 512 MB = 157.3 KB
     [exec] 2015-01-22 02:48:40,030 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^14 = 16384 entries
     [exec] 2015-01-22 02:48:40,038 INFO  namenode.AclConfigFlag (AclConfigFlag.java:<init>(40)) - ACLs enabled? false
     [exec] 2015-01-22 02:48:40,131 INFO  namenode.FSImage (FSImage.java:format(145)) - Allocated new BlockPoolId: BP-627007173-192.168.250.172-1421894920047
     [exec] 2015-01-22 02:48:40,206 INFO  common.Storage (NNStorage.java:format(550)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1 has been successfully formatted.
     [exec] 2015-01-22 02:48:40,243 INFO  common.Storage (NNStorage.java:format(550)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2 has been successfully formatted.
     [exec] 2015-01-22 02:48:40,464 INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(201)) - Going to retain 1 images with txid >= 0
     [exec] 2015-01-22 02:48:40,467 INFO  namenode.NameNode (NameNode.java:createNameNode(1298)) - createNameNode []
     [exec] 2015-01-22 02:48:40,512 WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(124)) - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
     [exec] 2015-01-22 02:48:40,596 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(344)) - Scheduled snapshot period at 10 second(s).
     [exec] 2015-01-22 02:48:40,596 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(183)) - NameNode metrics system started
     [exec] 2015-01-22 02:48:40,599 INFO  namenode.NameNode (NameNode.java:setClientNamenodeAddress(337)) - fs.defaultFS is hdfs://127.0.0.1:0
     [exec] 2015-01-22 02:48:40,628 INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1654)) - Starting web server as: ${dfs.web.authentication.kerberos.principal}
     [exec] 2015-01-22 02:48:40,628 INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1665)) - Starting Web-server for hdfs at: http://localhost:0
     [exec] 2015-01-22 02:48:40,671 INFO  mortbay.log (Slf4jLog.java:info(67)) - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
     [exec] 2015-01-22 02:48:40,675 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.namenode is not defined
     [exec] 2015-01-22 02:48:40,683 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(667)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
     [exec] 2015-01-22 02:48:40,684 INFO  http.HttpServer2 (HttpServer2.java:addFilter(645)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
     [exec] 2015-01-22 02:48:40,685 INFO  http.HttpServer2 (HttpServer2.java:addFilter(652)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
     [exec] 2015-01-22 02:48:40,712 INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(86)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
     [exec] 2015-01-22 02:48:40,713 INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(571)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
     [exec] 2015-01-22 02:48:40,732 INFO  http.HttpServer2 (HttpServer2.java:openListeners(855)) - Jetty bound to port 43278
     [exec] 2015-01-22 02:48:40,732 INFO  mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
     [exec] 2015-01-22 02:48:40,895 WARN  server.AuthenticationFilter (AuthenticationFilter.java:init(162)) - 'signature.secret' configuration not set, using a random value as secret
     [exec] 2015-01-22 02:48:40,922 INFO  mortbay.log (Slf4jLog.java:info(67)) - Started SelectChannelConnector@localhost:43278
     [exec] 2015-01-22 02:48:40,925 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(677)) - fsLock is fair:true
     [exec] 2015-01-22 02:48:40,926 INFO  namenode.HostFileManager (HostFileManager.java:refresh(304)) - read includes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:40,926 INFO  namenode.HostFileManager (HostFileManager.java:refresh(311)) - read excludes:
     [exec] HostSet(
     [exec] )
     [exec] 2015-01-22 02:48:40,926 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(234)) - dfs.block.invalidate.limit=1000
     [exec] 2015-01-22 02:48:40,927 INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(240)) - dfs.namenode.datanode.registration.ip-hostname-check=true
     [exec] 2015-01-22 02:48:40,927 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlocksMap
     [exec] 2015-01-22 02:48:40,927 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:40,927 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 2.0% max memory 512 MB = 10.2 MB
     [exec] 2015-01-22 02:48:40,927 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^20 = 1048576 entries
     [exec] 2015-01-22 02:48:40,949 INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(349)) - dfs.block.access.token.enable=false
     [exec] 2015-01-22 02:48:40,950 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(334)) - defaultReplication         = 1
     [exec] 2015-01-22 02:48:40,950 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(335)) - maxReplication             = 512
     [exec] 2015-01-22 02:48:40,950 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(336)) - minReplication             = 1
     [exec] 2015-01-22 02:48:40,951 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(337)) - maxReplicationStreams      = 2
     [exec] 2015-01-22 02:48:40,951 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(338)) - shouldCheckForEnoughRacks  = false
     [exec] 2015-01-22 02:48:40,951 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(339)) - replicationRecheckInterval = 3000
     [exec] 2015-01-22 02:48:40,951 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(340)) - encryptDataTransfer        = false
     [exec] 2015-01-22 02:48:40,951 INFO  blockmanagement.BlockManager (BlockManager.java:<init>(341)) - maxNumBlocksToLog          = 1000
     [exec] 2015-01-22 02:48:40,952 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(694)) - fsOwner             = root (auth:SIMPLE)
     [exec] 2015-01-22 02:48:40,952 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(695)) - supergroup          = supergroup
     [exec] 2015-01-22 02:48:40,952 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(696)) - isPermissionEnabled = true
     [exec] 2015-01-22 02:48:40,953 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(707)) - HA Enabled: false
     [exec] 2015-01-22 02:48:40,953 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(744)) - Append Enabled: true
     [exec] 2015-01-22 02:48:40,954 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map INodeMap
     [exec] 2015-01-22 02:48:40,954 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:40,954 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 1.0% max memory 512 MB = 5.1 MB
     [exec] 2015-01-22 02:48:40,955 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^19 = 524288 entries
     [exec] 2015-01-22 02:48:40,955 INFO  namenode.NameNode (FSDirectory.java:<init>(204)) - Caching file names occuring more than 10 times
     [exec] 2015-01-22 02:48:40,956 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map cachedBlocks
     [exec] 2015-01-22 02:48:40,956 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:40,956 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.25% max memory 512 MB = 1.3 MB
     [exec] 2015-01-22 02:48:40,956 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^17 = 131072 entries
     [exec] 2015-01-22 02:48:40,968 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4712)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
     [exec] 2015-01-22 02:48:40,968 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4713)) - dfs.namenode.safemode.min.datanodes = 0
     [exec] 2015-01-22 02:48:40,968 INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(4714)) - dfs.namenode.safemode.extension     = 0
     [exec] 2015-01-22 02:48:40,968 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(828)) - Retry cache on namenode is enabled
     [exec] 2015-01-22 02:48:40,969 INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(836)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
     [exec] 2015-01-22 02:48:40,969 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map NameNodeRetryCache
     [exec] 2015-01-22 02:48:40,969 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:40,969 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.029999999329447746% max memory 512 MB = 157.3 KB
     [exec] 2015-01-22 02:48:40,970 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^14 = 16384 entries
     [exec] 2015-01-22 02:48:40,972 INFO  namenode.AclConfigFlag (AclConfigFlag.java:<init>(40)) - ACLs enabled? false
     [exec] 2015-01-22 02:48:40,989 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/in_use.lock acquired by nodename 21396@sovmp172
     [exec] 2015-01-22 02:48:41,009 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/in_use.lock acquired by nodename 21396@sovmp172
     [exec] 2015-01-22 02:48:41,013 INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(322)) - Recovering unfinalized segments in /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current
     [exec] 2015-01-22 02:48:41,013 INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(322)) - Recovering unfinalized segments in /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/current
     [exec] 2015-01-22 02:48:41,014 INFO  namenode.FSImage (FSImage.java:loadFSImage(642)) - No edit log streams selected.
     [exec] 2015-01-22 02:48:41,030 INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(173)) - Loading 1 INodes.
     [exec] 2015-01-22 02:48:41,036 INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(170)) - Loaded FSImage in 0 seconds.
     [exec] 2015-01-22 02:48:41,036 INFO  namenode.FSImage (FSImage.java:loadFSImage(915)) - Loaded image for txid 0 from /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current/fsimage_0000000000000000000
     [exec] 2015-01-22 02:48:41,039 INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(897)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
     [exec] 2015-01-22 02:48:41,040 INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1112)) - Starting log segment at 1
     [exec] 2015-01-22 02:48:42,339 INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
     [exec] 2015-01-22 02:48:42,340 INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(645)) - Finished loading FSImage in 1368 msecs
     [exec] 2015-01-22 02:48:42,499 INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(295)) - RPC server is binding to localhost:0
     [exec] 2015-01-22 02:48:42,505 INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
     [exec] 2015-01-22 02:48:42,514 INFO  ipc.Server (Server.java:run(593)) - Starting Socket Reader #1 for port 56858
     [exec] 2015-01-22 02:48:42,565 INFO  namenode.NameNode (NameNode.java:initialize(567)) - Clients are to use localhost:56858 to access this namenode/service.
     [exec] 2015-01-22 02:48:42,569 INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5643)) - Registered FSNamesystemState MBean
     [exec] 2015-01-22 02:48:42,584 INFO  namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(5299)) - Number of blocks under construction: 0
     [exec] 2015-01-22 02:48:42,584 INFO  namenode.FSNamesystem (FSNamesystem.java:getCompleteBlocksTotal(5299)) - Number of blocks under construction: 0
     [exec] 2015-01-22 02:48:42,585 INFO  namenode.FSNamesystem (FSNamesystem.java:initializeReplQueues(1067)) - initializing replication queues
     [exec] 2015-01-22 02:48:42,586 INFO  hdfs.StateChange (FSNamesystem.java:leave(4786)) - STATE* Leaving safe mode after 1 secs
     [exec] 2015-01-22 02:48:42,586 INFO  hdfs.StateChange (FSNamesystem.java:leave(4797)) - STATE* Network topology has 0 racks and 0 datanodes
     [exec] 2015-01-22 02:48:42,586 INFO  hdfs.StateChange (FSNamesystem.java:leave(4800)) - STATE* UnderReplicatedBlocks has 0 blocks
     [exec] 2015-01-22 02:48:42,596 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2489)) - Total number of blocks            = 0
     [exec] 2015-01-22 02:48:42,596 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2490)) - Number of invalid blocks          = 0
     [exec] 2015-01-22 02:48:42,596 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2491)) - Number of under-replicated blocks = 0
     [exec] 2015-01-22 02:48:42,596 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2492)) - Number of  over-replicated blocks = 0
     [exec] 2015-01-22 02:48:42,597 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2494)) - Number of blocks being written    = 0
     [exec] 2015-01-22 02:48:42,597 INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(2495)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 11 msec
     [exec] 2015-01-22 02:48:42,637 INFO  ipc.Server (Server.java:run(815)) - IPC Server Responder: starting
     [exec] 2015-01-22 02:48:42,638 INFO  ipc.Server (Server.java:run(662)) - IPC Server listener on 56858: starting
     [exec] 2015-01-22 02:48:42,640 INFO  namenode.NameNode (NameNode.java:startCommonServices(609)) - NameNode RPC up at: localhost/127.0.0.1:56858
     [exec] 2015-01-22 02:48:42,640 INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(997)) - Starting services required for active state
     [exec] 2015-01-22 02:48:42,643 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(159)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
     [exec] 2015-01-22 02:48:42,643 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(172)) - Rescanning because of pending operations
     [exec] 2015-01-22 02:48:42,646 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(202)) - Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
     [exec] 2015-01-22 02:48:42,648 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1290)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1,[DISK]file:/opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2
     [exec] 2015-01-22 02:48:42,684 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(151)) - DataNode metrics system started (again)
     [exec] 2015-01-22 02:48:42,685 INFO  datanode.DataNode (DataNode.java:<init>(288)) - Configured hostname is 127.0.0.1
     [exec] 2015-01-22 02:48:42,688 INFO  datanode.DataNode (DataNode.java:startDataNode(764)) - Starting DataNode with maxLockedMemory = 0
     [exec] 2015-01-22 02:48:42,697 INFO  datanode.DataNode (DataNode.java:initDataXceiver(564)) - Opened streaming server at /127.0.0.1:42396
     [exec] 2015-01-22 02:48:42,699 INFO  datanode.DataNode (DataXceiverServer.java:<init>(73)) - Balancing bandwith is 1048576 bytes/s
     [exec] 2015-01-22 02:48:42,714 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.datanode is not defined
     [exec] 2015-01-22 02:48:42,715 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(667)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
     [exec] 2015-01-22 02:48:42,716 INFO  http.HttpServer2 (HttpServer2.java:addFilter(645)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
     [exec] 2015-01-22 02:48:42,716 INFO  http.HttpServer2 (HttpServer2.java:addFilter(652)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
     [exec] 2015-01-22 02:48:42,721 INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(571)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
     [exec] 2015-01-22 02:48:42,722 INFO  http.HttpServer2 (HttpServer2.java:openListeners(855)) - Jetty bound to port 38537
     [exec] 2015-01-22 02:48:42,722 INFO  mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26
     [exec] 2015-01-22 02:48:42,831 INFO  mortbay.log (Slf4jLog.java:info(67)) - Started SelectChannelConnector@localhost:38537
     [exec] 2015-01-22 02:48:42,832 INFO  datanode.DataNode (DataNode.java:startDataNode(781)) - dnUserName = root
     [exec] 2015-01-22 02:48:42,832 INFO  datanode.DataNode (DataNode.java:startDataNode(782)) - supergroup = supergroup
     [exec] 2015-01-22 02:48:42,838 INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(53)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue
     [exec] 2015-01-22 02:48:42,839 INFO  ipc.Server (Server.java:run(593)) - Starting Socket Reader #1 for port 52217
     [exec] 2015-01-22 02:48:42,845 INFO  datanode.DataNode (DataNode.java:initIpcServer(439)) - Opened IPC server at /127.0.0.1:52217
     [exec] 2015-01-22 02:48:42,855 INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(152)) - Refresh request received for nameservices: null
     [exec] 2015-01-22 02:48:42,857 INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(197)) - Starting BPOfferServices for nameservices: <default>
     [exec] 2015-01-22 02:48:42,862 INFO  datanode.DataNode (BPServiceActor.java:run(809)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:56858 starting to offer service
     [exec] 2015-01-22 02:48:42,863 INFO  ipc.Server (Server.java:run(815)) - IPC Server Responder: starting
     [exec] 2015-01-22 02:48:42,863 INFO  ipc.Server (Server.java:run(662)) - IPC Server listener on 52217: starting
     [exec] 2015-01-22 02:48:43,196 INFO  common.Storage (DataStorage.java:recoverTransitionRead(173)) - Data-node version: -55 and name-node layout version: -56
     [exec] 2015-01-22 02:48:43,227 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/in_use.lock acquired by nodename 21396@sovmp172
     [exec] 2015-01-22 02:48:43,228 INFO  common.Storage (DataStorage.java:recoverTransitionRead(197)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1 is not formatted
     [exec] 2015-01-22 02:48:43,228 INFO  common.Storage (DataStorage.java:recoverTransitionRead(198)) - Formatting ...
     [exec] 2015-01-22 02:48:43,237 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:43,237 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:43,263 INFO  common.Storage (Storage.java:tryLock(702)) - Lock on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/in_use.lock acquired by nodename 21396@sovmp172
     [exec] 2015-01-22 02:48:43,263 INFO  common.Storage (DataStorage.java:recoverTransitionRead(197)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2 is not formatted
     [exec] 2015-01-22 02:48:43,264 INFO  common.Storage (DataStorage.java:recoverTransitionRead(198)) - Formatting ...
     [exec] 2015-01-22 02:48:43,340 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:43,341 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:43,360 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(108)) - Analyzing storage directories for bpid BP-627007173-192.168.250.172-1421894920047
     [exec] 2015-01-22 02:48:43,361 INFO  common.Storage (Storage.java:lock(666)) - Locking is disabled
     [exec] 2015-01-22 02:48:43,362 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(130)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current/BP-627007173-192.168.250.172-1421894920047 is not formatted.
     [exec] 2015-01-22 02:48:43,362 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(131)) - Formatting ...
     [exec] 2015-01-22 02:48:43,362 INFO  common.Storage (BlockPoolSliceStorage.java:format(183)) - Formatting block pool BP-627007173-192.168.250.172-1421894920047 directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current/BP-627007173-192.168.250.172-1421894920047/current
     [exec] 2015-01-22 02:48:43,385 INFO  common.Storage (Storage.java:lock(666)) - Locking is disabled
     [exec] 2015-01-22 02:48:43,385 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(130)) - Storage directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current/BP-627007173-192.168.250.172-1421894920047 is not formatted.
     [exec] 2015-01-22 02:48:43,385 INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(131)) - Formatting ...
     [exec] 2015-01-22 02:48:43,385 INFO  common.Storage (BlockPoolSliceStorage.java:format(183)) - Formatting block pool BP-627007173-192.168.250.172-1421894920047 directory /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current/BP-627007173-192.168.250.172-1421894920047/current
     [exec] 2015-01-22 02:48:43,409 INFO  common.Storage (BlockPoolSliceStorage.java:doTransition(254)) - Restored 0 block files from trash.
     [exec] 2015-01-22 02:48:43,409 INFO  common.Storage (BlockPoolSliceStorage.java:doTransition(254)) - Restored 0 block files from trash.
     [exec] 2015-01-22 02:48:43,443 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:43,443 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:43,449 INFO  datanode.DataNode (DataNode.java:initStorage(976)) - Setting up storage: nsid=853575357;bpid=BP-627007173-192.168.250.172-1421894920047;lv=-55;nsInfo=lv=-56;cid=testClusterID;nsid=853575357;c=0;bpid=BP-627007173-192.168.250.172-1421894920047;dnuuid=null
     [exec] 2015-01-22 02:48:43,488 INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(809)) - Generated and persisted new Datanode UUID 34d21475-2772-4eec-9f86-6dfc36e17b26
     [exec] 2015-01-22 02:48:43,499 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:<init>(216)) - Added volume - /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current, StorageType: DISK
     [exec] 2015-01-22 02:48:43,499 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:<init>(216)) - Added volume - /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current, StorageType: DISK
     [exec] 2015-01-22 02:48:43,508 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(1378)) - Registered FSDatasetState MBean
     [exec] 2015-01-22 02:48:43,511 INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(330)) - Periodic Directory Tree Verification scan starting at 1421896355511 with interval 21600000
     [exec] 2015-01-22 02:48:43,512 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(1747)) - Adding block pool BP-627007173-192.168.250.172-1421894920047
     [exec] 2015-01-22 02:48:43,513 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(207)) - Scanning block pool BP-627007173-192.168.250.172-1421894920047 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current...
     [exec] 2015-01-22 02:48:43,513 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(207)) - Scanning block pool BP-627007173-192.168.250.172-1421894920047 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current...
     [exec] 2015-01-22 02:48:43,526 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(212)) - Time taken to scan block pool BP-627007173-192.168.250.172-1421894920047 on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current: 12ms
     [exec] 2015-01-22 02:48:43,536 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(212)) - Time taken to scan block pool BP-627007173-192.168.250.172-1421894920047 on /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current: 23ms
     [exec] 2015-01-22 02:48:43,537 INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(236)) - Total time to scan all replicas for block pool BP-627007173-192.168.250.172-1421894920047: 24ms
     [exec] 2015-01-22 02:48:43,537 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(108)) - Adding replicas to map for block pool BP-627007173-192.168.250.172-1421894920047 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current...
     [exec] 2015-01-22 02:48:43,537 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(108)) - Adding replicas to map for block pool BP-627007173-192.168.250.172-1421894920047 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current...
     [exec] 2015-01-22 02:48:43,538 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(113)) - Time to add replicas to map for block pool BP-627007173-192.168.250.172-1421894920047 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data1/current: 1ms
     [exec] 2015-01-22 02:48:43,538 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(113)) - Time to add replicas to map for block pool BP-627007173-192.168.250.172-1421894920047 on volume /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/data/data2/current: 0ms
     [exec] 2015-01-22 02:48:43,538 INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(136)) - Total time to add all replicas to map: 1ms
     [exec] 2015-01-22 02:48:43,540 INFO  datanode.DataNode (BPServiceActor.java:register(769)) - Block pool BP-627007173-192.168.250.172-1421894920047 (Datanode Uuid null) service to localhost/127.0.0.1:56858 beginning handshake with NN
     [exec] 2015-01-22 02:48:43,546 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2072)) - dnInfo.length != numDataNodes
     [exec] 2015-01-22 02:48:43,546 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2024)) - Waiting for cluster to become active
     [exec] 2015-01-22 02:48:43,549 INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(822)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=34d21475-2772-4eec-9f86-6dfc36e17b26, infoPort=38537, ipcPort=52217, storageInfo=lv=-55;cid=testClusterID;nsid=853575357;c=0) storage 34d21475-2772-4eec-9f86-6dfc36e17b26
     [exec] 2015-01-22 02:48:43,552 INFO  net.NetworkTopology (NetworkTopology.java:add(413)) - Adding a new node: /default-rack/127.0.0.1:42396
     [exec] 2015-01-22 02:48:43,555 INFO  datanode.DataNode (BPServiceActor.java:register(782)) - Block pool Block pool BP-627007173-192.168.250.172-1421894920047 (Datanode Uuid null) service to localhost/127.0.0.1:56858 successfully registered with NN
     [exec] 2015-01-22 02:48:43,555 INFO  datanode.DataNode (BPServiceActor.java:offerService(641)) - For namenode localhost/127.0.0.1:56858 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
     [exec] 2015-01-22 02:48:43,561 INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(667)) - Adding new storage ID DS-0006c147-e20d-43cf-8913-18143857b903 for DN 127.0.0.1:42396
     [exec] 2015-01-22 02:48:43,561 INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(667)) - Adding new storage ID DS-e332b9af-6d67-45bc-99d8-ee3333965bc9 for DN 127.0.0.1:42396
     [exec] 2015-01-22 02:48:43,566 INFO  datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(439)) - Namenode Block pool BP-627007173-192.168.250.172-1421894920047 (Datanode Uuid 34d21475-2772-4eec-9f86-6dfc36e17b26) service to localhost/127.0.0.1:56858 trying to claim ACTIVE state with txid=1
     [exec] 2015-01-22 02:48:43,567 INFO  datanode.DataNode (BPOfferService.java:updateActorStatesFromHeartbeat(451)) - Acknowledging ACTIVE Namenode Block pool BP-627007173-192.168.250.172-1421894920047 (Datanode Uuid 34d21475-2772-4eec-9f86-6dfc36e17b26) service to localhost/127.0.0.1:56858
     [exec] 2015-01-22 02:48:43,573 INFO  blockmanagement.BlockManager (BlockManager.java:processReport(1707)) - BLOCK* processReport: Received first block report from org.apache.hadoop.hdfs.server.protocol.DatanodeStorage@252f0446 after starting up or becoming active. Its block contents are no longer considered stale
     [exec] 2015-01-22 02:48:43,573 INFO  BlockStateChange (BlockManager.java:processReport(1723)) - BLOCK* processReport: from storage DS-e332b9af-6d67-45bc-99d8-ee3333965bc9 node DatanodeRegistration(127.0.0.1, datanodeUuid=34d21475-2772-4eec-9f86-6dfc36e17b26, infoPort=38537, ipcPort=52217, storageInfo=lv=-55;cid=testClusterID;nsid=853575357;c=0), blocks: 0, processing time: 2 msecs
     [exec] 2015-01-22 02:48:43,573 INFO  blockmanagement.BlockManager (BlockManager.java:processReport(1707)) - BLOCK* processReport: Received first block report from org.apache.hadoop.hdfs.server.protocol.DatanodeStorage@30f715be after starting up or becoming active. Its block contents are no longer considered stale
     [exec] 2015-01-22 02:48:43,573 INFO  BlockStateChange (BlockManager.java:processReport(1723)) - BLOCK* processReport: from storage DS-0006c147-e20d-43cf-8913-18143857b903 node DatanodeRegistration(127.0.0.1, datanodeUuid=34d21475-2772-4eec-9f86-6dfc36e17b26, infoPort=38537, ipcPort=52217, storageInfo=lv=-55;cid=testClusterID;nsid=853575357;c=0), blocks: 0, processing time: 0 msecs
     [exec] 2015-01-22 02:48:43,589 INFO  datanode.DataNode (BPServiceActor.java:blockReport(502)) - Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 22 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@ff1bca5f
     [exec] 2015-01-22 02:48:43,589 INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(618)) - Got finalize command for block pool BP-627007173-192.168.250.172-1421894920047
     [exec] 2015-01-22 02:48:43,591 INFO  util.GSet (LightWeightGSet.java:computeCapacity(354)) - Computing capacity for map BlockMap
     [exec] 2015-01-22 02:48:43,591 INFO  util.GSet (LightWeightGSet.java:computeCapacity(355)) - VM type       = 64-bit
     [exec] 2015-01-22 02:48:43,591 INFO  util.GSet (LightWeightGSet.java:computeCapacity(356)) - 0.5% max memory 512 MB = 2.6 MB
     [exec] 2015-01-22 02:48:43,591 INFO  util.GSet (LightWeightGSet.java:computeCapacity(361)) - capacity      = 2^18 = 262144 entries
     [exec] 2015-01-22 02:48:43,607 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:<init>(186)) - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-627007173-192.168.250.172-1421894920047
     [exec] 2015-01-22 02:48:43,617 INFO  datanode.DataBlockScanner (DataBlockScanner.java:addBlockPool(265)) - Added bpid=BP-627007173-192.168.250.172-1421894920047 to blockPoolScannerMap, new size=1
     [exec] testHdfsOperations(threadIdx=0): starting
     [exec] testHdfsOperations(threadIdx=1): starting
     [exec] testHdfsOperations(threadIdx=2): starting
     [exec] 2015-01-22 02:48:43,655 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2055)) - Cluster is active
     [exec] 2015-01-22 02:48:43,688 INFO  Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1009)) - dfs.block.size is deprecated. Instead, use dfs.blocksize
     [exec] 2015-01-22 02:48:43,695 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000	dst=null	perm=null
     [exec] 2015-01-22 02:48:43,732 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/tlhData0000	dst=null	perm=root:supergroup:rwxr-xr-x
     [exec] 2015-01-22 02:48:43,734 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001	dst=null	perm=null
     [exec] 2015-01-22 02:48:43,735 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002	dst=null	perm=null
     [exec] hdfsOpenFile(/tlhData0000/file1): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:
     [exec] hdfsOpenFile(/tlhData0001/file1): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:
     [exec] java.io.FileNotFoundException: File does not exist: /tlhData0001/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
     [exec] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
     [exec] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1144)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1122)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:264)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:231)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:224)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1295)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:300)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:296)
     [exec] 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:296)
     [exec] Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tlhData0001/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:620)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:219)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
     [exec] 	... 10 more
     [exec] java.io.FileNotFoundException: File does not exist: /tlhData0000/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
     [exec] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
     [exec] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1144)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1122)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:264)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:231)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:224)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1295)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:300)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:296)
     [exec] 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:296)
     [exec] Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tlhData0000/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:620)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:219)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
     [exec] 	... 10 more
     [exec] ERROR: cannot open an hdfs file in mode 0x3
     [exec] ERROR: cannot open an hdfs file in mode 0x3
     [exec] hdfsOpenFile(/tlhData0002/file1): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:
     [exec] java.io.FileNotFoundException: File does not exist: /tlhData0002/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     [exec] 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
     [exec] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
     [exec] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
     [exec] 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1144)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1122)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:264)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:231)
     [exec] 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:224)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1295)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:300)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:296)
     [exec] 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
     [exec] 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:296)
     [exec] Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tlhData0002/file1
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1728)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1671)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1651)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1625)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:620)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
     [exec] 	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:219)
     [exec] 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
     [exec] 	... 10 more
     [exec] ERROR: cannot open an hdfs file in mode 0x3
     [exec] 2015-01-22 02:48:43,748 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/tlhData0001	dst=null	perm=root:supergroup:rwxr-xr-x
     [exec] 2015-01-22 02:48:43,754 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 7 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:33599 Call#15 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0000/file1
     [exec] 2015-01-22 02:48:43,756 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/tlhData0002	dst=null	perm=root:supergroup:rwxr-xr-x
     [exec] 2015-01-22 02:48:43,756 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 8 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:33599 Call#16 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0001/file1
     [exec] 2015-01-22 02:48:43,759 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 9 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:33599 Call#17 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0002/file1
     [exec] 2015-01-22 02:48:43,784 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0000/file1	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:43,792 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0001/file1	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:43,800 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0002/file1	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:43,811 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0000/file1. BP-627007173-192.168.250.172-1421894920047 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-e332b9af-6d67-45bc-99d8-ee3333965bc9:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:43,813 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0001/file1. BP-627007173-192.168.250.172-1421894920047 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-0006c147-e20d-43cf-8913-18143857b903:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:43,829 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0002/file1. BP-627007173-192.168.250.172-1421894920047 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-e332b9af-6d67-45bc-99d8-ee3333965bc9:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:43,878 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-627007173-192.168.250.172-1421894920047:blk_1073741825_1001 src: /127.0.0.1:40599 dest: /127.0.0.1:42396
     [exec] 2015-01-22 02:48:43,878 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-627007173-192.168.250.172-1421894920047:blk_1073741826_1002 src: /127.0.0.1:40600 dest: /127.0.0.1:42396
     [exec] 2015-01-22 02:48:43,891 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-627007173-192.168.250.172-1421894920047:blk_1073741827_1003 src: /127.0.0.1:40601 dest: /127.0.0.1:42396
     [exec] 2015-01-22 02:48:44,014 INFO  hdfs.StateChange (FSNamesystem.java:fsync(3699)) - BLOCK* fsync: /tlhData0000/file1 for DFSClient_NONMAPREDUCE_1621625635_131
     [exec] 2015-01-22 02:48:44,023 INFO  hdfs.StateChange (FSNamesystem.java:fsync(3699)) - BLOCK* fsync: /tlhData0002/file1 for DFSClient_NONMAPREDUCE_-1981452634_133
     [exec] 2015-01-22 02:48:44,024 INFO  hdfs.StateChange (FSNamesystem.java:fsync(3699)) - BLOCK* fsync: /tlhData0001/file1 for DFSClient_NONMAPREDUCE_-2023713415_132
     [exec] 2015-01-22 02:48:44,029 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:40599, dest: /127.0.0.1:42396, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1621625635_131, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741825_1001, duration: 118101056
     [exec] 2015-01-22 02:48:44,029 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-627007173-192.168.250.172-1421894920047:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:44,030 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42396 is added to blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-0006c147-e20d-43cf-8913-18143857b903:NORMAL|RBW]]} size 12
     [exec] 2015-01-22 02:48:44,038 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:40601, dest: /127.0.0.1:42396, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1981452634_133, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741827_1003, duration: 129789563
     [exec] 2015-01-22 02:48:44,039 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-627007173-192.168.250.172-1421894920047:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:44,040 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42396 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-0006c147-e20d-43cf-8913-18143857b903:NORMAL|RBW]]} size 12
     [exec] 2015-01-22 02:48:44,046 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:40600, dest: /127.0.0.1:42396, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2023713415_132, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741826_1002, duration: 137780175
     [exec] 2015-01-22 02:48:44,046 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-627007173-192.168.250.172-1421894920047:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:44,047 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42396 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-e332b9af-6d67-45bc-99d8-ee3333965bc9:NORMAL|RBW]]} size 12
     [exec] 2015-01-22 02:48:44,050 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0000/file1 is closed by DFSClient_NONMAPREDUCE_1621625635_131
     [exec] 2015-01-22 02:48:44,059 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0002/file1 is closed by DFSClient_NONMAPREDUCE_-1981452634_133
     [exec] 2015-01-22 02:48:44,061 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,066 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0001/file1 is closed by DFSClient_NONMAPREDUCE_-2023713415_132
     [exec] 2015-01-22 02:48:44,067 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,069 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,087 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:42396, dest: /127.0.0.1:40602, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1981452634_133, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741827_1003, duration: 1568535
     [exec] 2015-01-22 02:48:44,087 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:42396, dest: /127.0.0.1:40603, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_1621625635_131, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741825_1001, duration: 1551904
     [exec] 2015-01-22 02:48:44,093 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,093 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,096 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:42396, dest: /127.0.0.1:40603, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-2023713415_132, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741826_1002, duration: 342271
     [exec] 2015-01-22 02:48:44,096 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,097 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,098 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,099 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,100 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,101 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,104 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,123 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0000/file2	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:44,138 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0002/file2	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:44,165 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:42396, dest: /127.0.0.1:40602, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_1621625635_131, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741825_1001, duration: 416113
     [exec] 2015-01-22 02:48:44,172 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0000/file2. BP-627007173-192.168.250.172-1421894920047 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-e332b9af-6d67-45bc-99d8-ee3333965bc9:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:44,182 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/tlhData0001/file2	dst=null	perm=root:supergroup:rw-r--r--
     [exec] 2015-01-22 02:48:44,189 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:42396, dest: /127.0.0.1:40603, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1981452634_133, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741827_1003, duration: 344012
     [exec] 2015-01-22 02:48:44,191 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0002/file2. BP-627007173-192.168.250.172-1421894920047 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-e332b9af-6d67-45bc-99d8-ee3333965bc9:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:44,205 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-627007173-192.168.250.172-1421894920047:blk_1073741828_1004 src: /127.0.0.1:40605 dest: /127.0.0.1:42396
     [exec] 2015-01-22 02:48:44,206 INFO  DataNode.clienttrace (BlockSender.java:sendBlock(738)) - src: /127.0.0.1:42396, dest: /127.0.0.1:40604, bytes: 16, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-2023713415_132, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741826_1002, duration: 692773
     [exec] 2015-01-22 02:48:44,208 INFO  hdfs.StateChange (FSNamesystem.java:saveAllocatedBlock(3070)) - BLOCK* allocateBlock: /tlhData0001/file2. BP-627007173-192.168.250.172-1421894920047 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-0006c147-e20d-43cf-8913-18143857b903:NORMAL|RBW]]}
     [exec] 2015-01-22 02:48:44,209 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:40605, dest: /127.0.0.1:42396, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1621625635_131, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741828_1004, duration: 1841020
     [exec] 2015-01-22 02:48:44,209 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-627007173-192.168.250.172-1421894920047:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:44,210 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42396 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-e332b9af-6d67-45bc-99d8-ee3333965bc9:NORMAL|RBW]]} size 0
     [exec] 2015-01-22 02:48:44,233 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-627007173-192.168.250.172-1421894920047:blk_1073741829_1005 src: /127.0.0.1:40607 dest: /127.0.0.1:42396
     [exec] 2015-01-22 02:48:44,237 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:40607, dest: /127.0.0.1:42396, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1981452634_133, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741829_1005, duration: 1845217
     [exec] 2015-01-22 02:48:44,238 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-627007173-192.168.250.172-1421894920047:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:44,238 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42396 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-0006c147-e20d-43cf-8913-18143857b903:NORMAL|RBW]]} size 0
     [exec] 2015-01-22 02:48:44,285 INFO  datanode.DataNode (DataXceiver.java:writeBlock(598)) - Receiving BP-627007173-192.168.250.172-1421894920047:blk_1073741830_1006 src: /127.0.0.1:40608 dest: /127.0.0.1:42396
     [exec] 2015-01-22 02:48:44,290 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1219)) - src: /127.0.0.1:40608, dest: /127.0.0.1:42396, bytes: 12, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2023713415_132, offset: 0, srvID: 34d21475-2772-4eec-9f86-6dfc36e17b26, blockid: BP-627007173-192.168.250.172-1421894920047:blk_1073741830_1006, duration: 2286025
     [exec] 2015-01-22 02:48:44,291 INFO  datanode.DataNode (BlockReceiver.java:run(1200)) - PacketResponder: BP-627007173-192.168.250.172-1421894920047:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
     [exec] 2015-01-22 02:48:44,291 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2350)) - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:42396 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-e332b9af-6d67-45bc-99d8-ee3333965bc9:NORMAL|RBW]]} size 0
     [exec] 2015-01-22 02:48:44,330 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0002/file2 is closed by DFSClient_NONMAPREDUCE_-1981452634_133
     [exec] 2015-01-22 02:48:44,330 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0000/file2 is closed by DFSClient_NONMAPREDUCE_1621625635_131
     [exec] 2015-01-22 02:48:44,402 INFO  hdfs.StateChange (FSNamesystem.java:completeFile(3003)) - DIR* completeFile: /tlhData0001/file2 is closed by DFSClient_NONMAPREDUCE_-2023713415_132
     [exec] 2015-01-22 02:48:44,418 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file2	dst=null	perm=root:doop:rw-r--r--
     [exec] 2015-01-22 02:48:44,418 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file2	dst=null	perm=root:doop:rw-r--r--
     [exec] 2015-01-22 02:48:44,426 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,426 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,429 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,430 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,438 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file2	dst=null	perm=root:doop:rw-r--r--
     [exec] 2015-01-22 02:48:44,440 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,443 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,493 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file2	dst=null	perm=ha:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:44,497 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,501 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,502 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file2	dst=null	perm=ha:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:44,502 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file2	dst=null	perm=ha:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:44,504 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,505 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,506 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,507 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,533 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file2	dst=null	perm=ha2:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:44,535 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,537 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0000/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,540 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 7 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:33599 Call#88 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0000/nonexistent-file-name
     [exec] 2015-01-22 02:48:44,545 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file2	dst=null	perm=ha2:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:44,545 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file2	dst=null	perm=ha2:doop2:rw-r--r--
     [exec] 2015-01-22 02:48:44,581 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,582 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,584 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0001/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,586 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tlhData0002/file2	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,588 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 8 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:33599 Call#94 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0001/nonexistent-file-name
     [exec] 2015-01-22 02:48:44,590 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 9 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:33599 Call#95 Retry#0: java.io.FileNotFoundException: File does not exist: /tlhData0002/nonexistent-file-name
     [exec] 2015-01-22 02:48:44,592 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(86)) - got exception trying to get groups for user foo
     [exec] org.apache.hadoop.util.Shell$ExitCodeException: id: foo: no such user
     [exec] 
     [exec] 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
     [exec] 	at org.apache.hadoop.util.Shell.run(Shell.java:418)
     [exec] 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
     [exec] 	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
     [exec] 	at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:81)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3324)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwnerInt(FSNamesystem.java:1592)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1581)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:563)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 2015-01-22 02:48:44,598 WARN  security.UserGroupInformation (UserGroupInformation.java:getGroupNames(1420)) - No groups available for user foo
     [exec] 2015-01-22 02:48:44,598 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=false	ugi=foo (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0000/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,598 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 2 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:33610 Call#91 Retry#0: org.apache.hadoop.security.AccessControlException: Permission denied
     [exec] 2015-01-22 02:48:44,676 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(86)) - got exception trying to get groups for user foo
     [exec] org.apache.hadoop.util.Shell$ExitCodeException: id: foo: no such user
     [exec] 
     [exec] 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
     [exec] 	at org.apache.hadoop.util.Shell.run(Shell.java:418)
     [exec] 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
     [exec] 	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
     [exec] 	at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:81)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3324)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwnerInt(FSNamesystem.java:1592)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1581)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:563)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 2015-01-22 02:48:44,677 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(86)) - got exception trying to get groups for user foo
     [exec] org.apache.hadoop.util.Shell$ExitCodeException: id: foo: no such user
     [exec] 
     [exec] 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
     [exec] 	at org.apache.hadoop.util.Shell.run(Shell.java:418)
     [exec] 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
     [exec] 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
     [exec] 	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
     [exec] 	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
     [exec] 	at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:81)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3324)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwnerInt(FSNamesystem.java:1592)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1581)
     [exec] 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:563)
     [exec] 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
     [exec] 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
     [exec] 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
     [exec] 	at java.security.AccessController.doPrivileged(AccessController.java:369)
     [exec] 	at javax.security.auth.Subject.doAs(Subject.java:572)
     [exec] 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
     [exec] 2015-01-22 02:48:44,679 WARN  security.UserGroupInformation (UserGroupInformation.java:getGroupNames(1420)) - No groups available for user foo
     [exec] 2015-01-22 02:48:44,681 WARN  security.UserGroupInformation (UserGroupInformation.java:getGroupNames(1420)) - No groups available for user foo
     [exec] 2015-01-22 02:48:44,684 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=false	ugi=foo (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0001/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,684 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=false	ugi=foo (auth:SIMPLE)	ip=/127.0.0.1	cmd=setOwner	src=/tlhData0002/file1	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,684 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 7 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:33611 Call#96 Retry#0: org.apache.hadoop.security.AccessControlException: Permission denied
     [exec] 2015-01-22 02:48:44,684 INFO  ipc.Server (Server.java:run(2027)) - IPC Server handler 4 on 56858, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from 127.0.0.1:33612 Call#97 Retry#0: org.apache.hadoop.security.AccessControlException: Permission denied
     [exec] 2015-01-22 02:48:44,706 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:42396 
     [exec] 2015-01-22 02:48:44,707 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741828_1004 127.0.0.1:42396 
     [exec] 2015-01-22 02:48:44,707 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tlhData0000	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,720 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741827_1003 127.0.0.1:42396 
     [exec] 2015-01-22 02:48:44,721 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:42396 
     [exec] 2015-01-22 02:48:44,721 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tlhData0002	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,728 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741826_1002 127.0.0.1:42396 
     [exec] 2015-01-22 02:48:44,729 INFO  BlockStateChange (BlockManager.java:addToInvalidates(1064)) - BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:42396 
     [exec] 2015-01-22 02:48:44,729 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7761)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tlhData0001	dst=null	perm=null
     [exec] 2015-01-22 02:48:44,760 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1546)) - Shutting down the Mini HDFS Cluster
     [exec] 2015-01-22 02:48:44,760 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNodes(1578)) - Shutting down DataNode 0
     [exec] 2015-01-22 02:48:44,760 WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(376)) - DirectoryScanner: shutdown has been called
     [exec] 2015-01-22 02:48:44,760 INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(240)) - Closing all peers.
     [exec] 2015-01-22 02:48:44,765 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped SelectChannelConnector@localhost:0
     [exec] 2015-01-22 02:48:44,766 INFO  datanode.DataNode (DataNode.java:shutdown(1303)) - Waiting for threadgroup to exit, active threads is 3
     [exec] 2015-01-22 02:48:44,767 INFO  ipc.Server (Server.java:stop(2398)) - Stopping server on 52217
     [exec] 2015-01-22 02:48:44,768 INFO  ipc.Server (Server.java:run(694)) - Stopping IPC Server listener on 52217
     [exec] 2015-01-22 02:48:44,768 INFO  ipc.Server (Server.java:run(820)) - Stopping IPC Server Responder
     [exec] 2015-01-22 02:48:44,768 WARN  datanode.DataNode (BPServiceActor.java:offerService(726)) - BPOfferService for Block pool BP-627007173-192.168.250.172-1421894920047 (Datanode Uuid 34d21475-2772-4eec-9f86-6dfc36e17b26) service to localhost/127.0.0.1:56858 interrupted
     [exec] 2015-01-22 02:48:44,769 WARN  datanode.DataNode (BPServiceActor.java:run(849)) - Ending block pool service for: Block pool BP-627007173-192.168.250.172-1421894920047 (Datanode Uuid 34d21475-2772-4eec-9f86-6dfc36e17b26) service to localhost/127.0.0.1:56858
     [exec] 2015-01-22 02:48:44,871 INFO  datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-627007173-192.168.250.172-1421894920047 (Datanode Uuid 34d21475-2772-4eec-9f86-6dfc36e17b26)
     [exec] 2015-01-22 02:48:44,871 INFO  datanode.DataBlockScanner (DataBlockScanner.java:removeBlockPool(274)) - Removed bpid=BP-627007173-192.168.250.172-1421894920047 from blockPoolScannerMap
     [exec] 2015-01-22 02:48:44,872 INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(1757)) - Removing block pool BP-627007173-192.168.250.172-1421894920047
     [exec] 2015-01-22 02:48:44,873 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(139)) - Shutting down all async disk service threads
     [exec] 2015-01-22 02:48:44,874 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(147)) - All async disk service threads have been shut down
     [exec] 2015-01-22 02:48:44,874 INFO  datanode.DataNode (DataNode.java:shutdown(1365)) - Shutdown complete.
     [exec] 2015-01-22 02:48:44,875 INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1095)) - Stopping services started for active state
     [exec] testLibHdfs: all threads succeeded.  SUCCESS.
     [exec] 2015-01-22 02:48:44,875 INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1153)) - Ending log segment 1
     [exec] 2015-01-22 02:48:44,875 INFO  namenode.FSNamesystem (FSNamesystem.java:run(4368)) - NameNodeEditLogRoller was interrupted, exiting
     [exec] 2015-01-22 02:48:44,888 INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(673)) - Number of transactions: 50 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 4 Number of syncs: 35 SyncTimes(ms): 346 390 
     [exec] 2015-01-22 02:48:44,890 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(130)) - Finalizing edits file /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current/edits_inprogress_0000000000000000001 -> /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name1/current/edits_0000000000000000001-0000000000000000050
     [exec] 2015-01-22 02:48:44,891 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(130)) - Finalizing edits file /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/current/edits_inprogress_0000000000000000001 -> /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/native/build/test/data/dfs/name2/current/edits_0000000000000000001-0000000000000000050
     [exec] 2015-01-22 02:48:44,898 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(168)) - Shutting down CacheReplicationMonitor
     [exec] 2015-01-22 02:48:44,899 INFO  ipc.Server (Server.java:stop(2398)) - Stopping server on 56858
     [exec] 2015-01-22 02:48:44,900 INFO  ipc.Server (Server.java:run(694)) - Stopping IPC Server listener on 56858
     [exec] 2015-01-22 02:48:44,900 INFO  blockmanagement.BlockManager (BlockManager.java:run(3363)) - Stopping ReplicationMonitor.
     [exec] 2015-01-22 02:48:44,900 INFO  ipc.Server (Server.java:run(820)) - Stopping IPC Server Responder
     [exec] 2015-01-22 02:48:44,901 WARN  blockmanagement.DecommissionManager (DecommissionManager.java:run(78)) - Monitor interrupted: java.lang.InterruptedException
     [exec] 2015-01-22 02:48:44,926 INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1095)) - Stopping services started for active state
     [exec] 2015-01-22 02:48:44,927 INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1177)) - Stopping services started for standby state
     [exec] 2015-01-22 02:48:44,931 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped SelectChannelConnector@localhost:0
     [exec] 2015-01-22 02:48:44,932 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(200)) - Stopping DataNode metrics system...
     [exec] 2015-01-22 02:48:44,933 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(206)) - DataNode metrics system stopped.
     [exec] 2015-01-22 02:48:44,934 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(572)) - DataNode metrics system shutdown complete.
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop HttpFS 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-hdfs-httpfs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-hdfs-httpfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-hdfs-httpfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-web-xmls) @ hadoop-hdfs-httpfs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-hdfs-httpfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-hdfs-httpfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-hdfs-httpfs ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.fs.http.server.TestHttpFSServer
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.916 sec - in org.apache.hadoop.fs.http.server.TestHttpFSServer
Running org.apache.hadoop.fs.http.server.TestCheckUploadContentTypeFilter
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.329 sec - in org.apache.hadoop.fs.http.server.TestCheckUploadContentTypeFilter
Running org.apache.hadoop.fs.http.server.TestHttpFSKerberosAuthenticationHandler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.108 sec - in org.apache.hadoop.fs.http.server.TestHttpFSKerberosAuthenticationHandler
Running org.apache.hadoop.fs.http.server.TestHttpFSCustomUserName
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.798 sec - in org.apache.hadoop.fs.http.server.TestHttpFSCustomUserName
Running org.apache.hadoop.fs.http.client.TestHttpFSFileSystemLocalFileSystem
Tests run: 32, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.609 sec - in org.apache.hadoop.fs.http.client.TestHttpFSFileSystemLocalFileSystem
Running org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem
Tests run: 32, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 27.377 sec - in org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem
Running org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem
Tests run: 32, Failures: 0, Errors: 32, Skipped: 0, Time elapsed: 0.634 sec <<< FAILURE! - in org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem
testOperation[0](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.006 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[0](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[1](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[1](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[2](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[2](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.003 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[3](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[3](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[4](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[4](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[5](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[5](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[6](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[6](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[7](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[7](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[8](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[8](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[9](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[9](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.004 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[10](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[10](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[11](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[11](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[12](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[12](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[13](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[13](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[14](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[14](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperation[15](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

testOperationDoAs[15](org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem)  Time elapsed: 0.001 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.fs.http.client.TestHttpFSFWithSWebhdfsFileSystem.<init>(TestHttpFSFWithSWebhdfsFileSystem.java:62)

Running org.apache.hadoop.fs.http.client.TestHttpFSWithHttpFSFileSystem
Tests run: 32, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.178 sec - in org.apache.hadoop.fs.http.client.TestHttpFSWithHttpFSFileSystem
Running org.apache.hadoop.lib.wsrs.TestJSONMapProvider
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.06 sec - in org.apache.hadoop.lib.wsrs.TestJSONMapProvider
Running org.apache.hadoop.lib.wsrs.TestParam
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.291 sec - in org.apache.hadoop.lib.wsrs.TestParam
Running org.apache.hadoop.lib.wsrs.TestJSONProvider
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.06 sec - in org.apache.hadoop.lib.wsrs.TestJSONProvider
Running org.apache.hadoop.lib.wsrs.TestUserProvider
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.337 sec - in org.apache.hadoop.lib.wsrs.TestUserProvider
Running org.apache.hadoop.lib.wsrs.TestInputStreamEntity
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.064 sec - in org.apache.hadoop.lib.wsrs.TestInputStreamEntity
Running org.apache.hadoop.lib.servlet.TestMDCFilter
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.407 sec - in org.apache.hadoop.lib.servlet.TestMDCFilter
Running org.apache.hadoop.lib.servlet.TestServerWebApp
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.334 sec - in org.apache.hadoop.lib.servlet.TestServerWebApp
Running org.apache.hadoop.lib.servlet.TestHostnameFilter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.435 sec - in org.apache.hadoop.lib.servlet.TestHostnameFilter
Running org.apache.hadoop.lib.server.TestBaseService
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.339 sec - in org.apache.hadoop.lib.server.TestBaseService
Running org.apache.hadoop.lib.server.TestServer
Tests run: 30, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.694 sec - in org.apache.hadoop.lib.server.TestServer
Running org.apache.hadoop.lib.server.TestServerConstructor
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.123 sec - in org.apache.hadoop.lib.server.TestServerConstructor
Running org.apache.hadoop.lib.lang.TestXException
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.07 sec - in org.apache.hadoop.lib.lang.TestXException
Running org.apache.hadoop.lib.lang.TestRunnableCallable
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.07 sec - in org.apache.hadoop.lib.lang.TestRunnableCallable
Running org.apache.hadoop.lib.service.instrumentation.TestInstrumentationService
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.892 sec - in org.apache.hadoop.lib.service.instrumentation.TestInstrumentationService
Running org.apache.hadoop.lib.service.hadoop.TestFileSystemAccessService
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.33 sec - in org.apache.hadoop.lib.service.hadoop.TestFileSystemAccessService
Running org.apache.hadoop.lib.service.scheduler.TestSchedulerService
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.362 sec - in org.apache.hadoop.lib.service.scheduler.TestSchedulerService
Running org.apache.hadoop.lib.service.security.TestGroupsService
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.432 sec - in org.apache.hadoop.lib.service.security.TestGroupsService
Running org.apache.hadoop.lib.service.security.TestProxyUserService
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.081 sec - in org.apache.hadoop.lib.service.security.TestProxyUserService
Running org.apache.hadoop.lib.service.security.TestDelegationTokenManagerService
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.749 sec - in org.apache.hadoop.lib.service.security.TestDelegationTokenManagerService
Running org.apache.hadoop.lib.util.TestCheck
Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.109 sec - in org.apache.hadoop.lib.util.TestCheck
Running org.apache.hadoop.lib.util.TestConfigurationUtils
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.278 sec - in org.apache.hadoop.lib.util.TestConfigurationUtils
Running org.apache.hadoop.test.TestExceptionHelper
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.06 sec - in org.apache.hadoop.test.TestExceptionHelper
Running org.apache.hadoop.test.TestHTestCase
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.34 sec - in org.apache.hadoop.test.TestHTestCase
Running org.apache.hadoop.test.TestDirHelper
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.065 sec - in org.apache.hadoop.test.TestDirHelper
Running org.apache.hadoop.test.TestHdfsHelper
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.069 sec - in org.apache.hadoop.test.TestHdfsHelper
Running org.apache.hadoop.test.TestHFSTestCase
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.468 sec - in org.apache.hadoop.test.TestHFSTestCase

Results :

Tests in error: 
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....
  TestHttpFSFWithSWebhdfsFileSystem.<init>:62 NoClassDefFound sun.security.x509....

Tests run: 318, Failures: 0, Errors: 32, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop HDFS BookKeeper Journal 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-hdfs-bkjournal ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-hdfs-bkjournal ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-hdfs-bkjournal ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-hdfs-bkjournal ---
[INFO] Compiling 1 source file to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-hdfs-bkjournal ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-hdfs-bkjournal ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-hdfs-bkjournal ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.contrib.bkjournal.TestCurrentInprogress
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.844 sec - in org.apache.hadoop.contrib.bkjournal.TestCurrentInprogress
Running org.apache.hadoop.contrib.bkjournal.TestBookKeeperJournalManager
Tests run: 16, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 23.638 sec <<< FAILURE! - in org.apache.hadoop.contrib.bkjournal.TestBookKeeperJournalManager
testEmptyInprogressNode(org.apache.hadoop.contrib.bkjournal.TestBookKeeperJournalManager)  Time elapsed: 0.045 sec  <<< ERROR!
java.io.IOException: Invalid/Incomplete data in znode
	at org.apache.hadoop.contrib.bkjournal.EditLogLedgerMetadata.read(EditLogLedgerMetadata.java:126)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.getLedgerList(BookKeeperJournalManager.java:795)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.getLedgerList(BookKeeperJournalManager.java:780)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.format(BookKeeperJournalManager.java:266)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperJournalManager.testEmptyInprogressNode(TestBookKeeperJournalManager.java:540)

testCorruptInprogressNode(org.apache.hadoop.contrib.bkjournal.TestBookKeeperJournalManager)  Time elapsed: 0.035 sec  <<< ERROR!
com.google.protobuf.TextFormat$ParseException: 1:1: Message type "hadoop.hdfs.EditLogLedgerProto" has no field named "WholeLottaJunk".
	at com.google.protobuf.TextFormat$Tokenizer.parseExceptionPreviousToken(TextFormat.java:923)
	at com.google.protobuf.TextFormat.mergeField(TextFormat.java:1125)
	at com.google.protobuf.TextFormat.merge(TextFormat.java:1062)
	at com.google.protobuf.TextFormat.merge(TextFormat.java:1008)
	at org.apache.hadoop.contrib.bkjournal.EditLogLedgerMetadata.read(EditLogLedgerMetadata.java:124)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.getLedgerList(BookKeeperJournalManager.java:795)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.getLedgerList(BookKeeperJournalManager.java:780)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.format(BookKeeperJournalManager.java:266)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperJournalManager.testCorruptInprogressNode(TestBookKeeperJournalManager.java:584)

Running org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints
Tests run: 8, Failures: 1, Errors: 7, Skipped: 0, Time elapsed: 8.241 sec <<< FAILURE! - in org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints
testSBNCheckpoints(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 1.861 sec  <<< ERROR!
java.io.IOException: Error cleaning up ledgers during format
	at org.apache.bookkeeper.client.BKException.create(BKException.java:64)
	at org.apache.bookkeeper.client.BookKeeper.deleteLedger(BookKeeper.java:467)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.format(BookKeeperJournalManager.java:268)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:371)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:149)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:899)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:282)
	at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:142)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:858)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:64)

testBothNodesInStandbyState(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 4.173 sec  <<< ERROR!
org.apache.hadoop.util.ExitUtil$ExitException: org.apache.hadoop.util.ExitUtil$ExitException: Error: flush failed for required journal (JournalAndStream(mgr=org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager@e11bbd3a, stream=org.apache.hadoop.contrib.bkjournal.BookKeeperEditLogOutputStream@d8d8afd0))
	at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:126)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:374)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.access$100(JournalSet.java:57)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream.flush(JournalSet.java:495)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:623)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1144)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:303)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1034)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1575)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
	at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1453)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1160)
	at org.apache.hadoop.hdfs.MiniDFSCluster.transitionToActive(MiniDFSCluster.java:1977)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

	at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:126)
	at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:170)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.doImmediateShutdown(NameNode.java:1555)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1578)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
	at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1453)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1160)
	at org.apache.hadoop.hdfs.MiniDFSCluster.transitionToActive(MiniDFSCluster.java:1977)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:75)

testBothNodesInStandbyState(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 4.178 sec  <<< FAILURE!
java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1552)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1539)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints.shutdownCluster(TestStandbyCheckpoints.java:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

testCheckpointWhenNoNewTransactionsHappened(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 0.351 sec  <<< ERROR!
java.net.BindException: Port in use: localhost:10001
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:132)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:557)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:64)

testCheckpointCancellation(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 0.343 sec  <<< ERROR!
java.net.BindException: Port in use: localhost:10001
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:132)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:557)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:64)

testCheckpointCancellationDuringUpload(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 0.352 sec  <<< ERROR!
java.net.BindException: Port in use: localhost:10001
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:132)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:557)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:64)

testStandbyExceptionThrownDuringCheckpoint(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 0.378 sec  <<< ERROR!
java.net.BindException: Port in use: localhost:10001
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:132)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:557)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:64)

testReadsAllowedDuringCheckpoint(org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints)  Time elapsed: 0.384 sec  <<< ERROR!
java.net.BindException: Port in use: localhost:10001
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:132)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:666)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:557)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:724)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:708)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1358)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:996)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints.setupCluster(TestBookKeeperHACheckpoints.java:64)

Running org.apache.hadoop.contrib.bkjournal.TestBookKeeperEditLogStreams
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.857 sec - in org.apache.hadoop.contrib.bkjournal.TestBookKeeperEditLogStreams
Running org.apache.hadoop.contrib.bkjournal.TestBookKeeperConfiguration
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.611 sec - in org.apache.hadoop.contrib.bkjournal.TestBookKeeperConfiguration
Running org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir
Tests run: 5, Failures: 1, Errors: 4, Skipped: 0, Time elapsed: 8.31 sec <<< FAILURE! - in org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir
testFailoverWithBK(org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir)  Time elapsed: 3.426 sec  <<< ERROR!
java.io.IOException: Error cleaning up ledgers during format
	at org.apache.bookkeeper.client.BKException.create(BKException.java:64)
	at org.apache.bookkeeper.client.BookKeeper.deleteLedger(BookKeeper.java:467)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.format(BookKeeperJournalManager.java:268)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:371)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:149)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:899)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:282)
	at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:142)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:858)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir.testFailoverWithBK(TestBookKeeperAsHASharedDir.java:101)

testFailoverWithFailingBKCluster(org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir)  Time elapsed: 0.333 sec  <<< ERROR!
java.io.IOException: Error cleaning up ledgers during format
	at org.apache.bookkeeper.client.BKException.create(BKException.java:64)
	at org.apache.bookkeeper.client.BookKeeper.deleteLedger(BookKeeper.java:467)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.format(BookKeeperJournalManager.java:268)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:371)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:149)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:899)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:282)
	at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:142)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:858)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir.testFailoverWithFailingBKCluster(TestBookKeeperAsHASharedDir.java:157)

testMultiplePrimariesStarted(org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir)  Time elapsed: 0.192 sec  <<< ERROR!
java.io.IOException: Error cleaning up ledgers during format
	at org.apache.bookkeeper.client.BKException.create(BKException.java:64)
	at org.apache.bookkeeper.client.BookKeeper.deleteLedger(BookKeeper.java:467)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.format(BookKeeperJournalManager.java:268)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:371)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:149)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:899)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:282)
	at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:142)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:858)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir.testMultiplePrimariesStarted(TestBookKeeperAsHASharedDir.java:230)

testInitializeBKSharedEdits(org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir)  Time elapsed: 2.815 sec  <<< FAILURE!
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertFalse(Assert.java:68)
	at org.junit.Assert.assertFalse(Assert.java:79)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir.testInitializeBKSharedEdits(TestBookKeeperAsHASharedDir.java:299)

testNameNodeMultipleSwitchesUsingBKJM(org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir)  Time elapsed: 0.181 sec  <<< ERROR!
java.io.IOException: Error cleaning up ledgers during format
	at org.apache.bookkeeper.client.BKException.create(BKException.java:64)
	at org.apache.bookkeeper.client.BookKeeper.deleteLedger(BookKeeper.java:467)
	at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.format(BookKeeperJournalManager.java:268)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:371)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:149)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:899)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:282)
	at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:142)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:858)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:702)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:374)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:355)
	at org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir.testNameNodeMultipleSwitchesUsingBKJM(TestBookKeeperAsHASharedDir.java:367)


Results :

Failed tests: 
  TestBookKeeperHACheckpoints>TestStandbyCheckpoints.shutdownCluster:115 Test resulted in an unexpected exit
  TestBookKeeperAsHASharedDir.testInitializeBKSharedEdits:299 null

Tests in error: 
  TestBookKeeperJournalManager.testEmptyInprogressNode:540 » IO Invalid/Incomple...
  TestBookKeeperJournalManager.testCorruptInprogressNode:584 » Parse 1:1: Messag...
  TestBookKeeperHACheckpoints.setupCluster:64 » IO Error cleaning up ledgers dur...
  TestBookKeeperHACheckpoints.setupCluster:75 » Exit org.apache.hadoop.util.Exit...
  TestBookKeeperHACheckpoints.setupCluster:64 » Bind Port in use: localhost:1000...
  TestBookKeeperHACheckpoints.setupCluster:64 » Bind Port in use: localhost:1000...
  TestBookKeeperHACheckpoints.setupCluster:64 » Bind Port in use: localhost:1000...
  TestBookKeeperHACheckpoints.setupCluster:64 » Bind Port in use: localhost:1000...
  TestBookKeeperHACheckpoints.setupCluster:64 » Bind Port in use: localhost:1000...
  TestBookKeeperAsHASharedDir.testFailoverWithBK:101 » IO Error cleaning up ledg...
  TestBookKeeperAsHASharedDir.testFailoverWithFailingBKCluster:157 » IO Error cl...
  TestBookKeeperAsHASharedDir.testMultiplePrimariesStarted:230 » IO Error cleani...
  TestBookKeeperAsHASharedDir.testNameNodeMultipleSwitchesUsingBKJM:367 » IO Err...

Tests run: 35, Failures: 2, Errors: 13, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop HDFS-NFS 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-hdfs-nfs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-hdfs-nfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-hdfs-nfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-hdfs-nfs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-hdfs-nfs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-hdfs-nfs ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.hdfs.nfs.TestReaddir
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.819 sec - in org.apache.hadoop.hdfs.nfs.TestReaddir
Running org.apache.hadoop.hdfs.nfs.TestMountd
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.122 sec - in org.apache.hadoop.hdfs.nfs.TestMountd
Running org.apache.hadoop.hdfs.nfs.nfs3.TestRpcProgramNfs3
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.094 sec - in org.apache.hadoop.hdfs.nfs.nfs3.TestRpcProgramNfs3
Running org.apache.hadoop.hdfs.nfs.nfs3.TestOffsetRange
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.096 sec - in org.apache.hadoop.hdfs.nfs.nfs3.TestOffsetRange
Running org.apache.hadoop.hdfs.nfs.nfs3.TestDFSClientCache
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.479 sec - in org.apache.hadoop.hdfs.nfs.nfs3.TestDFSClientCache
Running org.apache.hadoop.hdfs.nfs.nfs3.TestOpenFileCtxCache
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.126 sec - in org.apache.hadoop.hdfs.nfs.nfs3.TestOpenFileCtxCache
Running org.apache.hadoop.hdfs.nfs.nfs3.TestWrites
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.084 sec - in org.apache.hadoop.hdfs.nfs.nfs3.TestWrites
Running org.apache.hadoop.hdfs.nfs.nfs3.TestExportsTable
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.433 sec - in org.apache.hadoop.hdfs.nfs.nfs3.TestExportsTable

Results :

Tests run: 19, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop HDFS Project 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-hdfs-project ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-api 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-api ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-yarn-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-api ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-api ---
[INFO] Compiling 9 source files to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-api ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-api ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-api ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-common 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:version-info (version-info) @ hadoop-yarn-common ---
[WARNING] [svn, info] failed with error code 1
[INFO] SCM: GIT
[INFO] Computed MD5: f74113c8e511baadac8fbc14827d8b85
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-common ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-common ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-jar-plugin:2.3.1:test-jar (default) @ hadoop-yarn-common ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-common ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.api.TestContainerResourceIncreaseRequest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.414 sec - in org.apache.hadoop.yarn.api.TestContainerResourceIncreaseRequest
Running org.apache.hadoop.yarn.api.records.timeline.TestTimelineRecords
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.594 sec - in org.apache.hadoop.yarn.api.records.timeline.TestTimelineRecords
Running org.apache.hadoop.yarn.api.TestApplicationId
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.398 sec - in org.apache.hadoop.yarn.api.TestApplicationId
Running org.apache.hadoop.yarn.api.TestContainerResourceDecrease
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.498 sec - in org.apache.hadoop.yarn.api.TestContainerResourceDecrease
Running org.apache.hadoop.yarn.api.TestAllocateResponse
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.43 sec - in org.apache.hadoop.yarn.api.TestAllocateResponse
Running org.apache.hadoop.yarn.api.TestAllocateRequest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.439 sec - in org.apache.hadoop.yarn.api.TestAllocateRequest
Running org.apache.hadoop.yarn.api.TestApplicationAttemptId
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.376 sec - in org.apache.hadoop.yarn.api.TestApplicationAttemptId
Running org.apache.hadoop.yarn.api.TestApplicatonReport
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.415 sec - in org.apache.hadoop.yarn.api.TestApplicatonReport
Running org.apache.hadoop.yarn.api.TestContainerId
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.412 sec - in org.apache.hadoop.yarn.api.TestContainerId
Running org.apache.hadoop.yarn.api.TestGetApplicationsRequest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.432 sec - in org.apache.hadoop.yarn.api.TestGetApplicationsRequest
Running org.apache.hadoop.yarn.api.TestContainerResourceIncrease
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.385 sec - in org.apache.hadoop.yarn.api.TestContainerResourceIncrease
Running org.apache.hadoop.yarn.api.TestNodeId
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.376 sec - in org.apache.hadoop.yarn.api.TestNodeId
Running org.apache.hadoop.yarn.TestRPC
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.43 sec - in org.apache.hadoop.yarn.TestRPC
Running org.apache.hadoop.yarn.conf.TestYarnConfiguration
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.796 sec - in org.apache.hadoop.yarn.conf.TestYarnConfiguration
Running org.apache.hadoop.yarn.conf.TestHAUtil
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.725 sec - in org.apache.hadoop.yarn.conf.TestHAUtil
Running org.apache.hadoop.yarn.TestContainerLaunchRPC
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.141 sec - in org.apache.hadoop.yarn.TestContainerLaunchRPC
Running org.apache.hadoop.yarn.client.TestClientRMProxy
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.675 sec - in org.apache.hadoop.yarn.client.TestClientRMProxy
Running org.apache.hadoop.yarn.TestRecordFactory
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.293 sec - in org.apache.hadoop.yarn.TestRecordFactory
Running org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.278 sec - in org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock
Running org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 1.22 sec - in org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat
Running org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.115 sec - in org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService
Running org.apache.hadoop.yarn.TestRpcFactoryProvider
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.488 sec - in org.apache.hadoop.yarn.TestRpcFactoryProvider
Running org.apache.hadoop.yarn.ipc.TestRPCUtil
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.109 sec - in org.apache.hadoop.yarn.ipc.TestRPCUtil
Running org.apache.hadoop.yarn.webapp.TestParseRoute
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.229 sec - in org.apache.hadoop.yarn.webapp.TestParseRoute
Running org.apache.hadoop.yarn.webapp.hamlet.TestHamletImpl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.636 sec - in org.apache.hadoop.yarn.webapp.hamlet.TestHamletImpl
Running org.apache.hadoop.yarn.webapp.hamlet.TestHamlet
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.653 sec - in org.apache.hadoop.yarn.webapp.hamlet.TestHamlet
Running org.apache.hadoop.yarn.webapp.hamlet.TestParseSelector
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.137 sec - in org.apache.hadoop.yarn.webapp.hamlet.TestParseSelector
Running org.apache.hadoop.yarn.webapp.TestWebApp
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.883 sec - in org.apache.hadoop.yarn.webapp.TestWebApp
Running org.apache.hadoop.yarn.webapp.view.TestTwoColumnLayout
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.575 sec - in org.apache.hadoop.yarn.webapp.view.TestTwoColumnLayout
Running org.apache.hadoop.yarn.webapp.view.TestHtmlBlock
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.9 sec - in org.apache.hadoop.yarn.webapp.view.TestHtmlBlock
Running org.apache.hadoop.yarn.webapp.view.TestInfoBlock
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.822 sec - in org.apache.hadoop.yarn.webapp.view.TestInfoBlock
Running org.apache.hadoop.yarn.webapp.view.TestTwoColumnCssPage
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.513 sec - in org.apache.hadoop.yarn.webapp.view.TestTwoColumnCssPage
Running org.apache.hadoop.yarn.webapp.view.TestHtmlPage
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.89 sec - in org.apache.hadoop.yarn.webapp.view.TestHtmlPage
Running org.apache.hadoop.yarn.webapp.view.TestCommonViews
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.191 sec - in org.apache.hadoop.yarn.webapp.view.TestCommonViews
Running org.apache.hadoop.yarn.webapp.TestSubViews
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.957 sec - in org.apache.hadoop.yarn.webapp.TestSubViews
Running org.apache.hadoop.yarn.webapp.test.TestWebAppTests
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.711 sec - in org.apache.hadoop.yarn.webapp.test.TestWebAppTests
Running org.apache.hadoop.yarn.util.TestWindowsResourceCalculatorPlugin
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.728 sec - in org.apache.hadoop.yarn.util.TestWindowsResourceCalculatorPlugin
Running org.apache.hadoop.yarn.util.TestConverterUtils
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.491 sec - in org.apache.hadoop.yarn.util.TestConverterUtils
Running org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.15 sec - in org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree
Running org.apache.hadoop.yarn.util.TestRackResolverScriptBasedMapping
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.41 sec - in org.apache.hadoop.yarn.util.TestRackResolverScriptBasedMapping
Running org.apache.hadoop.yarn.util.TestFSDownload
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.385 sec - in org.apache.hadoop.yarn.util.TestFSDownload
Running org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.211 sec - in org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin
Running org.apache.hadoop.yarn.util.TestApplicationClassLoader
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.235 sec - in org.apache.hadoop.yarn.util.TestApplicationClassLoader
Running org.apache.hadoop.yarn.util.TestRackResolver
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.512 sec - in org.apache.hadoop.yarn.util.TestRackResolver
Running org.apache.hadoop.yarn.util.TestResourceCalculatorProcessTree
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.215 sec - in org.apache.hadoop.yarn.util.TestResourceCalculatorProcessTree
Running org.apache.hadoop.yarn.util.TestYarnVersionInfo
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.202 sec - in org.apache.hadoop.yarn.util.TestYarnVersionInfo
Running org.apache.hadoop.yarn.util.TestWindowsBasedProcessTree
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.236 sec - in org.apache.hadoop.yarn.util.TestWindowsBasedProcessTree
Running org.apache.hadoop.yarn.TestYarnUncaughtExceptionHandler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.493 sec - in org.apache.hadoop.yarn.TestYarnUncaughtExceptionHandler
Exception in thread "Thread-8" Exception in thread "Thread-9" 
Running org.apache.hadoop.yarn.TestRPCFactories
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.028 sec - in org.apache.hadoop.yarn.TestRPCFactories

Results :

Tests run: 142, Failures: 0, Errors: 0, Skipped: 1

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-server 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-server ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-server-common 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-server-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-yarn-server-common ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-server-common ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-server-common ---
[INFO] Compiling 3 source files to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-server-common ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-server-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-server-common ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.TestYSCRPCFactories
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.02 sec - in org.apache.hadoop.yarn.TestYSCRPCFactories
Running org.apache.hadoop.yarn.TestYarnServerApiClasses
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.471 sec - in org.apache.hadoop.yarn.TestYarnServerApiClasses
Running org.apache.hadoop.yarn.server.api.protocolrecords.TestRegisterNodeManagerResponse
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.35 sec - in org.apache.hadoop.yarn.server.api.protocolrecords.TestRegisterNodeManagerResponse
Running org.apache.hadoop.yarn.TestYSCRecordFactory
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.237 sec - in org.apache.hadoop.yarn.TestYSCRecordFactory
Running org.apache.hadoop.yarn.TestResourceTrackerPBClientImpl
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.281 sec - in org.apache.hadoop.yarn.TestResourceTrackerPBClientImpl
Running org.apache.hadoop.yarn.lib.TestZKClient
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.945 sec - in org.apache.hadoop.yarn.lib.TestZKClient

Results :

Tests run: 13, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-server-nodemanager 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-server-nodemanager ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-yarn-server-nodemanager ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-server-nodemanager ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-server-nodemanager ---
[INFO] Compiling 4 source files to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/classes
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (make) @ hadoop-yarn-server-nodemanager ---
[INFO] Executing tasks

main:
     [exec] -- Configuring done
     [exec] -- Generating done
     [exec] -- Build files have been written to: /opt/develop/hadoop-common/hadoop-yarn-project/hadooJAVA_HOME=, JAVA_JVM_LIBRARY=/opt/ibm/java-ppc64le-71/jre/lib/ppc64le/default/libjvm.so
     [exec] JAVA_INCLUDE_PATH=/opt/ibm/java-ppc64le-p-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native
     [exec] 71/include, JAVA_INCLUDE_PATH2=/opt/ibm/java-ppc64le-71/include/linux
     [exec] Located all JNI components successfully.
     [exec] /usr/bin/cmake -H/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src -B/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native --check-build-system CMakeFiles/Makefile.cmake 0
     [exec] /usr/bin/cmake -E cmake_progress_start /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles/progress.marks
     [exec] make -f CMakeFiles/Makefile2 all
     [exec] make[1]: Entering directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] make -f CMakeFiles/container.dir/build.make CMakeFiles/container.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles/container.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] make -f CMakeFiles/container.dir/build.make CMakeFiles/container.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/container.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles  1 2
     [exec] [ 50%] Built target container
     [exec] make -f CMakeFiles/container-executor.dir/build.make CMakeFiles/container-executor.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles/container-executor.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] make -f CMakeFiles/container-executor.dir/build.make CMakeFiles/container-executor.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/container-executor.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles  3
     [exec] [ 75%] Built target container-executor
     [exec] make -f CMakeFiles/test-container-executor.dir/build.make CMakeFiles/test-container-executor.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles/test-container-executor.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] make -f CMakeFiles/test-container-executor.dir/build.make CMakeFiles/test-container-executor.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/test-container-executor.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles  4
     [exec] [100%] Built target test-container-executor
     [exec] make[1]: Leaving directory `/opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_start /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/native/CMakeFiles 0
     [exec] [ 50%] Built target container
     [exec] [ 75%] Built target container-executor
     [exec] [100%] Built target test-container-executor
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-server-nodemanager ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-server-nodemanager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-server-nodemanager ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor
Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.799 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor
testContainerKillOnMemoryOverflow(org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor)  Time elapsed: 0.24 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow(TestContainersMonitor.java:190)

Running org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService
Tests run: 11, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.863 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService
testLogAggregationForRealContainerLaunch(org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService)  Time elapsed: 0.279 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testLogAggregationForRealContainerLaunch(TestLogAggregationService.java:773)

Running org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.TestNonAggregatingLogHandler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.181 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.TestNonAggregatingLogHandler
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager
Tests run: 10, Failures: 0, Errors: 10, Skipped: 0, Time elapsed: 4.172 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager
testContainerManagerInitialization(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.341 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerManagerInitialization(TestContainerManager.java:147)

testContainerSetup(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.189 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerSetup(TestContainerManager.java:179)

testContainerLaunchAndStop(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.118 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchAndStop(TestContainerManager.java:259)

testContainerLaunchAndExitSuccess(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.2 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchAndExitSuccess(TestContainerManager.java:439)

testContainerLaunchAndExitFailure(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.128 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchAndExitFailure(TestContainerManager.java:450)

testLocalFilesCleanup(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.231 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalFilesCleanup(TestContainerManager.java:467)

testContainerLaunchFromPreviousRM(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.176 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchFromPreviousRM(TestContainerManager.java:574)

testMultipleContainersLaunch(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.131 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testMultipleContainersLaunch(TestContainerManager.java:638)

testMultipleContainersStopAndGetStatus(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.286 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testMultipleContainersStopAndGetStatus(TestContainerManager.java:683)

testStartContainerFailureWithUnknownAuxService(org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager)  Time elapsed: 0.183 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testStartContainerFailureWithUnknownAuxService(TestContainerManager.java:760)

Running org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.267 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.737 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch
Tests run: 10, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 3.912 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch
testContainerEnvVariables(org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch)  Time elapsed: 0.302 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testContainerEnvVariables(TestContainerLaunch.java:370)

testDelayedKill(org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch)  Time elapsed: 0.185 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.internalKillTest(TestContainerLaunch.java:624)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testDelayedKill(TestContainerLaunch.java:748)

testImmediateKill(org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch)  Time elapsed: 0.122 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.internalKillTest(TestContainerLaunch.java:624)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testImmediateKill(TestContainerLaunch.java:753)

Running org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalizedResource
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.643 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalizedResource
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.545 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.232 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalCacheDirectoryManager
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.081 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalCacheDirectoryManager
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceRetention
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.876 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceRetention
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.052 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResource
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.389 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResource
Running org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.319 sec - in org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer
Running org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.722 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC
testLocalizerRPC(org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC)  Time elapsed: 0.314 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC$LocalizerService.start(TestPBLocalizerRPC.java:56)
	at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC.testLocalizerRPC(TestPBLocalizerRPC.java:85)

Running org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.681 sec - in org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl
Running org.apache.hadoop.yarn.server.nodemanager.TestNodeManager
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.685 sec - in org.apache.hadoop.yarn.server.nodemanager.TestNodeManager
Running org.apache.hadoop.yarn.server.nodemanager.metrics.TestNodeManagerMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.586 sec - in org.apache.hadoop.yarn.server.nodemanager.metrics.TestNodeManagerMetrics
Running org.apache.hadoop.yarn.server.nodemanager.TestDeletionService
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.562 sec - in org.apache.hadoop.yarn.server.nodemanager.TestDeletionService
Running org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 3.133 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync
testKillContainersOnResync(org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync)  Time elapsed: 2.064 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testKillContainersOnResync(TestNodeManagerResync.java:104)

testBlockNewContainerRequestsOnStartAndResync(org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync)  Time elapsed: 0.203 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testBlockNewContainerRequestsOnStartAndResync(TestNodeManagerResync.java:136)

testNMshutdownWhenResyncThrowException(org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync)  Time elapsed: 0.182 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMshutdownWhenResyncThrowException(TestNodeManagerResync.java:160)

testNMSentContainerStatusOnResync(org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync)  Time elapsed: 0.311 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync(TestNodeManagerResync.java:258)

Running org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.908 sec - in org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService
Running org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.61 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown
testKillContainersOnShutdown(org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown)  Time elapsed: 2.234 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testKillContainersOnShutdown(TestNodeManagerShutdown.java:116)

Running org.apache.hadoop.yarn.server.nodemanager.TestEventFlow
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.278 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.TestEventFlow
testSuccessfulContainerLaunch(org.apache.hadoop.yarn.server.nodemanager.TestEventFlow)  Time elapsed: 1.941 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.createServer(ResourceLocalizationService.java:281)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.serviceStart(ResourceLocalizationService.java:261)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:295)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestEventFlow.testSuccessfulContainerLaunch(TestEventFlow.java:133)

Running org.apache.hadoop.yarn.server.nodemanager.TestRecordFactory
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.27 sec - in org.apache.hadoop.yarn.server.nodemanager.TestRecordFactory
Running org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater
Tests run: 11, Failures: 1, Errors: 8, Skipped: 0, Time elapsed: 225.668 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater
testNMRegistration(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 200.199 sec  <<< FAILURE!
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertFalse(Assert.java:68)
	at org.junit.Assert.assertFalse(Assert.java:79)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNMRegistration(TestNodeStatusUpdater.java:856)

testStopReentrant(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 0.252 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testStopReentrant(TestNodeStatusUpdater.java:898)

testNodeDecommision(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 0.259 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNodeDecommision(TestNodeStatusUpdater.java:926)

testNMShutdownForRegistrationFailure(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 0.231 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.verifyNodeStartFailure(TestNodeStatusUpdater.java:1333)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNMShutdownForRegistrationFailure(TestNodeStatusUpdater.java:983)

testNMConnectionToRM(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 0.24 sec  <<< ERROR!
java.lang.Exception: NM should have tried re-connecting to RM during period of at least 5000 ms, but stopped retrying within 55000 ms: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNMConnectionToRM(TestNodeStatusUpdater.java:1016)

testApplicationKeepAlive(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 0.202 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testApplicationKeepAlive(TestNodeStatusUpdater.java:1119)

testCompletedContainerStatusBackup(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 0.203 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testCompletedContainerStatusBackup(TestNodeStatusUpdater.java:1169)

testNodeStatusUpdaterRetryAndNMShutdown(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 10.286 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNodeStatusUpdaterRetryAndNMShutdown(TestNodeStatusUpdater.java:1197)

testRMVersionLessThanMinimum(org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater)  Time elapsed: 0.215 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testRMVersionLessThanMinimum(TestNodeStatusUpdater.java:1252)

Running org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.494 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot
testClearLocalDirWhenNodeReboot(org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot)  Time elapsed: 2.283 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:197)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot.testClearLocalDirWhenNodeReboot(TestNodeManagerReboot.java:103)

Running org.apache.hadoop.yarn.server.nodemanager.TestContainerManagerWithLCE
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.113 sec - in org.apache.hadoop.yarn.server.nodemanager.TestContainerManagerWithLCE
Running org.apache.hadoop.yarn.server.nodemanager.TestDefaultContainerExecutor
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.865 sec - in org.apache.hadoop.yarn.server.nodemanager.TestDefaultContainerExecutor
Running org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.543 sec - in org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger
Running org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer
Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.834 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer
testNMWebApp(org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer)  Time elapsed: 0.042 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:62)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer.testNMWebApp(TestNMWebServer.java:167)

Running org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.615 sec - in org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices
Running org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage
Tests run: 2, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.953 sec - in org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage
Running org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.04 sec - in org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers
Running org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps
Tests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.595 sec - in org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps
Running org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.155 sec - in org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor
Running org.apache.hadoop.yarn.server.nodemanager.util.TestProcessIdFileReader
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.275 sec - in org.apache.hadoop.yarn.server.nodemanager.util.TestProcessIdFileReader
Running org.apache.hadoop.yarn.server.nodemanager.util.TestCgroupsLCEResourcesHandler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.004 sec - in org.apache.hadoop.yarn.server.nodemanager.util.TestCgroupsLCEResourcesHandler
Running org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.752 sec - in org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection
Running org.apache.hadoop.yarn.server.nodemanager.TestRPCFactories
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.024 sec - in org.apache.hadoop.yarn.server.nodemanager.TestRPCFactories
Running org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.214 sec - in org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks
Running org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.946 sec - in org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService

Results :

Failed tests: 
  TestNodeStatusUpdater.testNMRegistration:856 null

Tests in error: 
  TestContainersMonitor.testContainerKillOnMemoryOverflow:190 » YarnRuntime java...
  TestLogAggregationService.testLogAggregationForRealContainerLaunch:773 » YarnRuntime
  TestContainerManager.testContainerManagerInitialization:147 » YarnRuntime java...
  TestContainerManager.testContainerSetup:179 » YarnRuntime java.net.BindExcepti...
  TestContainerManager.testContainerLaunchAndStop:259 » YarnRuntime java.net.Bin...
  TestContainerManager.testContainerLaunchAndExitSuccess:439 » YarnRuntime java....
  TestContainerManager.testContainerLaunchAndExitFailure:450 » YarnRuntime java....
  TestContainerManager.testLocalFilesCleanup:467 » YarnRuntime java.net.BindExce...
  TestContainerManager.testContainerLaunchFromPreviousRM:574 » YarnRuntime java....
  TestContainerManager.testMultipleContainersLaunch:638 » YarnRuntime java.net.B...
  TestContainerManager.testMultipleContainersStopAndGetStatus:683 » YarnRuntime ...
  TestContainerManager.testStartContainerFailureWithUnknownAuxService:760 » YarnRuntime
  TestContainerLaunch.testContainerEnvVariables:370 » YarnRuntime java.net.BindE...
  TestContainerLaunch.testDelayedKill:748->internalKillTest:624 » YarnRuntime ja...
  TestContainerLaunch.testImmediateKill:753->internalKillTest:624 » YarnRuntime ...
  TestPBLocalizerRPC.testLocalizerRPC:85 » YarnRuntime java.net.BindException: P...
  TestNodeManagerResync.testKillContainersOnResync:104 » YarnRuntime NMWebapps f...
  TestNodeManagerResync.testBlockNewContainerRequestsOnStartAndResync:136 » YarnRuntime
  TestNodeManagerResync.testNMshutdownWhenResyncThrowException:160 » YarnRuntime
  TestNodeManagerResync.testNMSentContainerStatusOnResync:258 » YarnRuntime NMWe...
  TestNodeManagerShutdown.testKillContainersOnShutdown:116 » YarnRuntime NMWebap...
  TestEventFlow.testSuccessfulContainerLaunch:133 » YarnRuntime java.net.BindExc...
  TestNodeStatusUpdater.testStopReentrant:898 » YarnRuntime NMWebapps failed to ...
  TestNodeStatusUpdater.testNodeDecommision:926 » YarnRuntime NMWebapps failed t...
  TestNodeStatusUpdater.testNMShutdownForRegistrationFailure:983->verifyNodeStartFailure:1333 » YarnRuntime
  TestNodeStatusUpdater.testNMConnectionToRM:1026  NM should have tried re-conne...
  TestNodeStatusUpdater.testApplicationKeepAlive:1119 » YarnRuntime NMWebapps fa...
  TestNodeStatusUpdater.testCompletedContainerStatusBackup:1169 » YarnRuntime NM...
  TestNodeStatusUpdater.testNodeStatusUpdaterRetryAndNMShutdown:1197 » YarnRuntime
  TestNodeStatusUpdater.testRMVersionLessThanMinimum:1252 » YarnRuntime NMWebapp...
  TestNodeManagerReboot.testClearLocalDirWhenNodeReboot:103 » YarnRuntime NMWeba...
  TestNMWebServer.testNMWebApp:167 » YarnRuntime NMWebapps failed to start.

Tests run: 209, Failures: 1, Errors: 32, Skipped: 1

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-server-web-proxy 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-server-web-proxy ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-server-web-proxy ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-server-web-proxy ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-server-web-proxy ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-server-web-proxy ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-jar-plugin:2.3.1:test-jar (default) @ hadoop-yarn-server-web-proxy ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-server-web-proxy ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.49 sec - in org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilter
Running org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilterInitializer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.48 sec - in org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilterInitializer
Running org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.121 sec - in org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet
Running org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.387 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer
testStart(org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer)  Time elapsed: 1.296 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Could not start proxy web server
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxy.serviceStart(WebAppProxy.java:110)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer.testStart(TestWebAppProxyServer.java:51)

Running org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.411 sec - in org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils

Results :

Tests in error: 
  TestWebAppProxyServer.testStart:51 » YarnRuntime Could not start proxy web ser...

Tests run: 15, Failures: 0, Errors: 1, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-server-applicationhistoryservice 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-server-applicationhistoryservice ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-server-applicationhistoryservice ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-server-applicationhistoryservice ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-server-applicationhistoryservice ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-server-applicationhistoryservice ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-server-applicationhistoryservice ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestGenericObjectMapper
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.516 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestGenericObjectMapper
Running org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore
Tests run: 12, Failures: 0, Errors: 12, Skipped: 0, Time elapsed: 1.54 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore
testGetSingleEntity(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 1.078 sec  <<< ERROR!
java.lang.UnsatisfiedLinkError: Could not load library. Reasons: [leveldbjni64-1.8 (Not found in java.library.path), leveldbjni-1.8 (Not found in java.library.path), leveldbjni (Not found in java.library.path), /tmp/libleveldbjni-64-1-2198902307577690218.8 (/tmp/libleveldbjni-64-1-2198902307577690218.8: cannot open shared object file: No such file or directory)]
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testGetEntities(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.054 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testGetEntitiesWithFromId(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.035 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testGetEntitiesWithFromTs(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.027 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testGetEntitiesWithPrimaryFilters(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.023 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testGetEntitiesWithSecondaryFilters(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.021 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testGetEvents(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.019 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testCacheSizes(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.029 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testGetEntityTypes(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.019 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testDeleteEntities(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.018 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testDeleteEntitiesPrimaryFilters(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.018 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

testFromTsWithDeletion(org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore)  Time elapsed: 0.036 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestLeveldbTimelineStore.setup(TestLeveldbTimelineStore.java:62)

Running org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestMemoryTimelineStore
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.31 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.TestMemoryTimelineStore
Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 1.31 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService
testApplicationReport(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService)  Time elapsed: 0.909 sec  <<< ERROR!
java.lang.UnsatisfiedLinkError: Could not load library. Reasons: [leveldbjni64-1.8 (Not found in java.library.path), leveldbjni-1.8 (Not found in java.library.path), leveldbjni (Not found in java.library.path), /tmp/libleveldbjni-64-1-6534036182893864530.8 (/tmp/libleveldbjni-64-1-6534036182893864530.8: cannot open shared object file: No such file or directory)]
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:72)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService.setup(TestApplicationHistoryClientService.java:68)

testApplications(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService)  Time elapsed: 0.133 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:72)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService.setup(TestApplicationHistoryClientService.java:68)

testApplicationAttemptReport(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService)  Time elapsed: 0.071 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:72)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService.setup(TestApplicationHistoryClientService.java:68)

testApplicationAttempts(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService)  Time elapsed: 0.038 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:72)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService.setup(TestApplicationHistoryClientService.java:68)

testContainerReport(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService)  Time elapsed: 0.03 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:72)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService.setup(TestApplicationHistoryClientService.java:68)

testContainers(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService)  Time elapsed: 0.027 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org.fusesource.leveldbjni.JniDBFactory (initialization failure)
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:72)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService.setup(TestApplicationHistoryClientService.java:68)

Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerImpl
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.772 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerImpl
Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.489 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore
Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer
Tests run: 2, Failures: 1, Errors: 1, Skipped: 0, Time elapsed: 1.189 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer
testStartStopServer(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer)  Time elapsed: 1.012 sec  <<< ERROR!
java.lang.UnsatisfiedLinkError: Could not load library. Reasons: [leveldbjni64-1.8 (Not found in java.library.path), leveldbjni-1.8 (Not found in java.library.path), leveldbjni (Not found in java.library.path), /tmp/libleveldbjni-64-1-7963247551736834954.8 (/tmp/libleveldbjni-64-1-7963247551736834954.8: cannot open shared object file: No such file or directory)]
	at org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)
	at org.fusesource.hawtjni.runtime.Library.load(Library.java:140)
	at org.fusesource.leveldbjni.JniDBFactory.<clinit>(JniDBFactory.java:48)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:72)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testStartStopServer(TestApplicationHistoryServer.java:41)

testLaunch(org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer)  Time elapsed: 0.087 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<-1>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunch(TestApplicationHistoryServer.java:65)

Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 38.368 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore
Running org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestTimelineWebServices
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.316 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestTimelineWebServices
Running org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.793 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices
Running org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.69 sec - in org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp

Results :

Failed tests: 
  TestApplicationHistoryServer.testLaunch:65 expected:<0> but was:<-1>

Tests in error: 
  TestLeveldbTimelineStore.setup:62 » UnsatisfiedLink Could not load library. Re...
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestLeveldbTimelineStore.setup:62 » NoClassDefFound org.fusesource.leveldbjni....
  TestApplicationHistoryClientService.setup:68 » UnsatisfiedLink Could not load ...
  TestApplicationHistoryClientService.setup:68 » NoClassDefFound org.fusesource....
  TestApplicationHistoryClientService.setup:68 » NoClassDefFound org.fusesource....
  TestApplicationHistoryClientService.setup:68 » NoClassDefFound org.fusesource....
  TestApplicationHistoryClientService.setup:68 » NoClassDefFound org.fusesource....
  TestApplicationHistoryClientService.setup:68 » NoClassDefFound org.fusesource....
  TestApplicationHistoryServer.testStartStopServer:41 » UnsatisfiedLink Could no...

Tests run: 68, Failures: 1, Errors: 19, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-server-resourcemanager 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-server-resourcemanager ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-server-resourcemanager ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-server-resourcemanager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-server-resourcemanager ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-server-resourcemanager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-jar-plugin:2.3.1:test-jar (default) @ hadoop-yarn-server-resourcemanager ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-server-resourcemanager ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 3.188 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization
testAuthorizedAccess[0](org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization)  Time elapsed: 1.778 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testAuthorizedAccess(TestAMAuthorization.java:170)

testUnauthorizedAccess[0](org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization)  Time elapsed: 0.258 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testUnauthorizedAccess(TestAMAuthorization.java:231)

testAuthorizedAccess[1](org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization)  Time elapsed: 0.27 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testAuthorizedAccess(TestAMAuthorization.java:170)

testUnauthorizedAccess[1](org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization)  Time elapsed: 0.233 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testUnauthorizedAccess(TestAMAuthorization.java:231)

Running org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.TestRMContainerImpl
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.021 sec - in org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.TestRMContainerImpl
Running org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.76 sec - in org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates
Running org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.665 sec - in org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId
Running org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.121 sec - in org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart
Running org.apache.hadoop.yarn.server.resourcemanager.TestRMHA
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.991 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestRMHA
Running org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService
Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 9.062 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService
testTokenRenewalWrongUser(org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService)  Time elapsed: 0.009 sec  <<< FAILURE!
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:48)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService$4.run(TestClientRMService.java:481)
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService$4.run(TestClientRMService.java:474)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService.testTokenRenewalWrongUser(TestClientRMService.java:474)

Running org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService
Tests run: 15, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 11.632 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService
testServiceAclsRefreshWithLocalConfigurationProvider(org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService)  Time elapsed: 0.457 sec  <<< FAILURE!
java.lang.AssertionError: Using localConfigurationProvider. Should not get any exception.
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testServiceAclsRefreshWithLocalConfigurationProvider(TestRMAdminService.java:219)

testServiceAclsRefreshWithFileSystemBasedConfigurationProvider(org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService)  Time elapsed: 0.9 sec  <<< FAILURE!
java.lang.AssertionError: Should not get any exceptions
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testServiceAclsRefreshWithFileSystemBasedConfigurationProvider(TestRMAdminService.java:248)

testRMInitialsWithFileSystemBasedConfigurationProvider(org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService)  Time elapsed: 0.807 sec  <<< FAILURE!
java.lang.AssertionError: Should not get any exceptions
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider(TestRMAdminService.java:656)

Running org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens
Tests run: 6, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 18.577 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens
testShortCircuitRenewCancel(org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens)  Time elapsed: 0.261 sec  <<< ERROR!
java.lang.RuntimeException: getProxy
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens$YarnBadRPC.getProxy(TestClientRMTokens.java:334)
	at org.apache.hadoop.yarn.client.RMProxy$1.run(RMProxy.java:134)
	at java.security.AccessController.doPrivileged(AccessController.java:303)
	at javax.security.auth.Subject.doAs(Subject.java:494)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1536)
	at org.apache.hadoop.yarn.client.RMProxy.getProxy(RMProxy.java:130)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:93)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:70)
	at org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier$Renewer.getRmClient(RMDelegationTokenIdentifier.java:158)
	at org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier$Renewer.renew(RMDelegationTokenIdentifier.java:101)
	at org.apache.hadoop.security.token.Token.renew(Token.java:377)
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens.checkShortCircuitRenewCancel(TestClientRMTokens.java:306)
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens.testShortCircuitRenewCancel(TestClientRMTokens.java:240)

testShortCircuitRenewCancelWildcardAddress(org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens)  Time elapsed: 0.044 sec  <<< ERROR!
java.lang.NullPointerException: null
	at org.apache.hadoop.net.NetUtils.isLocalAddress(NetUtils.java:684)
	at org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier$Renewer.getRmClient(RMDelegationTokenIdentifier.java:149)
	at org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier$Renewer.renew(RMDelegationTokenIdentifier.java:101)
	at org.apache.hadoop.security.token.Token.renew(Token.java:377)
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens.checkShortCircuitRenewCancel(TestClientRMTokens.java:306)
	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens.testShortCircuitRenewCancelWildcardAddress(TestClientRMTokens.java:247)

Running org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterService
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.888 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterService
Running org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.797 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs
Running org.apache.hadoop.yarn.server.resourcemanager.TestAppManager
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.119 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestAppManager
Running org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.241 sec - in org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy
Running org.apache.hadoop.yarn.server.resourcemanager.monitor.TestSchedulingMonitor
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.148 sec - in org.apache.hadoop.yarn.server.resourcemanager.monitor.TestSchedulingMonitor
Running org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.681 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions
Running org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.881 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA
Running org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 27.033 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler
Running org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 153.94 sec - in org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter
Running org.apache.hadoop.yarn.server.resourcemanager.resource.TestResources
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.405 sec - in org.apache.hadoop.yarn.server.resourcemanager.resource.TestResources
Running org.apache.hadoop.yarn.server.resourcemanager.resource.TestResourceWeights
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.097 sec - in org.apache.hadoop.yarn.server.resourcemanager.resource.TestResourceWeights
Running org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 3.137 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication
testMoveRejectedByScheduler(org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication)  Time elapsed: 2.221 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication.setUp(TestMoveApplication.java:58)

testMoveTooLate(org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication)  Time elapsed: 0.211 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication.setUp(TestMoveApplication.java:58)

testMoveSuccessful(org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication)  Time elapsed: 0.218 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication.setUp(TestMoveApplication.java:58)

testMoveRejectedByPermissions(org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication)  Time elapsed: 0.375 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication.setUp(TestMoveApplication.java:58)

Running org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.335 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.545 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.551 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.718 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.94 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.56 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerQueueACLs
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 55.592 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerQueueACLs
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.58 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.029 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils
Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.554 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils
testValidateResourceBlacklistRequest(org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils)  Time elapsed: 1.876 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils.testValidateResourceBlacklistRequest(TestSchedulerUtils.java:291)

Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.623 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.619 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.856 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSSchedulerApp
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.61 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSSchedulerApp
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.551 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerEventLog
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.245 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerEventLog
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.282 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler
Tests run: 58, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.341 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSLeafQueue
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.419 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSLeafQueue
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.45 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.948 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerConfiguration
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.451 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerConfiguration
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestEmptyQueues
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.286 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestEmptyQueues
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.404 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueueManager
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.864 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueueManager
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestSchedulingPolicy
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.398 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestSchedulingPolicy
Running org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerQueueACLs
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 53.212 sec - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerQueueACLs
Running org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions
Tests run: 40, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.461 sec - in org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions
Running org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions
Tests run: 60, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 7.138 sec - in org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions
Running org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.143 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager
Running org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterLauncher
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.993 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterLauncher
Running org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart
Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 195.079 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart
Running org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.563 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService
Running org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.235 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger
Running org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.28 sec - in org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched
Running org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps
Tests run: 40, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 181.52 sec - in org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps
Running org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesFairScheduler
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.945 sec - in org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesFairScheduler
Running org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.036 sec - in org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices
Running org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.681 sec - in org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp
Running org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.89 sec - in org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage
Running org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes
Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.62 sec - in org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes
Running org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewerLifecycle
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.628 sec - in org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewerLifecycle
Running org.apache.hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.191 sec - in org.apache.hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens
Running org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.697 sec - in org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens
Running org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.74 sec - in org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer
Running org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 3.33 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens
testTokenExpiry[0](org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens)  Time elapsed: 1.672 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens.testTokenExpiry(TestAMRMTokens.java:96)

testMasterKeyRollOver[0](org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens)  Time elapsed: 0.322 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens.testMasterKeyRollOver(TestAMRMTokens.java:207)

testTokenExpiry[1](org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens)  Time elapsed: 0.31 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens.testTokenExpiry(TestAMRMTokens.java:96)

testMasterKeyRollOver[1](org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens)  Time elapsed: 0.239 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens.testMasterKeyRollOver(TestAMRMTokens.java:207)

Running org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMExpiry
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.538 sec - in org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMExpiry
Running org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestRMNMRPCResponseId
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.015 sec - in org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestRMNMRPCResponseId
Running org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMReconnect
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.319 sec - in org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMReconnect
Running org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.326 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup
Running org.apache.hadoop.yarn.server.resourcemanager.TestRM
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 70.109 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestRM
Running org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.019 sec - in org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore
Running org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.621 sec - in org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore
Running org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.847 sec - in org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections
Running org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.872 sec - in org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector

Results :

Failed tests: 
  TestClientRMService.testTokenRenewalWrongUser:474 null
  TestRMAdminService.testServiceAclsRefreshWithLocalConfigurationProvider:219 Using localConfigurationProvider. Should not get any exception.
  TestRMAdminService.testServiceAclsRefreshWithFileSystemBasedConfigurationProvider:248 Should not get any exceptions
  TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider:656 Should not get any exceptions

Tests in error: 
  TestAMAuthorization.testAuthorizedAccess:170 » YarnRuntime java.net.BindExcept...
  TestAMAuthorization.testUnauthorizedAccess:231 » YarnRuntime java.net.BindExce...
  TestAMAuthorization.testAuthorizedAccess:170 » YarnRuntime java.net.BindExcept...
  TestAMAuthorization.testUnauthorizedAccess:231 » YarnRuntime java.net.BindExce...
  TestClientRMTokens.testShortCircuitRenewCancel:240->checkShortCircuitRenewCancel:306 » Runtime
  TestClientRMTokens.testShortCircuitRenewCancelWildcardAddress:247->checkShortCircuitRenewCancel:306 » NullPointer
  TestMoveApplication.setUp:58 » YarnRuntime java.net.BindException: Problem bin...
  TestMoveApplication.setUp:58 » YarnRuntime java.net.BindException: Problem bin...
  TestMoveApplication.setUp:58 » YarnRuntime java.net.BindException: Problem bin...
  TestMoveApplication.setUp:58 » YarnRuntime java.net.BindException: Problem bin...
  TestSchedulerUtils.testValidateResourceBlacklistRequest:291 » YarnRuntime java...
  TestAMRMTokens.testTokenExpiry:96 » YarnRuntime java.net.BindException: Proble...
  TestAMRMTokens.testMasterKeyRollOver:207 » YarnRuntime java.net.BindException:...
  TestAMRMTokens.testTokenExpiry:96 » YarnRuntime java.net.BindException: Proble...
  TestAMRMTokens.testMasterKeyRollOver:207 » YarnRuntime java.net.BindException:...

Tests run: 592, Failures: 4, Errors: 15, Skipped: 1

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-server-tests 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-server-tests ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-server-tests ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-server-tests ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-server-tests ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-server-tests ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-jar-plugin:2.3.1:test-jar (default) @ hadoop-yarn-server-tests ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-server-tests ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.server.TestMiniYARNClusterForHA
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.998 sec - in org.apache.hadoop.yarn.server.TestMiniYARNClusterForHA
Running org.apache.hadoop.yarn.server.TestDiskFailures
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.242 sec - in org.apache.hadoop.yarn.server.TestDiskFailures
Running org.apache.hadoop.yarn.server.TestRMNMSecretKeys
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.628 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.TestRMNMSecretKeys
testNMUpdation(org.apache.hadoop.yarn.server.TestRMNMSecretKeys)  Time elapsed: 2.539 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.TestRMNMSecretKeys.validateRMNMKeyExchange(TestRMNMSecretKeys.java:73)
	at org.apache.hadoop.yarn.server.TestRMNMSecretKeys.testNMUpdation(TestRMNMSecretKeys.java:43)

Running org.apache.hadoop.yarn.server.TestContainerManagerSecurity
Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 31.243 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.TestContainerManagerSecurity
testContainerManager[0](org.apache.hadoop.yarn.server.TestContainerManagerSecurity)  Time elapsed: 18.96 sec  <<< FAILURE!
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:48)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.hadoop.yarn.server.TestContainerManagerSecurity.testNMTokens(TestContainerManagerSecurity.java:242)
	at org.apache.hadoop.yarn.server.TestContainerManagerSecurity.testContainerManager(TestContainerManagerSecurity.java:144)

testContainerManager[1](org.apache.hadoop.yarn.server.TestContainerManagerSecurity)  Time elapsed: 11.583 sec  <<< FAILURE!
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:48)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.hadoop.yarn.server.TestContainerManagerSecurity.testNMTokens(TestContainerManagerSecurity.java:242)
	at org.apache.hadoop.yarn.server.TestContainerManagerSecurity.testContainerManager(TestContainerManagerSecurity.java:144)


Results :

Failed tests: 
  TestContainerManagerSecurity.testContainerManager:144->testNMTokens:242 null
  TestContainerManagerSecurity.testContainerManager:144->testNMTokens:242 null

Tests in error: 
  TestRMNMSecretKeys.testNMUpdation:43->validateRMNMKeyExchange:73 » YarnRuntime

Tests run: 7, Failures: 2, Errors: 1, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-client 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-client ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-client ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-client ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-client ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.client.api.async.impl.TestAMRMClientAsync
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.092 sec - in org.apache.hadoop.yarn.client.api.async.impl.TestAMRMClientAsync
Running org.apache.hadoop.yarn.client.api.async.impl.TestNMClientAsync
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.358 sec - in org.apache.hadoop.yarn.client.api.async.impl.TestNMClientAsync
Running org.apache.hadoop.yarn.client.api.impl.TestAMRMClientContainerRequest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.95 sec - in org.apache.hadoop.yarn.client.api.impl.TestAMRMClientContainerRequest
Running org.apache.hadoop.yarn.client.api.impl.TestYarnClient
Tests run: 13, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 17.971 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.api.impl.TestYarnClient
testClientStop(org.apache.hadoop.yarn.client.api.impl.TestYarnClient)  Time elapsed: 2.23 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.serviceStart(ApplicationMasterService.java:136)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:493)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:874)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:871)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:871)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:915)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.api.impl.TestYarnClient.testClientStop(TestYarnClient.java:99)

testAMMRTokens(org.apache.hadoop.yarn.client.api.impl.TestYarnClient)  Time elapsed: 4.041 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":53625; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy48.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy49.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.yarn.client.api.impl.TestYarnClient.createApp(TestYarnClient.java:649)
	at org.apache.hadoop.yarn.client.api.impl.TestYarnClient.testAMMRTokens(TestYarnClient.java:597)

Running org.apache.hadoop.yarn.client.api.impl.TestAHSClient
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.568 sec - in org.apache.hadoop.yarn.client.api.impl.TestAHSClient
Running org.apache.hadoop.yarn.client.api.impl.TestAMRMClient
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.656 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.api.impl.TestAMRMClient
org.apache.hadoop.yarn.client.api.impl.TestAMRMClient  Time elapsed: 7.656 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":34399; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy78.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterNodes(ApplicationClientProtocolPBClientImpl.java:238)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy79.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNodeReports(YarnClientImpl.java:367)
	at org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.setup(TestAMRMClient.java:121)

Running org.apache.hadoop.yarn.client.api.impl.TestTimelineClient
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.479 sec - in org.apache.hadoop.yarn.client.api.impl.TestTimelineClient
Running org.apache.hadoop.yarn.client.api.impl.TestNMClient
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 12.196 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.api.impl.TestNMClient
testNMClientNoCleanupOnStop(org.apache.hadoop.yarn.client.api.impl.TestNMClient)  Time elapsed: 7.233 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":38541; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy76.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterNodes(ApplicationClientProtocolPBClientImpl.java:238)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy77.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNodeReports(YarnClientImpl.java:367)
	at org.apache.hadoop.yarn.client.api.impl.TestNMClient.setup(TestNMClient.java:102)

testNMClientNoCleanupOnStop(org.apache.hadoop.yarn.client.api.impl.TestNMClient)  Time elapsed: 7.233 sec  <<< ERROR!
java.lang.NullPointerException: null
	at org.apache.hadoop.yarn.client.api.impl.TestNMClient.tearDown(TestNMClient.java:187)

testNMClient(org.apache.hadoop.yarn.client.api.impl.TestNMClient)  Time elapsed: 4.861 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":59921; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy76.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterNodes(ApplicationClientProtocolPBClientImpl.java:238)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy77.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNodeReports(YarnClientImpl.java:367)
	at org.apache.hadoop.yarn.client.api.impl.TestNMClient.setup(TestNMClient.java:102)

testNMClient(org.apache.hadoop.yarn.client.api.impl.TestNMClient)  Time elapsed: 4.863 sec  <<< ERROR!
java.lang.NullPointerException: null
	at org.apache.hadoop.yarn.client.api.impl.TestNMClient.tearDown(TestNMClient.java:187)

Running org.apache.hadoop.yarn.client.TestRMAdminCLI
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.676 sec - in org.apache.hadoop.yarn.client.TestRMAdminCLI
Running org.apache.hadoop.yarn.client.TestRMFailover
Running org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA
Tests run: 17, Failures: 0, Errors: 17, Skipped: 0, Time elapsed: 33.468 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA
testGetApplicationReportOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 3.923 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetNewApplicationOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.924 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetClusterMetricsOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.903 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetApplicationsOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.875 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetClusterNodesOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.87 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetQueueInfoOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.826 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetQueueUserAclsOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.887 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetApplicationAttemptReportOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.838 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetApplicationAttemptsOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.857 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetContainerReportOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.754 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetContainersOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.821 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testSubmitApplicationOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.788 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testMoveApplicationAcrossQueuesOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.789 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testForceKillApplicationOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.788 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testGetDelegationTokenOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.764 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testRenewDelegationTokenOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.76 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

testCancelDelegationTokenOnHA(org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA)  Time elapsed: 1.828 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.initiate(TestApplicationClientProtocolOnHA.java:54)

Running org.apache.hadoop.yarn.client.TestYarnApiClasses
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.46 sec - in org.apache.hadoop.yarn.client.TestYarnApiClasses
Running org.apache.hadoop.yarn.client.TestResourceTrackerOnHA
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.102 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.TestResourceTrackerOnHA
testResourceTrackerOnHA(org.apache.hadoop.yarn.client.TestResourceTrackerOnHA)  Time elapsed: 3.83 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestResourceTrackerOnHA.initiate(TestResourceTrackerOnHA.java:44)

Running org.apache.hadoop.yarn.client.TestResourceManagerAdministrationProtocolPBClientImpl
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.314 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.TestResourceManagerAdministrationProtocolPBClientImpl
org.apache.hadoop.yarn.client.TestResourceManagerAdministrationProtocolPBClientImpl  Time elapsed: 3.314 sec  <<< ERROR!
java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.client.TestResourceManagerAdministrationProtocolPBClientImpl.setUpResourceManager(TestResourceManagerAdministrationProtocolPBClientImpl.java:92)

Running org.apache.hadoop.yarn.client.TestGetGroups
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.231 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.TestGetGroups
org.apache.hadoop.yarn.client.TestGetGroups  Time elapsed: 3.231 sec  <<< ERROR!
java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.client.TestGetGroups.setUpResourceManager(TestGetGroups.java:65)

Running org.apache.hadoop.yarn.client.TestApplicationMasterServiceOnHA
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.884 sec <<< FAILURE! - in org.apache.hadoop.yarn.client.TestApplicationMasterServiceOnHA
testAllocateOnHA(org.apache.hadoop.yarn.client.TestApplicationMasterServiceOnHA)  Time elapsed: 3.583 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:321)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:93)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:446)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.client.ProtocolHATestBase.startHACluster(ProtocolHATestBase.java:277)
	at org.apache.hadoop.yarn.client.TestApplicationMasterServiceOnHA.initiate(TestApplicationMasterServiceOnHA.java:52)

Running org.apache.hadoop.yarn.client.cli.TestLogsCLI
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.405 sec - in org.apache.hadoop.yarn.client.cli.TestLogsCLI
Running org.apache.hadoop.yarn.client.cli.TestYarnCLI
Tests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.551 sec - in org.apache.hadoop.yarn.client.cli.TestYarnCLI

Results :

Tests in error: 
  TestYarnClient.testClientStop:99 » YarnRuntime java.net.BindException: Problem...
  TestYarnClient.testAMMRTokens:597->createApp:649 » UnknownHost Invalid host na...
  TestAMRMClient.setup:121 » UnknownHost Invalid host name: local host is: (unkn...
  TestNMClient.setup:102 » UnknownHost Invalid host name: local host is: (unknow...
  TestNMClient.tearDown:187 NullPointer
  TestNMClient.setup:102 » UnknownHost Invalid host name: local host is: (unknow...
  TestNMClient.tearDown:187 NullPointer
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestApplicationClientProtocolOnHA.initiate:54->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestResourceTrackerOnHA.initiate:44->ProtocolHATestBase.startHACluster:277 » YarnRuntime
  TestResourceManagerAdministrationProtocolPBClientImpl.setUpResourceManager:92 IO
  TestGetGroups.setUpResourceManager:65 IO ResourceManager failed to start. Fina...
  TestApplicationMasterServiceOnHA.initiate:52->ProtocolHATestBase.startHACluster:277 » YarnRuntime

Tests run: 106, Failures: 0, Errors: 28, Skipped: 0

[ERROR] There was a timeout or other error in the fork
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-applications 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-applications ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-applications-distributedshell 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-applications-distributedshell ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-applications-distributedshell ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-applications-distributedshell ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-applications-distributedshell ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-applications-distributedshell ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-jar-plugin:2.3.1:jar (default) @ hadoop-yarn-applications-distributedshell ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-applications-distributedshell ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell
Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 224.811 sec <<< FAILURE! - in org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell
testDSShell(org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell)  Time elapsed: 33.054 sec  <<< FAILURE!
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:48)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShell(TestDistributedShell.java:188)


Results :

Failed tests: 
  TestDistributedShell.testDSShell:188 null

Tests run: 8, Failures: 1, Errors: 0, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-applications-unmanaged-am-launcher 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-applications-unmanaged-am-launcher ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-yarn-applications-unmanaged-am-launcher ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-yarn-applications-unmanaged-am-launcher ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-yarn-applications-unmanaged-am-launcher ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-yarn-applications-unmanaged-am-launcher ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-yarn-applications-unmanaged-am-launcher ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.applications.unmanagedamlauncher.TestUnmanagedAMLauncher
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.392 sec - in org.apache.hadoop.yarn.applications.unmanagedamlauncher.TestUnmanagedAMLauncher

Results :

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-site 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-site ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-yarn-project 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-project ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client-core 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client-core ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- avro-maven-plugin:1.7.4:protocol (default) @ hadoop-mapreduce-client-core ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-client-core ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-client-core ---
[INFO] Compiling 1 source file to /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-client-core ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-client-core ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-client-core ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapred.TestJobConf
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.188 sec - in org.apache.hadoop.mapred.TestJobConf
Running org.apache.hadoop.mapred.TestIndexCache
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.636 sec - in org.apache.hadoop.mapred.TestIndexCache
Running org.apache.hadoop.mapred.TestJobEndNotifier
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.665 sec - in org.apache.hadoop.mapred.TestJobEndNotifier
Running org.apache.hadoop.mapred.TestLineRecordReader
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.768 sec - in org.apache.hadoop.mapred.TestLineRecordReader
Running org.apache.hadoop.mapred.TestJobAclsManager
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1 sec - in org.apache.hadoop.mapred.TestJobAclsManager
Running org.apache.hadoop.mapred.TestTaskLogAppender
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.256 sec - in org.apache.hadoop.mapred.TestTaskLogAppender
Running org.apache.hadoop.mapred.TestFileOutputCommitter
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.91 sec - in org.apache.hadoop.mapred.TestFileOutputCommitter
Running org.apache.hadoop.mapred.TestClusterStatus
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.094 sec - in org.apache.hadoop.mapred.TestClusterStatus
Running org.apache.hadoop.mapred.TestMaster
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.009 sec - in org.apache.hadoop.mapred.TestMaster
Running org.apache.hadoop.mapred.TestQueue
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.132 sec - in org.apache.hadoop.mapred.TestQueue
Running org.apache.hadoop.mapred.TestCounters
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.821 sec - in org.apache.hadoop.mapred.TestCounters
Running org.apache.hadoop.mapred.TestTaskLog
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.364 sec - in org.apache.hadoop.mapred.TestTaskLog
Running org.apache.hadoop.mapred.TestJobInfo
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.729 sec - in org.apache.hadoop.mapred.TestJobInfo
Running org.apache.hadoop.mapred.TestJobQueueClient
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.144 sec - in org.apache.hadoop.mapred.TestJobQueueClient
Running org.apache.hadoop.mapred.TestClock
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.116 sec - in org.apache.hadoop.mapred.TestClock
Running org.apache.hadoop.mapred.TestOldMethodsJobID
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.208 sec - in org.apache.hadoop.mapred.TestOldMethodsJobID
Running org.apache.hadoop.mapred.TestFileInputFormat
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.366 sec - in org.apache.hadoop.mapred.TestFileInputFormat
Running org.apache.hadoop.mapred.lib.db.TestDBInputFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.595 sec - in org.apache.hadoop.mapred.lib.db.TestDBInputFormat
Running org.apache.hadoop.mapred.lib.TestCombineFileRecordReader
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.232 sec - in org.apache.hadoop.mapred.lib.TestCombineFileRecordReader
Running org.apache.hadoop.mapred.TestSkipBadRecords
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.772 sec - in org.apache.hadoop.mapred.TestSkipBadRecords
Running org.apache.hadoop.mapreduce.tools.TestCLI
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.625 sec - in org.apache.hadoop.mapreduce.tools.TestCLI
Running org.apache.hadoop.mapreduce.split.TestJobSplitWriter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.096 sec - in org.apache.hadoop.mapreduce.split.TestJobSplitWriter
Running org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.384 sec - in org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager
Running org.apache.hadoop.mapreduce.TestJobMonitorAndPrint
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.207 sec - in org.apache.hadoop.mapreduce.TestJobMonitorAndPrint
Running org.apache.hadoop.mapreduce.TestContextFactory
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.182 sec - in org.apache.hadoop.mapreduce.TestContextFactory
Running org.apache.hadoop.mapreduce.TestJob
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.23 sec - in org.apache.hadoop.mapreduce.TestJob
Running org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.785 sec - in org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader
Running org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.259 sec - in org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader
Running org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.728 sec - in org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
Running org.apache.hadoop.mapreduce.lib.db.TestSplitters
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.926 sec - in org.apache.hadoop.mapreduce.lib.db.TestSplitters
Running org.apache.hadoop.mapreduce.lib.db.TestDbClasses
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.22 sec - in org.apache.hadoop.mapreduce.lib.db.TestDbClasses
Running org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.992 sec - in org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter
Running org.apache.hadoop.mapreduce.lib.output.TestPreemptableFileOutputCommitter
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.656 sec - in org.apache.hadoop.mapreduce.lib.output.TestPreemptableFileOutputCommitter
Running org.apache.hadoop.mapreduce.lib.output.TestFileOutputFormat
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.108 sec - in org.apache.hadoop.mapreduce.lib.output.TestFileOutputFormat
Running org.apache.hadoop.mapreduce.security.TestTokenCache
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.491 sec - in org.apache.hadoop.mapreduce.security.TestTokenCache
Running org.apache.hadoop.mapreduce.task.reduce.TestEventFetcher
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.536 sec - in org.apache.hadoop.mapreduce.task.reduce.TestEventFetcher
Running org.apache.hadoop.mapreduce.task.reduce.TestShuffleScheduler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.522 sec - in org.apache.hadoop.mapreduce.task.reduce.TestShuffleScheduler
Running org.apache.hadoop.mapreduce.task.reduce.TestMergeManager
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.301 sec - in org.apache.hadoop.mapreduce.task.reduce.TestMergeManager
Running org.apache.hadoop.mapreduce.task.reduce.TestFetcher
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.666 sec - in org.apache.hadoop.mapreduce.task.reduce.TestFetcher
Running org.apache.hadoop.mapreduce.task.reduce.TestMerger
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.287 sec - in org.apache.hadoop.mapreduce.task.reduce.TestMerger
Running org.apache.hadoop.mapreduce.TestShufflePlugin
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.171 sec - in org.apache.hadoop.mapreduce.TestShufflePlugin

Results :

Tests run: 134, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client-common 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-mapreduce-client-common ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-client-common ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-client-common ---
[INFO] Compiling 3 source files to /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-client-common ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-client-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-client-common ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapred.TestLocalModeWithNewApis
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.595 sec - in org.apache.hadoop.mapred.TestLocalModeWithNewApis
Running org.apache.hadoop.mapred.TestMRWithDistributedCache
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.767 sec - in org.apache.hadoop.mapred.TestMRWithDistributedCache
Running org.apache.hadoop.mapred.TestJobClientGetJob
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.628 sec - in org.apache.hadoop.mapred.TestJobClientGetJob
Running org.apache.hadoop.mapred.TestLocalDistributedCacheManager
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.808 sec - in org.apache.hadoop.mapred.TestLocalDistributedCacheManager
Running org.apache.hadoop.mapred.TestJobClient
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.292 sec - in org.apache.hadoop.mapred.TestJobClient
Running org.apache.hadoop.mapreduce.TestTypeConverter
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.69 sec - in org.apache.hadoop.mapreduce.TestTypeConverter
Running org.apache.hadoop.mapreduce.v2.api.records.TestIds
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.502 sec - in org.apache.hadoop.mapreduce.v2.api.records.TestIds
Running org.apache.hadoop.mapreduce.v2.jobhistory.TestFileNameIndexUtils
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.484 sec - in org.apache.hadoop.mapreduce.v2.jobhistory.TestFileNameIndexUtils
Running org.apache.hadoop.mapreduce.v2.jobhistory.TestJobHistoryUtils
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.172 sec - in org.apache.hadoop.mapreduce.v2.jobhistory.TestJobHistoryUtils
Running org.apache.hadoop.mapreduce.v2.TestRecordFactory
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.306 sec - in org.apache.hadoop.mapreduce.v2.TestRecordFactory
Running org.apache.hadoop.mapreduce.v2.util.TestMRApps
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.782 sec - in org.apache.hadoop.mapreduce.v2.util.TestMRApps
Running org.apache.hadoop.mapreduce.v2.TestRPCFactories
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.44 sec - in org.apache.hadoop.mapreduce.v2.TestRPCFactories

Results :

Tests run: 50, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client-shuffle 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client-shuffle ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-client-shuffle ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-client-shuffle ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-client-shuffle ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-client-shuffle ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-client-shuffle ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapred.TestShuffleHandler
Tests run: 7, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 3.221 sec - in org.apache.hadoop.mapred.TestShuffleHandler
Running org.apache.hadoop.mapred.TestFadvisedFileRegion
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.871 sec - in org.apache.hadoop.mapred.TestFadvisedFileRegion

Results :

Tests run: 8, Failures: 0, Errors: 0, Skipped: 1

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client-app 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client-app ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-client-app ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-client-app ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-client-app ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-client-app ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-jar-plugin:2.3.1:test-jar (default) @ hadoop-mapreduce-client-app ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-client-app ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapred.TestLocalContainerLauncher
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.671 sec - in org.apache.hadoop.mapred.TestLocalContainerLauncher
Running org.apache.hadoop.mapred.TestTaskAttemptListenerImpl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.637 sec - in org.apache.hadoop.mapred.TestTaskAttemptListenerImpl
Running org.apache.hadoop.mapreduce.jobhistory.TestJobSummary
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.547 sec - in org.apache.hadoop.mapreduce.jobhistory.TestJobSummary
Running org.apache.hadoop.mapreduce.jobhistory.TestEvents
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.002 sec - in org.apache.hadoop.mapreduce.jobhistory.TestEvents
Running org.apache.hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.335 sec - in org.apache.hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler
Running org.apache.hadoop.mapreduce.v2.app.TestJobEndNotifier
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 36.905 sec - in org.apache.hadoop.mapreduce.v2.app.TestJobEndNotifier
Running org.apache.hadoop.mapreduce.v2.app.TestMRAppComponentDependencies
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.296 sec - in org.apache.hadoop.mapreduce.v2.app.TestMRAppComponentDependencies
Running org.apache.hadoop.mapreduce.v2.app.metrics.TestMRAppMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.66 sec - in org.apache.hadoop.mapreduce.v2.app.metrics.TestMRAppMetrics
Running org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskImpl
Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.134 sec - in org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskImpl
Running org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.587 sec - in org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt
Running org.apache.hadoop.mapreduce.v2.app.job.impl.TestJobImpl
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.305 sec - in org.apache.hadoop.mapreduce.v2.app.job.impl.TestJobImpl
Running org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttemptContainerRequest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.331 sec - in org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttemptContainerRequest
Running org.apache.hadoop.mapreduce.v2.app.job.impl.TestShuffleProvider
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.424 sec - in org.apache.hadoop.mapreduce.v2.app.job.impl.TestShuffleProvider
Running org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.251 sec - in org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM
Running org.apache.hadoop.mapreduce.v2.app.local.TestLocalContainerAllocator
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.804 sec - in org.apache.hadoop.mapreduce.v2.app.local.TestLocalContainerAllocator
Running org.apache.hadoop.mapreduce.v2.app.TestMRClientService
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.282 sec - in org.apache.hadoop.mapreduce.v2.app.TestMRClientService
Running org.apache.hadoop.mapreduce.v2.app.TestRMContainerAllocator
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 80.051 sec - in org.apache.hadoop.mapreduce.v2.app.TestRMContainerAllocator
Running org.apache.hadoop.mapreduce.v2.app.commit.TestCommitterEventHandler
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.602 sec - in org.apache.hadoop.mapreduce.v2.app.commit.TestCommitterEventHandler
Running org.apache.hadoop.mapreduce.v2.app.TestRecovery
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 48.069 sec - in org.apache.hadoop.mapreduce.v2.app.TestRecovery
Running org.apache.hadoop.mapreduce.v2.app.TestKill
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.282 sec - in org.apache.hadoop.mapreduce.v2.app.TestKill
Running org.apache.hadoop.mapreduce.v2.app.TestTaskHeartbeatHandler
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.876 sec - in org.apache.hadoop.mapreduce.v2.app.TestTaskHeartbeatHandler
Running org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.589 sec - in org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup
Running org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.014 sec - in org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher
Running org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncherImpl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.576 sec - in org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncherImpl
Running org.apache.hadoop.mapreduce.v2.app.TestMRApp
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.977 sec - in org.apache.hadoop.mapreduce.v2.app.TestMRApp
Running org.apache.hadoop.mapreduce.v2.app.TestFail
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.564 sec - in org.apache.hadoop.mapreduce.v2.app.TestFail
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobs
Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.764 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobs
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServices
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.355 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServices
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobConf
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.355 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesJobConf
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestBlocks
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.922 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestBlocks
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebApp
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.75 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebApp
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesTasks
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.149 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesTasks
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestAppController
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.525 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestAppController
Running org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesAttempts
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.391 sec - in org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServicesAttempts
Running org.apache.hadoop.mapreduce.v2.app.TestMRAppMaster
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.193 sec - in org.apache.hadoop.mapreduce.v2.app.TestMRAppMaster
Running org.apache.hadoop.mapreduce.v2.app.TestRuntimeEstimators
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 82.798 sec - in org.apache.hadoop.mapreduce.v2.app.TestRuntimeEstimators
Running org.apache.hadoop.mapreduce.v2.app.TestAMInfos
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.049 sec - in org.apache.hadoop.mapreduce.v2.app.TestAMInfos
Running org.apache.hadoop.mapreduce.v2.app.speculate.TestDataStatistics
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.134 sec - in org.apache.hadoop.mapreduce.v2.app.speculate.TestDataStatistics
Running org.apache.hadoop.mapreduce.v2.app.TestFetchFailure
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.063 sec - in org.apache.hadoop.mapreduce.v2.app.TestFetchFailure

Results :

Tests run: 266, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client-hs 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client-hs ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:2.4.1:protoc (compile-protoc) @ hadoop-mapreduce-client-hs ---
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-client-hs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-client-hs ---
[INFO] Compiling 1 source file to /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-client-hs ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-client-hs ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-client-hs ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapreduce.v2.hs.TestJobHistory
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.787 sec - in org.apache.hadoop.mapreduce.v2.hs.TestJobHistory
Running org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryEvents
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.364 sec - in org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryEvents
Running org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.079 sec - in org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer
Running org.apache.hadoop.mapreduce.v2.hs.TestJobIdHistoryFileInfoMap
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.655 sec - in org.apache.hadoop.mapreduce.v2.hs.TestJobIdHistoryFileInfoMap
Running org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryParsing
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 98.196 sec - in org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryParsing
Running org.apache.hadoop.mapreduce.v2.hs.TestJHSDelegationTokenSecretManager
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.357 sec - in org.apache.hadoop.mapreduce.v2.hs.TestJHSDelegationTokenSecretManager
Running org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerFileSystemStateStoreService
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.771 sec - in org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerFileSystemStateStoreService
Running org.apache.hadoop.mapreduce.v2.hs.TestJobListCache
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.623 sec - in org.apache.hadoop.mapreduce.v2.hs.TestJobListCache
Running org.apache.hadoop.mapreduce.v2.hs.TestHistoryFileManager
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.79 sec - in org.apache.hadoop.mapreduce.v2.hs.TestHistoryFileManager
Running org.apache.hadoop.mapreduce.v2.hs.TestCompletedTask
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.903 sec - in org.apache.hadoop.mapreduce.v2.hs.TestCompletedTask
Running org.apache.hadoop.mapreduce.v2.hs.webapp.dao.TestJobInfo
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.898 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.dao.TestJobInfo
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesTasks
Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.005 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesTasks
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobs
Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.712 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobs
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobConf
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.65 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobConf
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery
Tests run: 25, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.996 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesAcls
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.268 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesAcls
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestBlocks
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.388 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestBlocks
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServices
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.65 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServices
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHSWebApp
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHSWebApp
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesAttempts
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.968 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesAttempts
Running org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryEntities
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.802 sec - in org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryEntities
Running org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer
Tests run: 3, Failures: 1, Errors: 2, Skipped: 0, Time elapsed: 36.185 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer
testStartStopServer(org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer)  Time elapsed: 12.182 sec  <<< ERROR!
org.apache.hadoop.service.ServiceStateException: java.net.BindException: Problem binding to [0.0.0.0:10033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer.serviceInit(HSAdminServer.java:100)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:142)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer.testStartStopServer(TestJobHistoryServer.java:73)

testReports(org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer)  Time elapsed: 13.17 sec  <<< ERROR!
org.apache.hadoop.service.ServiceStateException: java.net.BindException: Problem binding to [0.0.0.0:10033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer.serviceInit(HSAdminServer.java:100)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:142)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer.testReports(TestJobHistoryServer.java:106)

testLaunch(org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer)  Time elapsed: 10.412 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<-1>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer.testLaunch(TestJobHistoryServer.java:190)


Results :

Failed tests: 
  TestJobHistoryServer.testLaunch:190 expected:<0> but was:<-1>

Tests in error: 
  TestJobHistoryServer.testStartStopServer:73 » ServiceState java.net.BindExcept...
  TestJobHistoryServer.testReports:106 » ServiceState java.net.BindException: Pr...

Tests run: 176, Failures: 1, Errors: 2, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client-jobclient 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client-jobclient ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-client-jobclient ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-client-jobclient ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-client-jobclient ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-client-jobclient ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-jar-plugin:2.3.1:test-jar (default) @ hadoop-mapreduce-client-jobclient ---
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-client-jobclient ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.conf.TestJobConf
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.262 sec - in org.apache.hadoop.conf.TestJobConf
Running org.apache.hadoop.conf.TestNoDefaultsJobConf
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.504 sec - in org.apache.hadoop.conf.TestNoDefaultsJobConf
Running org.apache.hadoop.mapred.TestClusterMapReduceTestCase
Tests run: 4, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 145.798 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestClusterMapReduceTestCase
testMapReduce(org.apache.hadoop.mapred.TestClusterMapReduceTestCase)  Time elapsed: 24.487 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":43665; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestClusterMapReduceTestCase._testMapReduce(TestClusterMapReduceTestCase.java:67)
	at org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMapReduce(TestClusterMapReduceTestCase.java:89)

testMapReduceRestarting(org.apache.hadoop.mapred.TestClusterMapReduceTestCase)  Time elapsed: 35.133 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":59179; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestClusterMapReduceTestCase._testMapReduce(TestClusterMapReduceTestCase.java:67)
	at org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMapReduceRestarting(TestClusterMapReduceTestCase.java:93)

Running org.apache.hadoop.mapred.TestMRCJCJobClient
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.075 sec - in org.apache.hadoop.mapred.TestMRCJCJobClient
Running org.apache.hadoop.mapred.TestMultiFileInputFormat
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 163.981 sec - in org.apache.hadoop.mapred.TestMultiFileInputFormat
Running org.apache.hadoop.mapred.TestMiniMRChildTask
Tests run: 4, Failures: 4, Errors: 0, Skipped: 0, Time elapsed: 22.366 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestMiniMRChildTask
testTaskTempDir(org.apache.hadoop.mapred.TestMiniMRChildTask)  Time elapsed: 0.192 sec  <<< FAILURE!
java.lang.AssertionError: Exception in testing temp dir
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskTempDir(TestMiniMRChildTask.java:393)

testMapRedExecutionEnv(org.apache.hadoop.mapred.TestMiniMRChildTask)  Time elapsed: 0.085 sec  <<< FAILURE!
java.lang.AssertionError: Exception in testing propagation of env setting to child task
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.mapred.TestMiniMRChildTask.testMapRedExecutionEnv(TestMiniMRChildTask.java:453)

testTaskEnv(org.apache.hadoop.mapred.TestMiniMRChildTask)  Time elapsed: 0.092 sec  <<< FAILURE!
java.lang.AssertionError: Exception in testing child env
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:476)

testTaskOldEnv(org.apache.hadoop.mapred.TestMiniMRChildTask)  Time elapsed: 0.086 sec  <<< FAILURE!
java.lang.AssertionError: Exception in testing child env
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:499)

Running org.apache.hadoop.mapred.TestJobName
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 43.451 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestJobName
testComplexName(org.apache.hadoop.mapred.TestJobName)  Time elapsed: 25.176 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":38889; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestJobName.testComplexName(TestJobName.java:55)

testComplexNameWithRegex(org.apache.hadoop.mapred.TestJobName)  Time elapsed: 18.19 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":50869; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestJobName.testComplexNameWithRegex(TestJobName.java:89)

Running org.apache.hadoop.mapred.TestTaskPerformanceSplits
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.145 sec - in org.apache.hadoop.mapred.TestTaskPerformanceSplits
Running org.apache.hadoop.mapred.TestCombineFileInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.117 sec - in org.apache.hadoop.mapred.TestCombineFileInputFormat
Running org.apache.hadoop.mapred.TestMiniMRClasspath
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 49.069 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestMiniMRClasspath
testClassPath(org.apache.hadoop.mapred.TestMiniMRClasspath)  Time elapsed: 27.67 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":56422; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy79.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestMiniMRClasspath.launchWordCount(TestMiniMRClasspath.java:84)
	at org.apache.hadoop.mapred.TestMiniMRClasspath.testClassPath(TestMiniMRClasspath.java:178)

testExternalWritable(org.apache.hadoop.mapred.TestMiniMRClasspath)  Time elapsed: 21.236 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":55628; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy79.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestMiniMRClasspath.launchExternal(TestMiniMRClasspath.java:141)
	at org.apache.hadoop.mapred.TestMiniMRClasspath.testExternalWritable(TestMiniMRClasspath.java:211)

Running org.apache.hadoop.mapred.TestClusterMRNotification
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 17.713 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestClusterMRNotification
testMR(org.apache.hadoop.mapred.TestClusterMRNotification)  Time elapsed: 17.611 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":38609; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:742)
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy48.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy49.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.NotificationTestCase.launchWordCount(NotificationTestCase.java:241)
	at org.apache.hadoop.mapred.NotificationTestCase.testMR(NotificationTestCase.java:156)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: java.net.UnknownHostException: null
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy48.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy49.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.NotificationTestCase.launchWordCount(NotificationTestCase.java:241)
	at org.apache.hadoop.mapred.NotificationTestCase.testMR(NotificationTestCase.java:156)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

Running org.apache.hadoop.mapred.TestMultipleTextOutputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.183 sec - in org.apache.hadoop.mapred.TestMultipleTextOutputFormat
Running org.apache.hadoop.mapred.TestOldCombinerGrouping
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.433 sec - in org.apache.hadoop.mapred.TestOldCombinerGrouping
Running org.apache.hadoop.mapred.TestUserDefinedCounters
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.658 sec - in org.apache.hadoop.mapred.TestUserDefinedCounters
Running org.apache.hadoop.mapred.TestClientServiceDelegate
Tests run: 24, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.1 sec - in org.apache.hadoop.mapred.TestClientServiceDelegate
Running org.apache.hadoop.mapred.TestKeyValueTextInputFormat
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.808 sec - in org.apache.hadoop.mapred.TestKeyValueTextInputFormat
Running org.apache.hadoop.mapred.TestMultiFileSplit
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.216 sec - in org.apache.hadoop.mapred.TestMultiFileSplit
Running org.apache.hadoop.mapred.TestSequenceFileAsBinaryOutputFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.834 sec - in org.apache.hadoop.mapred.TestSequenceFileAsBinaryOutputFormat
Running org.apache.hadoop.mapred.TestNetworkedJob
Tests run: 5, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 53.48 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestNetworkedJob
testGetJobStatus(org.apache.hadoop.mapred.TestNetworkedJob)  Time elapsed: 19.442 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":50755; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy75.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy76.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.TestNetworkedJob.testGetJobStatus(TestNetworkedJob.java:102)

testNetworkedJob(org.apache.hadoop.mapred.TestNetworkedJob)  Time elapsed: 16.711 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":57657; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy75.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy76.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.TestNetworkedJob.testNetworkedJob(TestNetworkedJob.java:155)

testJobQueueClient(org.apache.hadoop.mapred.TestNetworkedJob)  Time elapsed: 16.708 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":33470; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy75.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy76.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.TestNetworkedJob.testJobQueueClient(TestNetworkedJob.java:340)

Running org.apache.hadoop.mapred.TestReduceFetchFromPartialMem
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 24.634 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestReduceFetchFromPartialMem
testReduceFromPartialMem(org.apache.hadoop.mapred.TestReduceFetchFromPartialMem)  Time elapsed: 0.249 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36453; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestReduceFetchFromPartialMem.runJob(TestReduceFetchFromPartialMem.java:300)
	at org.apache.hadoop.mapred.TestReduceFetchFromPartialMem.testReduceFromPartialMem(TestReduceFetchFromPartialMem.java:93)

Running org.apache.hadoop.mapred.TestMapRed
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 27.835 sec - in org.apache.hadoop.mapred.TestMapRed
Running org.apache.hadoop.mapred.TestSequenceFileInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.904 sec - in org.apache.hadoop.mapred.TestSequenceFileInputFormat
Running org.apache.hadoop.mapred.TestSequenceFileInputFilter
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.379 sec - in org.apache.hadoop.mapred.TestSequenceFileInputFilter
Running org.apache.hadoop.mapred.TestAuditLogger
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.301 sec - in org.apache.hadoop.mapred.TestAuditLogger
Running org.apache.hadoop.mapred.TestInputPath
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.034 sec - in org.apache.hadoop.mapred.TestInputPath
Running org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 48.77 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers
testDistinctUsers(org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers)  Time elapsed: 26.555 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":42601; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers$1.run(TestMiniMRWithDFSWithDistinctUsers.java:68)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers$1.run(TestMiniMRWithDFSWithDistinctUsers.java:66)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.runJobAsUser(TestMiniMRWithDFSWithDistinctUsers.java:66)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.testDistinctUsers(TestMiniMRWithDFSWithDistinctUsers.java:116)

testMultipleSpills(org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers)  Time elapsed: 21.316 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":49120; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers$1.run(TestMiniMRWithDFSWithDistinctUsers.java:68)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers$1.run(TestMiniMRWithDFSWithDistinctUsers.java:66)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.runJobAsUser(TestMiniMRWithDFSWithDistinctUsers.java:66)
	at org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.testMultipleSpills(TestMiniMRWithDFSWithDistinctUsers.java:148)

Running org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.665 sec - in org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath
Running org.apache.hadoop.mapred.TestMiniMRDFSCaching
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.068 sec - in org.apache.hadoop.mapred.TestMiniMRDFSCaching
Running org.apache.hadoop.mapred.TestYARNRunner
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.425 sec - in org.apache.hadoop.mapred.TestYARNRunner
Running org.apache.hadoop.mapred.TestLineRecordReaderJobs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.056 sec - in org.apache.hadoop.mapred.TestLineRecordReaderJobs
Running org.apache.hadoop.mapred.TestIFile
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.206 sec - in org.apache.hadoop.mapred.TestIFile
Running org.apache.hadoop.mapred.join.TestWrappedRecordReaderClassloader
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.041 sec - in org.apache.hadoop.mapred.join.TestWrappedRecordReaderClassloader
Running org.apache.hadoop.mapred.join.TestTupleWritable
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.148 sec - in org.apache.hadoop.mapred.join.TestTupleWritable
Running org.apache.hadoop.mapred.join.TestDatamerge
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.949 sec - in org.apache.hadoop.mapred.join.TestDatamerge
Running org.apache.hadoop.mapred.TestComparators
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.053 sec - in org.apache.hadoop.mapred.TestComparators
Running org.apache.hadoop.mapred.TestTextInputFormat
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 51.609 sec - in org.apache.hadoop.mapred.TestTextInputFormat
Running org.apache.hadoop.mapred.TestTaskStatus
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.346 sec - in org.apache.hadoop.mapred.TestTaskStatus
Running org.apache.hadoop.mapred.TestMiniMRClientCluster
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.057 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestMiniMRClientCluster
testJob(org.apache.hadoop.mapred.TestMiniMRClientCluster)  Time elapsed: 0.204 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36276; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapred.TestMiniMRClientCluster.testJob(TestMiniMRClientCluster.java:162)

Running org.apache.hadoop.mapred.TestUtils
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.151 sec - in org.apache.hadoop.mapred.TestUtils
Running org.apache.hadoop.mapred.TestFieldSelection
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.617 sec - in org.apache.hadoop.mapred.TestFieldSelection
Running org.apache.hadoop.mapred.TestConcatenatedCompressedInput
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.085 sec - in org.apache.hadoop.mapred.TestConcatenatedCompressedInput
Running org.apache.hadoop.mapred.pipes.TestPipesNonJavaInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.267 sec - in org.apache.hadoop.mapred.pipes.TestPipesNonJavaInputFormat
Running org.apache.hadoop.mapred.pipes.TestPipeApplication
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.508 sec - in org.apache.hadoop.mapred.pipes.TestPipeApplication
Running org.apache.hadoop.mapred.pipes.TestPipes
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.065 sec - in org.apache.hadoop.mapred.pipes.TestPipes
Running org.apache.hadoop.mapred.TestMRCJCFileOutputCommitter
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.257 sec - in org.apache.hadoop.mapred.TestMRCJCFileOutputCommitter
Running org.apache.hadoop.mapred.TestReduceFetch
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 24.879 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestReduceFetch
testReduceFromPartialMem(org.apache.hadoop.mapred.TestReduceFetchFromPartialMem)  Time elapsed: 0.341 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":56727; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestReduceFetchFromPartialMem.runJob(TestReduceFetchFromPartialMem.java:300)
	at org.apache.hadoop.mapred.TestReduceFetchFromPartialMem.testReduceFromPartialMem(TestReduceFetchFromPartialMem.java:93)

Running org.apache.hadoop.mapred.TestReduceTask
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.093 sec - in org.apache.hadoop.mapred.TestReduceTask
Running org.apache.hadoop.mapred.TestReporter
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.241 sec - in org.apache.hadoop.mapred.TestReporter
Running org.apache.hadoop.mapred.TestFileOutputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.406 sec - in org.apache.hadoop.mapred.TestFileOutputFormat
Running org.apache.hadoop.mapred.TestFileInputFormatPathFilter
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.629 sec - in org.apache.hadoop.mapred.TestFileInputFormatPathFilter
Running org.apache.hadoop.mapred.TestCombineOutputCollector
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.218 sec - in org.apache.hadoop.mapred.TestCombineOutputCollector
Running org.apache.hadoop.mapred.TestResourceMgrDelegate
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.114 sec - in org.apache.hadoop.mapred.TestResourceMgrDelegate
Running org.apache.hadoop.mapred.TestFixedLengthInputFormat
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 96.503 sec - in org.apache.hadoop.mapred.TestFixedLengthInputFormat
Running org.apache.hadoop.mapred.TestGetSplitHosts
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.265 sec - in org.apache.hadoop.mapred.TestGetSplitHosts
Running org.apache.hadoop.mapred.TestJobCounters
Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 34.579 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestJobCounters
testHeapUsageCounter(org.apache.hadoop.mapred.TestJobCounters)  Time elapsed: 16.55 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":54730; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.TestJobCounters.runHeapUsageTestJob(TestJobCounters.java:629)
	at org.apache.hadoop.mapred.TestJobCounters.testHeapUsageCounter(TestJobCounters.java:678)

Running org.apache.hadoop.mapred.TestJobCleanup
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 19.172 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestJobCleanup
testDefaultCleanupAndAbort(org.apache.hadoop.mapred.TestJobCleanup)  Time elapsed: 0.321 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":53016; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.TestJobCleanup.testSuccessfulJob(TestJobCleanup.java:165)
	at org.apache.hadoop.mapred.TestJobCleanup.testDefaultCleanupAndAbort(TestJobCleanup.java:268)

testCustomAbort(org.apache.hadoop.mapred.TestJobCleanup)  Time elapsed: 0.106 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":53016; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.TestJobCleanup.testSuccessfulJob(TestJobCleanup.java:165)
	at org.apache.hadoop.mapred.TestJobCleanup.testCustomAbort(TestJobCleanup.java:288)

testCustomCleanup(org.apache.hadoop.mapred.TestJobCleanup)  Time elapsed: 0.105 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":53016; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.TestJobCleanup.testSuccessfulJob(TestJobCleanup.java:165)
	at org.apache.hadoop.mapred.TestJobCleanup.testCustomCleanup(TestJobCleanup.java:311)

Running org.apache.hadoop.mapred.TestClientRedirect
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.427 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestClientRedirect
testRedirect(org.apache.hadoop.mapred.TestClientRedirect)  Time elapsed: 0.86 sec  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:10020] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
	at org.apache.hadoop.mapred.TestClientRedirect$AMService.start(TestClientRedirect.java:453)
	at org.apache.hadoop.mapred.TestClientRedirect.testRedirect(TestClientRedirect.java:152)

Running org.apache.hadoop.mapred.TestJavaSerialization
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.981 sec - in org.apache.hadoop.mapred.TestJavaSerialization
Running org.apache.hadoop.mapred.TestSequenceFileAsTextInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.641 sec - in org.apache.hadoop.mapred.TestSequenceFileAsTextInputFormat
Running org.apache.hadoop.mapred.TestQueueConfigurationParser
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.333 sec - in org.apache.hadoop.mapred.TestQueueConfigurationParser
Running org.apache.hadoop.mapred.TestMultipleLevelCaching
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.081 sec - in org.apache.hadoop.mapred.TestMultipleLevelCaching
Running org.apache.hadoop.mapred.TestSortedRanges
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.225 sec - in org.apache.hadoop.mapred.TestSortedRanges
Running org.apache.hadoop.mapred.TestSequenceFileAsBinaryInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.794 sec - in org.apache.hadoop.mapred.TestSequenceFileAsBinaryInputFormat
Running org.apache.hadoop.mapred.jobcontrol.TestJobControl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.294 sec - in org.apache.hadoop.mapred.jobcontrol.TestJobControl
Running org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.609 sec - in org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl
Running org.apache.hadoop.mapred.TestMapProgress
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.461 sec - in org.apache.hadoop.mapred.TestMapProgress
Running org.apache.hadoop.mapred.TestLocalMRNotification
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.038 sec - in org.apache.hadoop.mapred.TestLocalMRNotification
Running org.apache.hadoop.mapred.TestMerge
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 25.651 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestMerge
testMerge(org.apache.hadoop.mapred.TestMerge)  Time elapsed: 25.54 sec  <<< ERROR!
java.io.FileNotFoundException: File /testplugin/output does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:654)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)
	at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:708)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:708)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1483)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1523)
	at org.apache.hadoop.mapred.TestMerge.verifyOutput(TestMerge.java:157)
	at org.apache.hadoop.mapred.TestMerge.runMergeTest(TestMerge.java:146)
	at org.apache.hadoop.mapred.TestMerge.testMerge(TestMerge.java:87)

Running org.apache.hadoop.mapred.TestMapOutputType
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.597 sec - in org.apache.hadoop.mapred.TestMapOutputType
Running org.apache.hadoop.mapred.TestTextOutputFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.25 sec - in org.apache.hadoop.mapred.TestTextOutputFormat
Running org.apache.hadoop.mapred.TestIFileStreams
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.513 sec - in org.apache.hadoop.mapred.TestIFileStreams
Running org.apache.hadoop.mapred.TestBadRecords
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.064 sec - in org.apache.hadoop.mapred.TestBadRecords
Running org.apache.hadoop.mapred.TestJobSysDirWithDFS
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 27.062 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestJobSysDirWithDFS
testWithDFS(org.apache.hadoop.mapred.TestJobSysDirWithDFS)  Time elapsed: 26.794 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":59858; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestJobSysDirWithDFS.launchWordCount(TestJobSysDirWithDFS.java:90)
	at org.apache.hadoop.mapred.TestJobSysDirWithDFS.runWordCount(TestJobSysDirWithDFS.java:108)
	at org.apache.hadoop.mapred.TestJobSysDirWithDFS.testWithDFS(TestJobSysDirWithDFS.java:132)

Running org.apache.hadoop.mapred.TestMRCJCFileInputFormat
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.584 sec - in org.apache.hadoop.mapred.TestMRCJCFileInputFormat
Running org.apache.hadoop.mapred.TestCombineSequenceFileInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.33 sec - in org.apache.hadoop.mapred.TestCombineSequenceFileInputFormat
Running org.apache.hadoop.mapred.TestLazyOutput
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 26.561 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestLazyOutput
testLazyOutput(org.apache.hadoop.mapred.TestLazyOutput)  Time elapsed: 26.462 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":57896; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
	at org.apache.hadoop.mapred.TestLazyOutput.runTestLazyOutput(TestLazyOutput.java:120)
	at org.apache.hadoop.mapred.TestLazyOutput.testLazyOutput(TestLazyOutput.java:156)

Running org.apache.hadoop.mapred.TestWritableJobConf
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.178 sec - in org.apache.hadoop.mapred.TestWritableJobConf
Running org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.682 sec - in org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner
Running org.apache.hadoop.mapred.lib.TestKeyFieldBasedPartitioner
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.696 sec - in org.apache.hadoop.mapred.lib.TestKeyFieldBasedPartitioner
Running org.apache.hadoop.mapred.lib.TestDelegatingInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.937 sec - in org.apache.hadoop.mapred.lib.TestDelegatingInputFormat
Running org.apache.hadoop.mapred.lib.db.TestConstructQuery
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.75 sec - in org.apache.hadoop.mapred.lib.db.TestConstructQuery
Running org.apache.hadoop.mapred.lib.aggregate.TestAggregates
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.819 sec - in org.apache.hadoop.mapred.lib.aggregate.TestAggregates
Running org.apache.hadoop.mapred.lib.TestLineInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.33 sec - in org.apache.hadoop.mapred.lib.TestLineInputFormat
Running org.apache.hadoop.mapred.lib.TestMultipleInputs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.735 sec - in org.apache.hadoop.mapred.lib.TestMultipleInputs
Running org.apache.hadoop.mapred.lib.TestChainMapReduce
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.017 sec - in org.apache.hadoop.mapred.lib.TestChainMapReduce
Running org.apache.hadoop.mapred.lib.TestChain
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.957 sec - in org.apache.hadoop.mapred.lib.TestChain
Running org.apache.hadoop.mapred.lib.TestMultipleOutputs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.467 sec - in org.apache.hadoop.mapred.lib.TestMultipleOutputs
Running org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.515 sec - in org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator
Running org.apache.hadoop.mapred.TestCombineTextInputFormat
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.742 sec - in org.apache.hadoop.mapred.TestCombineTextInputFormat
Running org.apache.hadoop.mapred.TestStatisticsCollector
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.585 sec - in org.apache.hadoop.mapred.TestStatisticsCollector
Running org.apache.hadoop.mapred.TestMRCJCJobConf
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.085 sec - in org.apache.hadoop.mapred.TestMRCJCJobConf
Running org.apache.hadoop.mapred.TestTaskCommit
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.161 sec - in org.apache.hadoop.mapred.TestTaskCommit
Running org.apache.hadoop.mapred.TestCommandLineJobSubmission
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.076 sec - in org.apache.hadoop.mapred.TestCommandLineJobSubmission
Running org.apache.hadoop.mapred.TestCollect
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.446 sec - in org.apache.hadoop.mapred.TestCollect
Running org.apache.hadoop.mapred.TestMiniMRBringup
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.307 sec - in org.apache.hadoop.mapred.TestMiniMRBringup
Running org.apache.hadoop.fs.TestJHLA
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.299 sec - in org.apache.hadoop.fs.TestJHLA
Running org.apache.hadoop.fs.TestDFSIO
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.776 sec - in org.apache.hadoop.fs.TestDFSIO
Running org.apache.hadoop.fs.TestFileSystem
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.416 sec - in org.apache.hadoop.fs.TestFileSystem
Running org.apache.hadoop.fs.slive.TestSlive
Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.069 sec - in org.apache.hadoop.fs.slive.TestSlive
Running org.apache.hadoop.ipc.TestMRCJCSocketFactory
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.997 sec <<< FAILURE! - in org.apache.hadoop.ipc.TestMRCJCSocketFactory
testSocketFactory(org.apache.hadoop.ipc.TestMRCJCSocketFactory)  Time elapsed: 21.742 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":42640; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy78.getApplications(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy79.getApplications(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:344)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getAllJobs(ResourceMgrDelegate.java:126)
	at org.apache.hadoop.mapred.YARNRunner.getAllJobs(YARNRunner.java:166)
	at org.apache.hadoop.mapreduce.Cluster.getAllJobStatuses(Cluster.java:293)
	at org.apache.hadoop.mapred.JobClient$5.run(JobClient.java:811)
	at org.apache.hadoop.mapred.JobClient$5.run(JobClient.java:808)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:806)
	at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:790)
	at org.apache.hadoop.ipc.TestMRCJCSocketFactory.testSocketFactory(TestMRCJCSocketFactory.java:96)

Running org.apache.hadoop.mapreduce.TestMapReduceLazyOutput
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 26.194 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.TestMapReduceLazyOutput
testLazyOutput(org.apache.hadoop.mapreduce.TestMapReduceLazyOutput)  Time elapsed: 26.078 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":57852; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.TestMapReduceLazyOutput.runTestLazyOutput(TestMapReduceLazyOutput.java:110)
	at org.apache.hadoop.mapreduce.TestMapReduceLazyOutput.testLazyOutput(TestMapReduceLazyOutput.java:146)

Running org.apache.hadoop.mapreduce.TestChild
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 20.098 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.TestChild
testChild(org.apache.hadoop.mapreduce.TestChild)  Time elapsed: 20.005 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":54694; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy74.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy75.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.TestChild.submitAndValidateJob(TestChild.java:133)
	at org.apache.hadoop.mapreduce.TestChild.testChild(TestChild.java:151)

Running org.apache.hadoop.mapreduce.TestNewCombinerGrouping
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.476 sec - in org.apache.hadoop.mapreduce.TestNewCombinerGrouping
Running org.apache.hadoop.mapreduce.TestMRJobClient
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 40.991 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.TestMRJobClient
testJobClient(org.apache.hadoop.mapreduce.TestMRJobClient)  Time elapsed: 17.532 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":35925; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.TestMRJobClient.runJob(TestMRJobClient.java:61)
	at org.apache.hadoop.mapreduce.TestMRJobClient.testJobClient(TestMRJobClient.java:126)

Running org.apache.hadoop.mapreduce.TestNoJobSetupCleanup
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.089 sec - in org.apache.hadoop.mapreduce.TestNoJobSetupCleanup
Running org.apache.hadoop.mapreduce.TestCounters
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.866 sec - in org.apache.hadoop.mapreduce.TestCounters
Running org.apache.hadoop.mapreduce.TestLocalRunner
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.452 sec - in org.apache.hadoop.mapreduce.TestLocalRunner
Running org.apache.hadoop.mapreduce.TestLargeSort
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.943 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.TestLargeSort
testLargeSort(org.apache.hadoop.mapreduce.TestLargeSort)  Time elapsed: 19.745 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":37829; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.LargeSorter.run(LargeSorter.java:251)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.mapreduce.TestLargeSort.testLargeSort(TestLargeSort.java:61)

Running org.apache.hadoop.mapreduce.filecache.TestURIFragments
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.159 sec - in org.apache.hadoop.mapreduce.filecache.TestURIFragments
Running org.apache.hadoop.mapreduce.TestValueIterReset
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.602 sec - in org.apache.hadoop.mapreduce.TestValueIterReset
Running org.apache.hadoop.mapreduce.TestYarnClientProtocolProvider
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.807 sec - in org.apache.hadoop.mapreduce.TestYarnClientProtocolProvider
Running org.apache.hadoop.mapreduce.TestClientProtocolProviderImpls
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.213 sec - in org.apache.hadoop.mapreduce.TestClientProtocolProviderImpls
Running org.apache.hadoop.mapreduce.TestMapperReducerCleanup
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.641 sec - in org.apache.hadoop.mapreduce.TestMapperReducerCleanup
Running org.apache.hadoop.mapreduce.TestMapCollection
Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.003 sec - in org.apache.hadoop.mapreduce.TestMapCollection
Running org.apache.hadoop.mapreduce.v2.TestMRJobs
Tests run: 6, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 25.843 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMRJobs
testSleepJob(org.apache.hadoop.mapreduce.v2.TestMRJobs)  Time elapsed: 0.172 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36108; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testSleepJob(TestMRJobs.java:194)

testJobClassloader(org.apache.hadoop.mapreduce.v2.TestMRJobs)  Time elapsed: 0.078 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36108; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testJobClassloader(TestMRJobs.java:236)

testRandomWriter(org.apache.hadoop.mapreduce.v2.TestMRJobs)  Time elapsed: 0.051 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36108; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testRandomWriter(TestMRJobs.java:292)

testFailingMapper(org.apache.hadoop.mapreduce.v2.TestMRJobs)  Time elapsed: 0.052 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36108; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.runFailingMapperJob(TestMRJobs.java:404)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testFailingMapper(TestMRJobs.java:345)

testContainerRollingLog(org.apache.hadoop.mapreduce.v2.TestMRJobs)  Time elapsed: 0.05 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36108; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testContainerRollingLog(TestMRJobs.java:492)

testDistributedCache(org.apache.hadoop.mapreduce.v2.TestMRJobs)  Time elapsed: 0.767 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":36108; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs._testDistributedCache(TestMRJobs.java:742)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testDistributedCache(TestMRJobs.java:755)

Running org.apache.hadoop.mapreduce.v2.TestUberAM
Tests run: 7, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 24.716 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestUberAM
testSleepJob(org.apache.hadoop.mapreduce.v2.TestUberAM)  Time elapsed: 0.208 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":33070; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testSleepJob(TestMRJobs.java:194)
	at org.apache.hadoop.mapreduce.v2.TestUberAM.testSleepJob(TestUberAM.java:62)

testSleepJobWithMultipleReducers(org.apache.hadoop.mapreduce.v2.TestUberAM)  Time elapsed: 0.056 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":33070; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testSleepJob(TestMRJobs.java:194)
	at org.apache.hadoop.mapreduce.v2.TestUberAM.testSleepJobWithMultipleReducers(TestUberAM.java:72)

testRandomWriter(org.apache.hadoop.mapreduce.v2.TestUberAM)  Time elapsed: 0.169 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":33070; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testRandomWriter(TestMRJobs.java:292)
	at org.apache.hadoop.mapreduce.v2.TestUberAM.testRandomWriter(TestUberAM.java:105)

testFailingMapper(org.apache.hadoop.mapreduce.v2.TestUberAM)  Time elapsed: 0.061 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":33070; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.runFailingMapperJob(TestMRJobs.java:404)
	at org.apache.hadoop.mapreduce.v2.TestUberAM.testFailingMapper(TestUberAM.java:131)

testJobClassloader(org.apache.hadoop.mapreduce.v2.TestUberAM)  Time elapsed: 0.068 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":33070; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:742)
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testJobClassloader(TestMRJobs.java:236)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
Caused by: java.net.UnknownHostException: null
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testJobClassloader(TestMRJobs.java:236)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)

testContainerRollingLog(org.apache.hadoop.mapreduce.v2.TestUberAM)  Time elapsed: 0.06 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":33070; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:86)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:58)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:542)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:742)
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testContainerRollingLog(TestMRJobs.java:492)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
Caused by: java.net.UnknownHostException: null
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy80.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy81.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testContainerRollingLog(TestMRJobs.java:492)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)

Running org.apache.hadoop.mapreduce.v2.TestRMNMInfo
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.121 sec - in org.apache.hadoop.mapreduce.v2.TestRMNMInfo
Running org.apache.hadoop.mapreduce.v2.TestNonExistentJob
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 25.535 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestNonExistentJob
testGetInvalidJob(org.apache.hadoop.mapreduce.v2.TestNonExistentJob)  Time elapsed: 25.448 sec  <<< FAILURE!
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:48)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.hadoop.mapreduce.v2.TestNonExistentJob.testGetInvalidJob(TestNonExistentJob.java:98)

Running org.apache.hadoop.mapreduce.v2.TestMRAppWithCombiner
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.246 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMRAppWithCombiner
testCombinerShouldUpdateTheReporter(org.apache.hadoop.mapreduce.v2.TestMRAppWithCombiner)  Time elapsed: 0.362 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":60101; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapreduce.v2.TestMRAppWithCombiner.runJob(TestMRAppWithCombiner.java:147)
	at org.apache.hadoop.mapreduce.v2.TestMRAppWithCombiner.testCombinerShouldUpdateTheReporter(TestMRAppWithCombiner.java:124)

Running org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 24.714 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser
testValidProxyUser(org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser)  Time elapsed: 24.629 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":48384; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser.mrRun(TestMiniMRProxyUser.java:123)
	at org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser.access$000(TestMiniMRProxyUser.java:41)

Running org.apache.hadoop.mapreduce.v2.TestMROldApiJobs
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 18.258 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMROldApiJobs
testJobSucceed(org.apache.hadoop.mapreduce.v2.TestMROldApiJobs)  Time elapsed: 0.302 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":49044; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapreduce.v2.TestMROldApiJobs.runJob(TestMROldApiJobs.java:212)
	at org.apache.hadoop.mapreduce.v2.TestMROldApiJobs.runJobSucceed(TestMROldApiJobs.java:177)
	at org.apache.hadoop.mapreduce.v2.TestMROldApiJobs.testJobSucceed(TestMROldApiJobs.java:116)

testJobFail(org.apache.hadoop.mapreduce.v2.TestMROldApiJobs)  Time elapsed: 0.142 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":49044; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
	at org.apache.hadoop.mapreduce.v2.TestMROldApiJobs.runJob(TestMROldApiJobs.java:212)
	at org.apache.hadoop.mapreduce.v2.TestMROldApiJobs.runJobFail(TestMROldApiJobs.java:165)
	at org.apache.hadoop.mapreduce.v2.TestMROldApiJobs.testJobFail(TestMROldApiJobs.java:145)

Running org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.085 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution
testSpeculativeExecution(org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution)  Time elapsed: 0.297 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":39653; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution.runSpecTest(TestSpeculativeExecution.java:306)
	at org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution.testSpeculativeExecution(TestSpeculativeExecution.java:207)

Running org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 44.595 sec - in org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp
Running org.apache.hadoop.mapreduce.v2.TestMRAMWithNonNormalizedCapabilities
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 18.055 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMRAMWithNonNormalizedCapabilities
testJobWithNonNormalizedCapabilities(org.apache.hadoop.mapreduce.v2.TestMRAMWithNonNormalizedCapabilities)  Time elapsed: 17.135 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":55993; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.v2.TestMRAMWithNonNormalizedCapabilities.testJobWithNonNormalizedCapabilities(TestMRAMWithNonNormalizedCapabilities.java:102)

Running org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 17.981 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler
testProfiler(org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler)  Time elapsed: 16.917 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":51022; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler.testProfiler(TestMRJobsWithProfiler.java:138)

Running org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 18.032 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService
testJobHistoryData(org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService)  Time elapsed: 17.116 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":43306; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy77.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy78.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService.testJobHistoryData(TestMRJobsWithHistoryService.java:131)

Running org.apache.hadoop.mapreduce.util.TestMRAsyncDiskService
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.299 sec - in org.apache.hadoop.mapreduce.util.TestMRAsyncDiskService
Running org.apache.hadoop.mapreduce.TestTaskContext
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.07 sec - in org.apache.hadoop.mapreduce.TestTaskContext
Running org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.493 sec - in org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection
Running org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.882 sec - in org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsTextInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.408 sec - in org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsTextInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestDelegatingInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.509 sec - in org.apache.hadoop.mapreduce.lib.input.TestDelegatingInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsBinaryInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.754 sec - in org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsBinaryInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestLineRecordReaderJobs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.115 sec - in org.apache.hadoop.mapreduce.lib.input.TestLineRecordReaderJobs
Running org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.364 sec - in org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter
Running org.apache.hadoop.mapreduce.lib.input.TestFixedLengthInputFormat
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 83.82 sec - in org.apache.hadoop.mapreduce.lib.input.TestFixedLengthInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestNLineInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.074 sec - in org.apache.hadoop.mapreduce.lib.input.TestNLineInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.086 sec - in org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs
Running org.apache.hadoop.mapreduce.lib.input.TestMRKeyValueTextInputFormat
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 52.454 sec - in org.apache.hadoop.mapreduce.lib.input.TestMRKeyValueTextInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestMRCJCFileInputFormat
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.95 sec - in org.apache.hadoop.mapreduce.lib.input.TestMRCJCFileInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestCombineSequenceFileInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.529 sec - in org.apache.hadoop.mapreduce.lib.input.TestCombineSequenceFileInputFormat
Running org.apache.hadoop.mapreduce.lib.input.TestCombineTextInputFormat
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.768 sec - in org.apache.hadoop.mapreduce.lib.input.TestCombineTextInputFormat
Running org.apache.hadoop.mapreduce.lib.db.TestIntegerSplitter
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.074 sec - in org.apache.hadoop.mapreduce.lib.db.TestIntegerSplitter
Running org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.757 sec - in org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat
Running org.apache.hadoop.mapreduce.lib.db.TestDBOutputFormat
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.886 sec - in org.apache.hadoop.mapreduce.lib.db.TestDBOutputFormat
Running org.apache.hadoop.mapreduce.lib.db.TestTextSplitter
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.271 sec - in org.apache.hadoop.mapreduce.lib.db.TestTextSplitter
Running org.apache.hadoop.mapreduce.lib.aggregate.TestMapReduceAggregates
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.414 sec - in org.apache.hadoop.mapreduce.lib.aggregate.TestMapReduceAggregates
Running org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.204 sec - in org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper
Running org.apache.hadoop.mapreduce.lib.join.TestJoinProperties
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.496 sec - in org.apache.hadoop.mapreduce.lib.join.TestJoinProperties
Running org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.996 sec - in org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge
Running org.apache.hadoop.mapreduce.lib.join.TestWrappedRRClassloader
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.961 sec - in org.apache.hadoop.mapreduce.lib.join.TestWrappedRRClassloader
Running org.apache.hadoop.mapreduce.lib.join.TestJoinTupleWritable
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.124 sec - in org.apache.hadoop.mapreduce.lib.join.TestJoinTupleWritable
Running org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.559 sec - in org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter
Running org.apache.hadoop.mapreduce.lib.output.TestMRSequenceFileAsBinaryOutputFormat
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.733 sec - in org.apache.hadoop.mapreduce.lib.output.TestMRSequenceFileAsBinaryOutputFormat
Running org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.033 sec - in org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs
Running org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 49.194 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter
testDefaultCleanupAndAbort(org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter)  Time elapsed: 18.451 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":49091; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy74.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy75.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testSuccessfulJob(TestJobOutputCommitter.java:149)
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testDefaultCleanupAndAbort(TestJobOutputCommitter.java:224)

testCustomAbort(org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter)  Time elapsed: 15.302 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":42708; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy74.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy75.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testSuccessfulJob(TestJobOutputCommitter.java:149)
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testCustomAbort(TestJobOutputCommitter.java:243)

testCustomCleanup(org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter)  Time elapsed: 15.354 sec  <<< ERROR!
java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: "sovmp172.lop.siteox.com":43816; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1452)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy74.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:193)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
	at java.lang.reflect.Method.invoke(Method.java:620)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy75.getNewApplication(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:165)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:173)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:179)
	at org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:230)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(AccessController.java:369)
	at javax.security.auth.Subject.doAs(Subject.java:572)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testSuccessfulJob(TestJobOutputCommitter.java:149)
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testCustomCleanup(TestJobOutputCommitter.java:269)

Running org.apache.hadoop.mapreduce.lib.partition.TestBinaryPartitioner
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.707 sec - in org.apache.hadoop.mapreduce.lib.partition.TestBinaryPartitioner
Running org.apache.hadoop.mapreduce.lib.partition.TestTotalOrderPartitioner
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.5 sec - in org.apache.hadoop.mapreduce.lib.partition.TestTotalOrderPartitioner
Running org.apache.hadoop.mapreduce.lib.partition.TestInputSampler
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.119 sec - in org.apache.hadoop.mapreduce.lib.partition.TestInputSampler
Running org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedPartitioner
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.69 sec - in org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedPartitioner
Running org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedComparator
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.752 sec - in org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedComparator
Running org.apache.hadoop.mapreduce.lib.partition.TestKeyFieldHelper
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.248 sec - in org.apache.hadoop.mapreduce.lib.partition.TestKeyFieldHelper
Running org.apache.hadoop.mapreduce.lib.jobcontrol.TestControlledJob
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.063 sec - in org.apache.hadoop.mapreduce.lib.jobcontrol.TestControlledJob
Running org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControlWithMocks
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.397 sec - in org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControlWithMocks
Running org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 47.244 sec - in org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl
Running org.apache.hadoop.mapreduce.lib.chain.TestSingleElementChain
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.774 sec - in org.apache.hadoop.mapreduce.lib.chain.TestSingleElementChain
Running org.apache.hadoop.mapreduce.lib.chain.TestMapReduceChain
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.563 sec - in org.apache.hadoop.mapreduce.lib.chain.TestMapReduceChain
Running org.apache.hadoop.mapreduce.lib.chain.TestChainErrors
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.265 sec - in org.apache.hadoop.mapreduce.lib.chain.TestChainErrors
Running org.apache.hadoop.mapreduce.security.TestMRCredentials
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.894 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.security.TestMRCredentials
test(org.apache.hadoop.mapreduce.security.TestMRCredentials)  Time elapsed: 0.17 sec  <<< FAILURE!
java.lang.AssertionError: Job failed
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.mapreduce.security.TestMRCredentials.test(TestMRCredentials.java:133)

Running org.apache.hadoop.mapreduce.security.TestUmbilicalProtocolWithJobToken
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.458 sec - in org.apache.hadoop.mapreduce.security.TestUmbilicalProtocolWithJobToken
Running org.apache.hadoop.mapreduce.security.token.delegation.TestDelegationToken
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.065 sec - in org.apache.hadoop.mapreduce.security.token.delegation.TestDelegationToken
Running org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 0.335 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle
encryptedShuffleWithClientCerts(org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle)  Time elapsed: 0.011 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle.createCustomYarnClasspath(TestEncryptedShuffle.java:69)

encryptedShuffleWithClientCerts(org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle)  Time elapsed: 0.012 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle.cleanUpMiniClusterSpecialConfig(TestEncryptedShuffle.java:77)

encryptedShuffleWithoutClientCerts(org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle)  Time elapsed: 0.002 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle.createCustomYarnClasspath(TestEncryptedShuffle.java:69)

encryptedShuffleWithoutClientCerts(org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle)  Time elapsed: 0.004 sec  <<< ERROR!
java.lang.NoClassDefFoundError: sun.security.x509.X509CertImpl
	at java.net.URLClassLoader.findClass(URLClassLoader.java:666)
	at java.lang.ClassLoader.loadClassHelper(ClassLoader.java:942)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:869)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:336)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:847)
	at org.apache.hadoop.mapreduce.security.ssl.TestEncryptedShuffle.cleanUpMiniClusterSpecialConfig(TestEncryptedShuffle.java:77)

Running org.apache.hadoop.mapreduce.security.TestBinaryTokenFile
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.136 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.security.TestBinaryTokenFile
testBinaryTokenFile(org.apache.hadoop.mapreduce.security.TestBinaryTokenFile)  Time elapsed: 0.254 sec  <<< FAILURE!
java.lang.AssertionError: Job failed
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.mapreduce.security.TestBinaryTokenFile.testBinaryTokenFile(TestBinaryTokenFile.java:251)

Running org.apache.hadoop.mapreduce.security.TestJHSSecurity
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.325 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.security.TestJHSSecurity
testDelegationToken(org.apache.hadoop.mapreduce.security.TestJHSSecurity)  Time elapsed: 12.006 sec  <<< ERROR!
org.apache.hadoop.service.ServiceStateException: java.net.BindException: Problem binding to [0.0.0.0:10033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:719)
	at org.apache.hadoop.ipc.Server.bind(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:561)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2166)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer.serviceInit(HSAdminServer.java:100)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:142)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.mapreduce.security.TestJHSSecurity.testDelegationToken(TestJHSSecurity.java:111)

Running org.apache.hadoop.mapreduce.TestMapReduce
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.388 sec - in org.apache.hadoop.mapreduce.TestMapReduce
Running org.apache.hadoop.mapreduce.TestMROutputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.488 sec - in org.apache.hadoop.mapreduce.TestMROutputFormat
Running org.apache.hadoop.io.TestSequenceFileMergeProgress
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.1 sec - in org.apache.hadoop.io.TestSequenceFileMergeProgress
Running org.apache.hadoop.util.TestMRCJCRunJar
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.726 sec - in org.apache.hadoop.util.TestMRCJCRunJar
Running org.apache.hadoop.util.TestMRCJCReflectionUtils
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.612 sec - in org.apache.hadoop.util.TestMRCJCReflectionUtils

Results :

Failed tests: 
  TestMiniMRChildTask.testTaskTempDir:393 Exception in testing temp dir
  TestMiniMRChildTask.testMapRedExecutionEnv:453 Exception in testing propagation of env setting to child task
  TestMiniMRChildTask.testTaskEnv:476 Exception in testing child env
  TestMiniMRChildTask.testTaskOldEnv:499 Exception in testing child env
  TestNonExistentJob.testGetInvalidJob:98 null
  TestMRCredentials.test:133 Job failed
  TestBinaryTokenFile.testBinaryTokenFile:251 Job failed

Tests in error: 
  TestClusterMapReduceTestCase.testMapReduce:89->_testMapReduce:67 » UnknownHost
  TestClusterMapReduceTestCase.testMapReduceRestarting:93->_testMapReduce:67 » UnknownHost
  TestJobName.testComplexName:55 » UnknownHost Invalid host name: local host is:...
  TestJobName.testComplexNameWithRegex:89 » UnknownHost Invalid host name: local...
  TestMiniMRClasspath.testClassPath:178->launchWordCount:84 » UnknownHost Invali...
  TestMiniMRClasspath.testExternalWritable:211->launchExternal:141 » UnknownHost
  TestClusterMRNotification>NotificationTestCase.testMR:156->NotificationTestCase.launchWordCount:241 » UnknownHost
  TestNetworkedJob.testGetJobStatus:102 » UnknownHost Invalid host name: local h...
  TestNetworkedJob.testNetworkedJob:155 » UnknownHost Invalid host name: local h...
  TestNetworkedJob.testJobQueueClient:340 » UnknownHost Invalid host name: local...
  TestReduceFetchFromPartialMem.testReduceFromPartialMem:93->runJob:300 » UnknownHost
  TestMiniMRWithDFSWithDistinctUsers.testDistinctUsers:116->runJobAsUser:66 » UnknownHost
  TestMiniMRWithDFSWithDistinctUsers.testMultipleSpills:148->runJobAsUser:66 » UnknownHost
  TestMiniMRClientCluster.testJob:162 » UnknownHost Invalid host name: local hos...
  TestReduceFetchFromPartialMem.testReduceFromPartialMem:93->runJob:300 » UnknownHost
  TestJobCounters.testHeapUsageCounter:678->runHeapUsageTestJob:629 » UnknownHost
  TestJobCleanup.testDefaultCleanupAndAbort:268->testSuccessfulJob:165 » UnknownHost
  TestJobCleanup.testCustomAbort:288->testSuccessfulJob:165 » UnknownHost Invali...
  TestJobCleanup.testCustomCleanup:311->testSuccessfulJob:165 » UnknownHost Inva...
  TestClientRedirect.testRedirect:152 » YarnRuntime java.net.BindException: Prob...
  TestMerge.testMerge:87->runMergeTest:146->verifyOutput:157 » FileNotFound File...
  TestJobSysDirWithDFS.testWithDFS:132->runWordCount:108->launchWordCount:90 » UnknownHost
  TestLazyOutput.testLazyOutput:156->runTestLazyOutput:120 » UnknownHost Invalid...
  TestMRCJCSocketFactory.testSocketFactory:96 » UnknownHost Invalid host name: l...
  TestMapReduceLazyOutput.testLazyOutput:146->runTestLazyOutput:110 » UnknownHost
  TestChild.testChild:151->submitAndValidateJob:133 » UnknownHost Invalid host n...
  TestMRJobClient.testJobClient:126->runJob:61 » UnknownHost Invalid host name: ...
  TestLargeSort.testLargeSort:61 » UnknownHost Invalid host name: local host is:...
  TestMRJobs.testSleepJob:194 » UnknownHost Invalid host name: local host is: (u...
  TestMRJobs.testJobClassloader:236 » UnknownHost Invalid host name: local host ...
  TestMRJobs.testRandomWriter:292 » UnknownHost Invalid host name: local host is...
  TestMRJobs.testFailingMapper:345->runFailingMapperJob:404 » UnknownHost Invali...
  TestMRJobs.testContainerRollingLog:492 » UnknownHost Invalid host name: local ...
  TestMRJobs.testDistributedCache:755->_testDistributedCache:742 » UnknownHost I...
  TestUberAM.testSleepJob:62->TestMRJobs.testSleepJob:194 » UnknownHost Invalid ...
  TestUberAM.testSleepJobWithMultipleReducers:72->TestMRJobs.testSleepJob:194 » UnknownHost
  TestUberAM.testRandomWriter:105->TestMRJobs.testRandomWriter:292 » UnknownHost
  TestUberAM.testFailingMapper:131->TestMRJobs.runFailingMapperJob:404 » UnknownHost
  TestUberAM>TestMRJobs.testJobClassloader:236 » UnknownHost Invalid host name: ...
  TestUberAM>TestMRJobs.testContainerRollingLog:492 » UnknownHost Invalid host n...
  TestMRAppWithCombiner.testCombinerShouldUpdateTheReporter:124->runJob:147 » UnknownHost
  TestMiniMRProxyUser.testValidProxyUser:135->access$000:41->mrRun:123 » UnknownHost
  TestMROldApiJobs.testJobSucceed:116->runJobSucceed:177->runJob:212 » UnknownHost
  TestMROldApiJobs.testJobFail:145->runJobFail:165->runJob:212 » UnknownHost Inv...
  TestSpeculativeExecution.testSpeculativeExecution:207->runSpecTest:306 » UnknownHost
  TestMRAMWithNonNormalizedCapabilities.testJobWithNonNormalizedCapabilities:102 » UnknownHost
  TestMRJobsWithProfiler.testProfiler:138 » UnknownHost Invalid host name: local...
  TestMRJobsWithHistoryService.testJobHistoryData:131 » UnknownHost Invalid host...
  TestJobOutputCommitter.testDefaultCleanupAndAbort:224->testSuccessfulJob:149 » UnknownHost
  TestJobOutputCommitter.testCustomAbort:243->testSuccessfulJob:149 » UnknownHost
  TestJobOutputCommitter.testCustomCleanup:269->testSuccessfulJob:149 » UnknownHost
  TestEncryptedShuffle.createCustomYarnClasspath:69 NoClassDefFound sun.security...
  TestEncryptedShuffle.cleanUpMiniClusterSpecialConfig:77 NoClassDefFound sun.se...
  TestEncryptedShuffle.createCustomYarnClasspath:69 NoClassDefFound sun.security...
  TestEncryptedShuffle.cleanUpMiniClusterSpecialConfig:77 NoClassDefFound sun.se...
  TestJHSSecurity.testDelegationToken:111 » ServiceState java.net.BindException:...

Tests run: 500, Failures: 7, Errors: 56, Skipped: 11

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce-client-hs-plugins 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-client-hs-plugins ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-client-hs-plugins ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-client-hs-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-client-hs-plugins ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-client-hs-plugins ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-client-hs-plugins ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs-plugins/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapreduce.v2.hs.webapp.TestMapReduceTrackingUriPlugin
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.894 sec - in org.apache.hadoop.mapreduce.v2.hs.webapp.TestMapReduceTrackingUriPlugin

Results :

Tests run: 1, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop MapReduce Examples 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce-examples ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-mapreduce-examples ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-mapreduce-examples ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-mapreduce-examples ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-mapreduce-examples ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-mapreduce-examples ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-examples/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.examples.TestBaileyBorweinPlouffe
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.631 sec - in org.apache.hadoop.examples.TestBaileyBorweinPlouffe
Running org.apache.hadoop.examples.TestWordStats
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.683 sec - in org.apache.hadoop.examples.TestWordStats
Running org.apache.hadoop.examples.terasort.TestTeraSort
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.075 sec - in org.apache.hadoop.examples.terasort.TestTeraSort
Running org.apache.hadoop.examples.pi.math.TestSummation
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.13 sec - in org.apache.hadoop.examples.pi.math.TestSummation
Running org.apache.hadoop.examples.pi.math.TestModular
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.336 sec - in org.apache.hadoop.examples.pi.math.TestModular
Running org.apache.hadoop.examples.pi.math.TestLongLong
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.303 sec - in org.apache.hadoop.examples.pi.math.TestLongLong
Running org.apache.hadoop.mapreduce.lib.db.TestDBJob
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.543 sec - in org.apache.hadoop.mapreduce.lib.db.TestDBJob

Results :

Tests run: 11, Failures: 0, Errors: 0, Skipped: 1

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hadoop-mapreduce 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-mapreduce ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop MapReduce Streaming 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-streaming ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-streaming ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-streaming ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-streaming ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-log-dir) @ hadoop-streaming ---
[INFO] Executing tasks

main:
   [delete] Deleting directory /opt/develop/hadoop-common/hadoop-tools/hadoop-streaming/target/test-dir
    [mkdir] Created dir: /opt/develop/hadoop-common/hadoop-tools/hadoop-streaming/target/test-dir
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (copy-test-bin) @ hadoop-streaming ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-streaming ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-streaming ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-streaming/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.streaming.TestClassWithNoPackage
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.673 sec - in org.apache.hadoop.streaming.TestClassWithNoPackage
Running org.apache.hadoop.streaming.TestStreamJob
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.366 sec - in org.apache.hadoop.streaming.TestStreamJob
Running org.apache.hadoop.streaming.TestStreamingCounters
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.701 sec - in org.apache.hadoop.streaming.TestStreamingCounters
Running org.apache.hadoop.streaming.TestMRFramework
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.653 sec - in org.apache.hadoop.streaming.TestMRFramework
Running org.apache.hadoop.streaming.TestStreamingBadRecords
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.729 sec - in org.apache.hadoop.streaming.TestStreamingBadRecords
Running org.apache.hadoop.streaming.TestStreamingSeparator
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.446 sec - in org.apache.hadoop.streaming.TestStreamingSeparator
Running org.apache.hadoop.streaming.TestRawBytesStreaming
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.582 sec - in org.apache.hadoop.streaming.TestRawBytesStreaming
Running org.apache.hadoop.streaming.TestAutoInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.644 sec - in org.apache.hadoop.streaming.TestAutoInputFormat
Running org.apache.hadoop.streaming.TestSymLink
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 62.724 sec - in org.apache.hadoop.streaming.TestSymLink
Running org.apache.hadoop.streaming.TestMultipleArchiveFiles
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 47.366 sec - in org.apache.hadoop.streaming.TestMultipleArchiveFiles
Running org.apache.hadoop.streaming.TestStreamingCombiner
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.702 sec - in org.apache.hadoop.streaming.TestStreamingCombiner
Running org.apache.hadoop.streaming.TestGzipInput
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.771 sec - in org.apache.hadoop.streaming.TestGzipInput
Running org.apache.hadoop.streaming.TestDumpTypedBytes
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.691 sec - in org.apache.hadoop.streaming.TestDumpTypedBytes
Running org.apache.hadoop.streaming.TestStreamingBackground
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.534 sec - in org.apache.hadoop.streaming.TestStreamingBackground
Running org.apache.hadoop.streaming.TestStreamingKeyValue
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.077 sec - in org.apache.hadoop.streaming.TestStreamingKeyValue
Running org.apache.hadoop.streaming.TestStreamReduceNone
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.695 sec - in org.apache.hadoop.streaming.TestStreamReduceNone
Running org.apache.hadoop.streaming.TestStreamingExitStatus
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.654 sec - in org.apache.hadoop.streaming.TestStreamingExitStatus
Running org.apache.hadoop.streaming.TestStreamXmlMultipleRecords
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.108 sec - in org.apache.hadoop.streaming.TestStreamXmlMultipleRecords
Running org.apache.hadoop.streaming.TestMultipleCachefiles
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 61.239 sec - in org.apache.hadoop.streaming.TestMultipleCachefiles
Running org.apache.hadoop.streaming.TestLoadTypedBytes
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.933 sec - in org.apache.hadoop.streaming.TestLoadTypedBytes
Running org.apache.hadoop.streaming.TestTypedBytesStreaming
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.912 sec - in org.apache.hadoop.streaming.TestTypedBytesStreaming
Running org.apache.hadoop.streaming.TestStreamXmlRecordReader
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.629 sec - in org.apache.hadoop.streaming.TestStreamXmlRecordReader
Running org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.238 sec - in org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes
Running org.apache.hadoop.streaming.io.TestKeyOnlyTextOutputReader
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.464 sec - in org.apache.hadoop.streaming.io.TestKeyOnlyTextOutputReader
Running org.apache.hadoop.streaming.TestStreamingOutputOnlyKeys
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.711 sec - in org.apache.hadoop.streaming.TestStreamingOutputOnlyKeys
Running org.apache.hadoop.streaming.TestFileArgs
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 52.657 sec - in org.apache.hadoop.streaming.TestFileArgs
Running org.apache.hadoop.streaming.TestStreamingFailure
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.154 sec - in org.apache.hadoop.streaming.TestStreamingFailure
Running org.apache.hadoop.streaming.TestStreaming
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.509 sec - in org.apache.hadoop.streaming.TestStreaming
Running org.apache.hadoop.streaming.TestStreamDataProtocol
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.57 sec - in org.apache.hadoop.streaming.TestStreamDataProtocol
Running org.apache.hadoop.streaming.TestStreamingStderr
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.318 sec - in org.apache.hadoop.streaming.TestStreamingStderr
Running org.apache.hadoop.streaming.TestUnconsumedInput
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.637 sec - in org.apache.hadoop.streaming.TestUnconsumedInput
Running org.apache.hadoop.streaming.TestStreamAggregate
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.927 sec - in org.apache.hadoop.streaming.TestStreamAggregate
Running org.apache.hadoop.typedbytes.TestTypedBytesWritable
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.054 sec - in org.apache.hadoop.typedbytes.TestTypedBytesWritable

Results :

Tests run: 58, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Distributed Copy 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-distcp ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-distcp ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-distcp ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-distcp ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-distcp ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-distcp ---
[WARNING] The parameter forkMode is deprecated since version 2.14. Use forkCount and reuseForks instead.
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-distcp/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.tools.TestIntegration
Tests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.148 sec - in org.apache.hadoop.tools.TestIntegration
Running org.apache.hadoop.tools.TestGlobbedCopyListing
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.313 sec - in org.apache.hadoop.tools.TestGlobbedCopyListing
Running org.apache.hadoop.tools.TestCopyListing
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.371 sec - in org.apache.hadoop.tools.TestCopyListing
Running org.apache.hadoop.tools.TestOptionsParser
Tests run: 25, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.473 sec - in org.apache.hadoop.tools.TestOptionsParser
Running org.apache.hadoop.tools.TestDistCpViewFs
Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32 sec - in org.apache.hadoop.tools.TestDistCpViewFs
Running org.apache.hadoop.tools.mapred.TestRetriableFileCopyCommand
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.382 sec - in org.apache.hadoop.tools.mapred.TestRetriableFileCopyCommand
Running org.apache.hadoop.tools.mapred.TestCopyOutputFormat
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.141 sec - in org.apache.hadoop.tools.mapred.TestCopyOutputFormat
Running org.apache.hadoop.tools.mapred.TestCopyMapper
Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.293 sec - in org.apache.hadoop.tools.mapred.TestCopyMapper
Running org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.732 sec - in org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat
Running org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 72.517 sec - in org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat
Running org.apache.hadoop.tools.mapred.TestCopyCommitter
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.08 sec - in org.apache.hadoop.tools.mapred.TestCopyCommitter
Running org.apache.hadoop.tools.TestExternalCall
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.362 sec - in org.apache.hadoop.tools.TestExternalCall
Running org.apache.hadoop.tools.TestFileBasedCopyListing
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 46.602 sec - in org.apache.hadoop.tools.TestFileBasedCopyListing
Running org.apache.hadoop.tools.util.TestDistCpUtils
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.821 sec - in org.apache.hadoop.tools.util.TestDistCpUtils
Running org.apache.hadoop.tools.util.TestThrottledInputStream
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 61.097 sec - in org.apache.hadoop.tools.util.TestThrottledInputStream
Running org.apache.hadoop.tools.util.TestRetriableCommand
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.907 sec - in org.apache.hadoop.tools.util.TestRetriableCommand

Results :

Tests run: 116, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Archives 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-archives ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-archives ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-archives ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-archives ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-log-dir) @ hadoop-archives ---
[INFO] Executing tasks

main:
   [delete] Deleting directory /opt/develop/hadoop-common/hadoop-tools/hadoop-archives/target/test-dir
    [mkdir] Created dir: /opt/develop/hadoop-common/hadoop-tools/hadoop-archives/target/test-dir
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-archives ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-archives ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-archives/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.tools.TestHadoopArchives
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.74 sec - in org.apache.hadoop.tools.TestHadoopArchives

Results :

Tests run: 4, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Rumen 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-rumen ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-rumen ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-rumen ---
[INFO] Compiling 1 source file to /opt/develop/hadoop-common/hadoop-tools/hadoop-rumen/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-rumen ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-log-dir) @ hadoop-rumen ---
[INFO] Executing tasks

main:
   [delete] Deleting directory /opt/develop/hadoop-common/hadoop-tools/hadoop-rumen/target/test-dir
    [mkdir] Created dir: /opt/develop/hadoop-common/hadoop-tools/hadoop-rumen/target/test-dir
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-rumen ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-rumen ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-rumen/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.tools.rumen.TestHistograms
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.027 sec - in org.apache.hadoop.tools.rumen.TestHistograms
Running org.apache.hadoop.tools.rumen.TestRandomSeedGenerator
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.54 sec - in org.apache.hadoop.tools.rumen.TestRandomSeedGenerator
Running org.apache.hadoop.tools.rumen.TestPiecewiseLinearInterpolation
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.498 sec - in org.apache.hadoop.tools.rumen.TestPiecewiseLinearInterpolation

Results :

Tests run: 3, Failures: 0, Errors: 0, Skipped: 1

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Gridmix 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-gridmix ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-gridmix ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-gridmix ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-gridmix ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-log-dir) @ hadoop-gridmix ---
[INFO] Executing tasks

main:
   [delete] Deleting directory /opt/develop/hadoop-common/hadoop-tools/hadoop-gridmix/target/test-dir
    [mkdir] Created dir: /opt/develop/hadoop-common/hadoop-tools/hadoop-gridmix/target/test-dir
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-gridmix ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-gridmix ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-gridmix/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 68.591 sec - in org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation
Running org.apache.hadoop.mapred.gridmix.TestHighRamJob
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.523 sec - in org.apache.hadoop.mapred.gridmix.TestHighRamJob
Running org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.995 sec - in org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs
Running org.apache.hadoop.mapred.gridmix.TestGridmixSummary
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.014 sec - in org.apache.hadoop.mapred.gridmix.TestGridmixSummary
Running org.apache.hadoop.mapred.gridmix.TestUserResolve
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.133 sec - in org.apache.hadoop.mapred.gridmix.TestUserResolve
Running org.apache.hadoop.mapred.gridmix.TestGridmixSubmission
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 219.933 sec - in org.apache.hadoop.mapred.gridmix.TestGridmixSubmission
Running org.apache.hadoop.mapred.gridmix.TestGridmixStatistics
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.33 sec - in org.apache.hadoop.mapred.gridmix.TestGridmixStatistics
Running org.apache.hadoop.mapred.gridmix.TestRecordFactory
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.743 sec - in org.apache.hadoop.mapred.gridmix.TestRecordFactory
Running org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.417 sec - in org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils
Running org.apache.hadoop.mapred.gridmix.TestSleepJob
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 343.161 sec - in org.apache.hadoop.mapred.gridmix.TestSleepJob
Running org.apache.hadoop.mapred.gridmix.TestRandomTextDataGenerator
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.24 sec - in org.apache.hadoop.mapred.gridmix.TestRandomTextDataGenerator
Running org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.904 sec - in org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators
Running org.apache.hadoop.mapred.gridmix.TestGridMixClasses
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.077 sec - in org.apache.hadoop.mapred.gridmix.TestGridMixClasses
Running org.apache.hadoop.mapred.gridmix.TestLoadJob
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 229.057 sec - in org.apache.hadoop.mapred.gridmix.TestLoadJob
Running org.apache.hadoop.mapred.gridmix.TestFileQueue
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.265 sec - in org.apache.hadoop.mapred.gridmix.TestFileQueue
Running org.apache.hadoop.mapred.gridmix.TestGridmixRecord
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.533 sec - in org.apache.hadoop.mapred.gridmix.TestGridmixRecord
Running org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.748 sec - in org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation
Running org.apache.hadoop.mapred.gridmix.TestFilePool
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.511 sec - in org.apache.hadoop.mapred.gridmix.TestFilePool
Running org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.588 sec - in org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm

Results :

Tests run: 74, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Data Join 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-datajoin ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-datajoin ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-datajoin ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-datajoin ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-log-dir) @ hadoop-datajoin ---
[INFO] Executing tasks

main:
   [delete] Deleting directory /opt/develop/hadoop-common/hadoop-tools/hadoop-datajoin/target/test-dir
    [mkdir] Created dir: /opt/develop/hadoop-common/hadoop-tools/hadoop-datajoin/target/test-dir
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-datajoin ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-datajoin ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-datajoin/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.contrib.utils.join.TestDataJoin
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.75 sec - in org.apache.hadoop.contrib.utils.join.TestDataJoin

Results :

Tests run: 1, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Extras 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-extras ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-extras ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-extras ---
[INFO] Compiling 2 source files to /opt/develop/hadoop-common/hadoop-tools/hadoop-extras/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-extras ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-log-dir) @ hadoop-extras ---
[INFO] Executing tasks

main:
   [delete] Deleting directory /opt/develop/hadoop-common/hadoop-tools/hadoop-extras/target/test-dir
    [mkdir] Created dir: /opt/develop/hadoop-common/hadoop-tools/hadoop-extras/target/test-dir
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-extras ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-extras ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-extras/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.tools.TestLogalyzer
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.659 sec - in org.apache.hadoop.tools.TestLogalyzer
Running org.apache.hadoop.tools.TestDistCh
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.178 sec - in org.apache.hadoop.tools.TestDistCh
Running org.apache.hadoop.tools.TestCopyFiles
Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 103.565 sec - in org.apache.hadoop.tools.TestCopyFiles
Running org.apache.hadoop.mapred.tools.TestGetGroups
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.033 sec - in org.apache.hadoop.mapred.tools.TestGetGroups

Results :

Tests run: 20, Failures: 0, Errors: 0, Skipped: 1

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Pipes 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-pipes ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (make) @ hadoop-pipes ---
[INFO] Executing tasks

main:
     [exec] -- Configuring done
     [exec] -- Generating done
     [exec] -- Build files have been written to: /opt/develop/hadoop-common/hadoop-tools/hadoop-pipesJAVA_HOME=, JAVA_JVM_LIBRARY=/opt/ibm/java-ppc64le-71/jre/lib/ppc64le/default/libjvm.so
     [exec] JAVA_INCLUDE_PATH=/opt/ibm/java-ppc64le-/target/native
     [exec] 71/include, JAVA_INCLUDE_PATH2=/opt/ibm/java-ppc64le-71/include/linux
     [exec] Located all JNI components successfully.
     [exec] /usr/bin/cmake -H/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src -B/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native --check-build-system CMakeFiles/Makefile.cmake 0
     [exec] /usr/bin/cmake -E cmake_progress_start /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/progress.marks
     [exec] make -f CMakeFiles/Makefile2 all
     [exec] make[1]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make -f CMakeFiles/hadooppipes.dir/build.make CMakeFiles/hadooppipes.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/hadooppipes.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make -f CMakeFiles/hadooppipes.dir/build.make CMakeFiles/hadooppipes.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/hadooppipes.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles  1
     [exec] [ 14%] Built target hadooppipes
     [exec] make -f CMakeFiles/hadooputils.dir/build.make CMakeFiles/hadooputils.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/hadooputils.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make -f CMakeFiles/hadooputils.dir/build.make CMakeFiles/hadooputils.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/hadooputils.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles  2 3
     [exec] [ 42%] Built target hadooputils
     [exec] make -f CMakeFiles/pipes-sort.dir/build.make CMakeFiles/pipes-sort.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/pipes-sort.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make -f CMakeFiles/pipes-sort.dir/build.make CMakeFiles/pipes-sort.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/pipes-sort.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles  4
     [exec] [ 57%] Built target pipes-sort
     [exec] make -f CMakeFiles/wordcount-nopipe.dir/build.make CMakeFiles/wordcount-nopipe.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/wordcount-nopipe.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make -f CMakeFiles/wordcount-nopipe.dir/build.make CMakeFiles/wordcount-nopipe.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/wordcount-nopipe.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles  5
     [exec] [ 71%] Built target wordcount-nopipe
     [exec] make -f CMakeFiles/wordcount-part.dir/build.make CMakeFiles/wordcount-part.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/wordcount-part.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make -f CMakeFiles/wordcount-part.dir/build.make CMakeFiles/wordcount-part.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/wordcount-part.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles  6
     [exec] [ 85%] Built target wordcount-part
     [exec] make -f CMakeFiles/wordcount-simple.dir/build.make CMakeFiles/wordcount-simple.dir/depend
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] cd /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/src /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/wordcount-simple.dir/DependInfo.cmake --color=
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make -f CMakeFiles/wordcount-simple.dir/build.make CMakeFiles/wordcount-simple.dir/build
     [exec] make[2]: Entering directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] make[2]: Nothing to be done for `CMakeFiles/wordcount-simple.dir/build'.
     [exec] make[2]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_report /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles  7
     [exec] [100%] Built target wordcount-simple
     [exec] make[1]: Leaving directory `/opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native'
     [exec] /usr/bin/cmake -E cmake_progress_start /opt/develop/hadoop-common/hadoop-tools/hadoop-pipes/target/native/CMakeFiles 0
     [exec] [ 14%] Built target hadooppipes
     [exec] [ 42%] Built target hadooputils
     [exec] [ 57%] Built target pipes-sort
     [exec] [ 71%] Built target wordcount-nopipe
     [exec] [ 85%] Built target wordcount-part
     [exec] [100%] Built target wordcount-simple
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop OpenStack support 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-openstack ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-openstack ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-openstack ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-openstack ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-openstack ---
[INFO] Not compiling test sources
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-openstack ---
[INFO] Tests are skipped.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Client 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-client ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-client ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-client ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-client ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-client ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-client ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Mini-Cluster 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-minicluster ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-minicluster ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-minicluster ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-minicluster ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-minicluster ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-minicluster ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Scheduler Load Simulator 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-sls ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-sls ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-sls ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-sls ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-sls ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-sls ---
[INFO] Surefire report directory: /opt/develop/hadoop-common/hadoop-tools/hadoop-sls/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.567 sec - in org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner
Running org.apache.hadoop.yarn.sls.web.TestSLSWebApp
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.239 sec - in org.apache.hadoop.yarn.sls.web.TestSLSWebApp
Running org.apache.hadoop.yarn.sls.TestSLSRunner
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.667 sec <<< FAILURE! - in org.apache.hadoop.yarn.sls.TestSLSRunner
testSimulatorRunning(org.apache.hadoop.yarn.sls.TestSLSRunner)  Time elapsed: 2.598 sec  <<< ERROR!
org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:568)
	at sun.nio.ch.Net.bind(Net.java:548)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:260)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:87)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:854)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:273)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:814)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:918)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.sls.SLSRunner.startRM(SLSRunner.java:164)
	at org.apache.hadoop.yarn.sls.SLSRunner.start(SLSRunner.java:137)
	at org.apache.hadoop.yarn.sls.SLSRunner.main(SLSRunner.java:524)
	at org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning(TestSLSRunner.java:39)

Running org.apache.hadoop.yarn.sls.utils.TestSLSUtils
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.086 sec - in org.apache.hadoop.yarn.sls.utils.TestSLSUtils

Results :

Tests in error: 
  TestSLSRunner.testSimulatorRunning:39 » WebApp Error starting http server

Tests run: 10, Failures: 0, Errors: 1, Skipped: 0

[ERROR] There are test failures.

Please refer to /opt/develop/hadoop-common/hadoop-tools/hadoop-sls/target/surefire-reports for the individual test results.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Tools Dist 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-tools-dist ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-tools-dist ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-tools-dist ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-tools-dist ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-tools-dist ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-tools-dist ---
[INFO] No tests to run.
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Tools 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-tools ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Distribution 2.4.1
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-dist ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-resources-plugin:2.2:resources (default-resources) @ hadoop-dist ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:compile (default-compile) @ hadoop-dist ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.2:testResources (default-testResources) @ hadoop-dist ---
[INFO] Using default encoding to copy filtered resources.
[INFO] 
[INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-dist ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hadoop-dist ---
[INFO] No tests to run.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................ SUCCESS [0.558s]
[INFO] Apache Hadoop Project POM ......................... SUCCESS [0.422s]
[INFO] Apache Hadoop Annotations ......................... SUCCESS [0.481s]
[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [0.076s]
[INFO] Apache Hadoop Assemblies .......................... SUCCESS [0.075s]
[INFO] Apache Hadoop Maven Plugins ....................... SUCCESS [1.623s]
[INFO] Apache Hadoop MiniKDC ............................. SUCCESS [29.475s]
[INFO] Apache Hadoop Auth ................................ SUCCESS [2:37.921s]
[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [0.105s]
[INFO] Apache Hadoop Common .............................. FAILURE [7.905s]
[INFO] Apache Hadoop NFS ................................. SUCCESS [10.988s]
[INFO] Apache Hadoop Common Project ...................... SUCCESS [0.053s]
[INFO] Apache Hadoop HDFS ................................ SUCCESS [2:48:46.967s]
[INFO] Apache Hadoop HttpFS .............................. SUCCESS [2:27.139s]
[INFO] Apache Hadoop HDFS BookKeeper Journal ............. SUCCESS [47.057s]
[INFO] Apache Hadoop HDFS-NFS ............................ SUCCESS [1:09.153s]
[INFO] Apache Hadoop HDFS Project ........................ SUCCESS [0.082s]
[INFO] hadoop-yarn ....................................... SUCCESS [0.048s]
[INFO] hadoop-yarn-api ................................... SUCCESS [3.527s]
[INFO] hadoop-yarn-common ................................ SUCCESS [1:00.069s]
[INFO] hadoop-yarn-server ................................ SUCCESS [0.053s]
[INFO] hadoop-yarn-server-common ......................... SUCCESS [6.847s]
[INFO] hadoop-yarn-server-nodemanager .................... SUCCESS [6:10.268s]
[INFO] hadoop-yarn-server-web-proxy ...................... SUCCESS [6.820s]
[INFO] hadoop-yarn-server-applicationhistoryservice ...... SUCCESS [1:12.283s]
[INFO] hadoop-yarn-server-resourcemanager ................ SUCCESS [22:08.344s]
[INFO] hadoop-yarn-server-tests .......................... SUCCESS [1:02.831s]
[INFO] hadoop-yarn-client ................................ SUCCESS [16:50.548s]
[INFO] hadoop-yarn-applications .......................... SUCCESS [0.050s]
[INFO] hadoop-yarn-applications-distributedshell ......... SUCCESS [3:45.696s]
[INFO] hadoop-yarn-applications-unmanaged-am-launcher .... SUCCESS [25.861s]
[INFO] hadoop-yarn-site .................................. SUCCESS [0.052s]
[INFO] hadoop-yarn-project ............................... SUCCESS [0.070s]
[INFO] hadoop-mapreduce-client ........................... SUCCESS [0.065s]
[INFO] hadoop-mapreduce-client-core ...................... SUCCESS [1:06.110s]
[INFO] hadoop-mapreduce-client-common .................... SUCCESS [29.214s]
[INFO] hadoop-mapreduce-client-shuffle ................... SUCCESS [5.011s]
[INFO] hadoop-mapreduce-client-app ....................... SUCCESS [9:00.448s]
[INFO] hadoop-mapreduce-client-hs ........................ SUCCESS [5:33.025s]
[INFO] hadoop-mapreduce-client-jobclient ................. SUCCESS [40:27.877s]
[INFO] hadoop-mapreduce-client-hs-plugins ................ SUCCESS [1.469s]
[INFO] Apache Hadoop MapReduce Examples .................. SUCCESS [15.529s]
[INFO] hadoop-mapreduce .................................. SUCCESS [0.071s]
[INFO] Apache Hadoop MapReduce Streaming ................. SUCCESS [6:34.084s]
[INFO] Apache Hadoop Distributed Copy .................... SUCCESS [6:09.653s]
[INFO] Apache Hadoop Archives ............................ SUCCESS [26.361s]
[INFO] Apache Hadoop Rumen ............................... SUCCESS [4.380s]
[INFO] Apache Hadoop Gridmix ............................. SUCCESS [15:36.743s]
[INFO] Apache Hadoop Data Join ........................... SUCCESS [9.360s]
[INFO] Apache Hadoop Extras .............................. SUCCESS [2:28.171s]
[INFO] Apache Hadoop Pipes ............................... SUCCESS [1.773s]
[INFO] Apache Hadoop OpenStack support ................... SUCCESS [0.068s]
[INFO] Apache Hadoop Client .............................. SUCCESS [0.063s]
[INFO] Apache Hadoop Mini-Cluster ........................ SUCCESS [0.065s]
[INFO] Apache Hadoop Scheduler Load Simulator ............ SUCCESS [5.210s]
[INFO] Apache Hadoop Tools Dist .......................... SUCCESS [0.093s]
[INFO] Apache Hadoop Tools ............................... SUCCESS [0.026s]
[INFO] Apache Hadoop Distribution ........................ SUCCESS [0.056s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 5:18:10.519s
[INFO] Finished at: Thu Jan 22 05:14:37 UTC 2015
[INFO] Final Memory: 84M/575M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-common: An Ant BuildException has occured: exec returned: 1
[ERROR] around Ant part ...<exec dir="/opt/develop/hadoop-common/hadoop-common-project/hadoop-common/target/native" executable="cmake" failonerror="true">... @ 4:130 in /opt/develop/hadoop-common/hadoop-common-project/hadoop-common/target/antrun/build-main.xml
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-common
[INFO] Build failures were ignored.
